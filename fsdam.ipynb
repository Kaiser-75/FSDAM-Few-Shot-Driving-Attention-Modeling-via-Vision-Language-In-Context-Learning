{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ca03a18-8261-48fb-9ced-d214d24ab3dc",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a9800b-9a30-4533-b3e4-29db0a035ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA H100 PCIe, VRAM: 79GB\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision transformers bitsandbytes peft accelerate pillow lightning sentencepiece -q\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}, VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.0f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03889247-7469-427d-bf90-d234b2aa4d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: rouge-score in ./.local/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: joblib in /usr/lib/python3/dist-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: absl-py in /usr/lib/python3/dist-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from rouge-score) (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tf-keras in ./.local/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in /usr/lib/python3/dist-packages (from tf-keras) (2.19.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/utils/fixes.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version  # type: ignore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_score version: 0.1.2\n",
      "setuptools version: 80.9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk pandas tqdm rouge-score\n",
    "!pip install tf-keras\n",
    "!pip install nltk -q\n",
    "!pip install rouge_score -q\n",
    "!pip install --upgrade setuptools -q\n",
    "import nltk\n",
    "import pkg_resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')  \n",
    "try:\n",
    "    rouge_version = pkg_resources.get_distribution(\"rouge_score\").version\n",
    "    print(\"rouge_score version:\", rouge_version)\n",
    "    setuptools_version = pkg_resources.get_distribution(\"setuptools\").version\n",
    "    print(\"setuptools version:\", setuptools_version)\n",
    "except Exception as e:\n",
    "    print(f\"Version check failed: {e}\")\n",
    "    print(\"Proceeding with usage.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f057a4f-33a4-4fad-b124-68c732bdc3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory completely\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f0ea24-8012-4ce7-9dfc-a25860facbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-cache-dir --upgrade sentencepiece -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e04ca26c-af29-4cd1-ab52-10598aaf85ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece version: 0.2.0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece ; print(\"SentencePiece version:\", sentencepiece.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdf75f-8c38-427c-9ac5-4a2293d25bfc",
   "metadata": {},
   "source": [
    "Directory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be2fc9b2-7f60-47bf-8791-6098926473cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train records: 80\n",
      "Val   records: 20\n",
      "Train images: 80\n",
      "Val   images: 20\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "train_json = \"fsdam/few_shot/train_llava.json\"\n",
    "val_json   = \"fsdam/few_shot/val_llava.json\"\n",
    "train_dir  = \"fsdam/few_shot/train\"\n",
    "val_dir    = \"fsdam/few_shot/val\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train records:\", len(json.load(open(train_json))))\n",
    "print(\"Val   records:\", len(json.load(open(val_json))))\n",
    "print(\"Train images:\", len(os.listdir(train_dir)))\n",
    "print(\"Val   images:\", len(os.listdir(val_dir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55232a-a516-4889-9807-15e1d557e319",
   "metadata": {},
   "source": [
    "MODEL Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acbc4878-80e1-466c-90a0-f06a90420a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import os\n",
    "# import json\n",
    "# from PIL import Image\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "# from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "# import lightning.pytorch as L\n",
    "# from torchmetrics.text import ROUGEScore\n",
    "# from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "# # Set mixed precision\n",
    "# torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# # Define paths and constants\n",
    "# MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "# DATASET_DIR = \"fsdam\"\n",
    "# FEW_SHOT_DIR = os.path.join(DATASET_DIR, \"few_shot\")\n",
    "# ONE_SHOT_DIR = os.path.join(DATASET_DIR, \"one_shot\")\n",
    "# TEST_DIR = os.path.join(DATASET_DIR, \"test_set\")\n",
    "# TRAIN_JSON_FEW = os.path.join(FEW_SHOT_DIR, \"train_llava.json\")\n",
    "# TRAIN_JSON_ONE = os.path.join(ONE_SHOT_DIR, \"train_llava.json\")\n",
    "# VAL_JSON = os.path.join(FEW_SHOT_DIR, \"val_llava.json\")\n",
    "# TEST_JSON = os.path.join(TEST_DIR, \"test_llava.json\")\n",
    "# CHECKPOINT_DIR = \"fsdam-s3/logs\"\n",
    "# MAX_LENGTH = 1200\n",
    "\n",
    "# # Create checkpoint directory\n",
    "# os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# # Verify paths\n",
    "# print(\"Few-shot train JSON exists:\", os.path.exists(TRAIN_JSON_FEW))\n",
    "# print(\"One-shot train JSON exists:\", os.path.exists(TRAIN_JSON_ONE))\n",
    "# print(\"Val JSON exists:\", os.path.exists(VAL_JSON))\n",
    "# print(\"Test JSON exists:\", os.path.exists(TEST_JSON))\n",
    "\n",
    "# # Load processor and model\n",
    "# try:\n",
    "#     processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "#     processor.tokenizer.padding_side = \"right\"\n",
    "#     if processor.tokenizer.pad_token is None or processor.tokenizer.pad_token == \"<unk>\":\n",
    "#         processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "#         processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "#     print(\"Pad token:\", processor.tokenizer.pad_token)\n",
    "\n",
    "#     bnb_cfg = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,\n",
    "#         bnb_4bit_quant_type=\"nf4\",\n",
    "#         bnb_4bit_compute_dtype=torch.float16\n",
    "#     )\n",
    "#     model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "#         MODEL_ID, torch_dtype=torch.float16, quantization_config=bnb_cfg, trust_remote_code=True\n",
    "#     )\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading model or processor: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# # Configure LoRA\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.2,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"multi_modal_projector.linear_1\", \"multi_modal_projector.linear_2\"],\n",
    "#     init_lora_weights=\"gaussian\"\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.enable_input_require_grads()\n",
    "\n",
    "# # Move to GPU\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# print(\"Model device:\", next(model.parameters()).device)\n",
    "# print(\"Model and processor loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b0d63-1a90-4604-bd2d-b78294556c83",
   "metadata": {},
   "source": [
    "Dataset visual in Llava format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3831bd54-247e-4144-a0f6-233f37f95c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LlavaDataset(Dataset):\n",
    "#     def __init__(self, json_path, image_dir):\n",
    "#         try:\n",
    "#             with open(json_path, 'r') as f:\n",
    "#                 self.data = json.load(f)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading JSON {json_path}: {e}\")\n",
    "#             raise e\n",
    "#         self.image_dir = image_dir\n",
    "#         # Debug JSON structure\n",
    "#         print(f\"Sample JSON entry from {json_path}:\", json.dumps(self.data[0], indent=2)[:500] + \"...\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         try:\n",
    "#             item = self.data[idx]\n",
    "#             image_path = os.path.join(self.image_dir, os.path.basename(item['image']))\n",
    "#             if 'conversations' in item:\n",
    "#                 for conv in item['conversations']:\n",
    "#                     if conv.get('from') == 'gpt':\n",
    "#                         return image_path, conv['value']\n",
    "#             if 'response' in item:\n",
    "#                 return image_path, item['response']\n",
    "#             if 'answer' in item:\n",
    "#                 return image_path, item['answer']\n",
    "#             raise ValueError(f\"No valid response found for item {idx}: {json.dumps(item, indent=2)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error accessing item {idx}: {e}\")\n",
    "#             raise e\n",
    "\n",
    "# # Load datasets\n",
    "# try:\n",
    "#     train_dataset_few_shot = LlavaDataset(TRAIN_JSON_FEW, FEW_SHOT_DIR + \"/train\")\n",
    "#     train_dataset_one_shot = LlavaDataset(TRAIN_JSON_ONE, ONE_SHOT_DIR + \"/train\")\n",
    "#     val_dataset = LlavaDataset(VAL_JSON, FEW_SHOT_DIR + \"/val\")\n",
    "#     test_dataset = LlavaDataset(TEST_JSON, TEST_DIR + \"/test\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Dataset loading error: {e}\")\n",
    "#     raise e\n",
    "\n",
    "# # Debug dataset sizes\n",
    "# print(\"Few-shot train size:\", len(train_dataset_few_shot))\n",
    "# print(\"One-shot train size:\", len(train_dataset_one_shot))\n",
    "# print(\"Val size:\", len(val_dataset))\n",
    "# print(\"Test size:\", len(test_dataset))\n",
    "\n",
    "# # Collate function with few-shot support\n",
    "# def collate_fn(batch, processor=processor, prompt_template=None, use_few_shot_examples=False, device=\"cuda\"):\n",
    "#     imgs, seqs = zip(*batch)\n",
    "#     processed_imgs = []\n",
    "#     for img_path in imgs:\n",
    "#         try:\n",
    "#             img = Image.open(img_path).convert(\"RGB\").resize((224, 224))\n",
    "#             processed_imgs.append(img)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing image {img_path}: {e}\")\n",
    "#             raise e\n",
    "#     prompt = prompt_template or \"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] {seq}\"\n",
    "#     if use_few_shot_examples:\n",
    "#         few_shot_prompt = \"\"\n",
    "#         for i in range(min(2, len(train_dataset_few_shot))):\n",
    "#             img_path, example_seq = train_dataset_few_shot[i]\n",
    "#             few_shot_prompt += f\"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] {example_seq}\\n\"\n",
    "#         conversation_texts = [few_shot_prompt + prompt.format(seq=seq) for seq in seqs]\n",
    "#     else:\n",
    "#         conversation_texts = [prompt.format(seq=seq) for seq in seqs]\n",
    "#     inputs = processor(\n",
    "#         text=conversation_texts,\n",
    "#         images=processed_imgs,\n",
    "#         return_tensors=\"pt\",\n",
    "#         padding=True,\n",
    "#         truncation=True,\n",
    "#         max_length=MAX_LENGTH\n",
    "#     )\n",
    "#     labels = inputs[\"input_ids\"].clone()\n",
    "#     labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "#     inputs[\"labels\"] = labels\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     return inputs\n",
    "\n",
    "# print(\"Datasets and collate function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe6acb0-3cef-462c-9d16-b29cddb33af2",
   "metadata": {},
   "source": [
    "# Few shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28fa97c3-bf77-49b2-b5de-6a78cc803b11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Few-shot train JSON exists: True\n",
      "INFO:__main__:Val JSON exists: True\n",
      "INFO:__main__:Test JSON exists: True\n",
      "INFO:__main__:Pad token: </s>\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.57s/it]\n",
      "INFO:__main__:Model device: cuda:0\n",
      "INFO:__main__:Model and processor loaded successfully.\n",
      "INFO:__main__:Sample JSON entry from fsdam/few_shot/train_llava.json: {\n",
      "  \"id\": \"1000_00007\",\n",
      "  \"image\": \"fsdam/few_shot/train/1000_00007.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image> Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"The scene shows a city street with parked cars and buildings. The current gaze focuses on the intersection ahead. The future gaze will likely remain the same because the driver needs to monitor cross tr...\n",
      "INFO:__main__:Sample JSON entry from fsdam/few_shot/val_llava.json: {\n",
      "  \"id\": \"1737_00085\",\n",
      "  \"image\": \"fsdam/few_shot/val/1737_00085.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image> Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. This...\n",
      "INFO:__main__:Sample JSON entry from fsdam/test_set/test_llava.json: {\n",
      "  \"id\": \"1003_00057\",\n",
      "  \"image\": \"fsdam/test_set/test/1003_00057.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image> Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"The scene shows a city street with cars and buildings. The current gaze focuses on the blue car ahead. The future gaze will likely remain on the blue car. This is because the driver needs to maintain a s...\n",
      "INFO:__main__:Few-shot train size: 80\n",
      "INFO:__main__:Val size: 20\n",
      "INFO:__main__:Test size: 81\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory fsdam-s3/logs/metrics/few_shot exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/ubuntu/fsdam-s3/logs exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | PeftModel  | 3.7 B  | train\n",
      "1 | rouge | ROUGEScore | 0      | train\n",
      "---------------------------------------------\n",
      "77.4 M    Trainable params\n",
      "3.7 B     Non-trainable params\n",
      "3.7 B     Total params\n",
      "14,965.383Total estimated model params size (MB)\n",
      "2023      Modules in train mode\n",
      "725       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation dataset size: 20\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=25` in the `DataLoader` to improve performance.\n",
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2695: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:04<00:04,  0.22it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training dataset size: 80\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=25` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/20 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 0\n",
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/111_00123.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1080_00285.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1067_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1102_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1301]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 4\n",
      "INFO:__main__:Sample Training Prediction: sierp…ALL\n",
      "1th [ [by,. buildingmentamentsingsessest, of.ishersings buildings and buildings buildings ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|▌         | 1/20 [00:01<00:28,  0.68it/s, v_num=shot, train_loss_step=14.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1124_00042.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1009_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1006_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1092_00119.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1281]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  10%|█         | 2/20 [00:02<00:26,  0.69it/s, v_num=shot, train_loss_step=10.90]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1136_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1027_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1091_00103.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1000_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1272]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  15%|█▌        | 3/20 [00:04<00:24,  0.69it/s, v_num=shot, train_loss_step=10.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1036_00017.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1096_00208.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1095_00016.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/10_00048.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  20%|██        | 4/20 [00:05<00:23,  0.68it/s, v_num=shot, train_loss_step=7.470]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1129_00181.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1082_00070.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1024_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1046_00005.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1288]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  25%|██▌       | 5/20 [00:07<00:21,  0.68it/s, v_num=shot, train_loss_step=5.390]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1100_00111.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1145_00038.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1135_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1064_00116.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1302]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 5, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  30%|███       | 6/20 [00:08<00:20,  0.68it/s, v_num=shot, train_loss_step=4.940]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1066_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1058_00033.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1016_00053.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1076_00009.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1277]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 6, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  35%|███▌      | 7/20 [00:10<00:19,  0.68it/s, v_num=shot, train_loss_step=4.320]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/105_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1021_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1106_00061.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1077_00265.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 7, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  40%|████      | 8/20 [00:11<00:17,  0.68it/s, v_num=shot, train_loss_step=4.290]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1040_00131.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1069_00127.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1130_00145.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1142_00123.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1299]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 8, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  45%|████▌     | 9/20 [00:13<00:16,  0.68it/s, v_num=shot, train_loss_step=4.220]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1029_00109.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1056_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1065_00130.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1020_00018.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1271]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 9, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 10/20 [00:14<00:14,  0.69it/s, v_num=shot, train_loss_step=4.160]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:03<00:13,  0.30it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:04<00:06,  0.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:08<00:05,  0.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:14<00:03,  0.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:17<00:00,  0.29it/s]\u001b[A\n",
      "Epoch 0:  50%|█████     | 10/20 [00:32<00:32,  0.31it/s, v_num=shot, train_loss_step=4.160, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1050_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1044_00041.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1111_00361.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1099_00087.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 10, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  55%|█████▌    | 11/20 [00:44<00:36,  0.25it/s, v_num=shot, train_loss_step=4.100, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1068_00017.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1134_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1039_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/107_00084.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 11, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  60%|██████    | 12/20 [00:45<00:30,  0.26it/s, v_num=shot, train_loss_step=4.090, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1071_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1022_00154.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1128_00030.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1052_00287.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1283]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 12, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  65%|██████▌   | 13/20 [00:47<00:25,  0.28it/s, v_num=shot, train_loss_step=4.060, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1023_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1084_00106.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1119_00195.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1005_00018.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1283]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 13, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  70%|███████   | 14/20 [00:48<00:20,  0.29it/s, v_num=shot, train_loss_step=4.050, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1093_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1131_00010.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1090_00165.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1018_00083.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1281]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 14, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  75%|███████▌  | 15/20 [00:49<00:16,  0.30it/s, v_num=shot, train_loss_step=4.040, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1094_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1075_00022.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1117_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/102_00274.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 15, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|████████  | 16/20 [00:51<00:12,  0.31it/s, v_num=shot, train_loss_step=3.990, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1126_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/106_00169.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1081_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1059_00038.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 16, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  85%|████████▌ | 17/20 [00:52<00:09,  0.32it/s, v_num=shot, train_loss_step=3.960, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1105_00120.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1032_00158.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/109_00214.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1104_00101.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 17, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  90%|█████████ | 18/20 [00:54<00:06,  0.33it/s, v_num=shot, train_loss_step=3.960, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1028_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1115_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1121_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1127_00323.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1308]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 18, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  95%|█████████▌| 19/20 [00:55<00:02,  0.34it/s, v_num=shot, train_loss_step=3.940, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1108_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1079_00082.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1116_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1004_00271.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 19, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 20/20 [00:57<00:00,  0.35it/s, v_num=shot, train_loss_step=3.940, val_loss_step=4.100, val_rouge_step=0.845, val_loss_epoch=4.100, val_rouge_epoch=0.803]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:02<00:08,  0.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:03<00:05,  0.57it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:06<00:04,  0.43it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:08<00:02,  0.49it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:09<00:00,  0.52it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/20 [00:00<?, ?it/s, v_num=shot, train_loss_step=3.940, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 1\n",
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1119_00195.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1093_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/105_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1105_00120.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 4\n",
      "INFO:__main__:Sample Training Prediction: kwiet]. sierpcribe the scene and the driver's current gaze, and where that gaze will likely shift ne...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   5%|▌         | 1/20 [00:01<00:26,  0.72it/s, v_num=shot, train_loss_step=3.920, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1142_00123.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1134_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1104_00101.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1080_00285.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  10%|█         | 2/20 [00:02<00:25,  0.70it/s, v_num=shot, train_loss_step=3.900, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/106_00169.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1039_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1102_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1095_00016.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  15%|█▌        | 3/20 [00:04<00:24,  0.69it/s, v_num=shot, train_loss_step=3.880, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1020_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1128_00030.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1091_00103.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1066_00005.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1277]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  20%|██        | 4/20 [00:05<00:22,  0.70it/s, v_num=shot, train_loss_step=3.860, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1009_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1000_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1044_00041.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1116_00009.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▌       | 5/20 [00:07<00:21,  0.70it/s, v_num=shot, train_loss_step=3.860, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1016_00053.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/111_00123.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1071_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1058_00033.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1301]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 5, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  30%|███       | 6/20 [00:08<00:19,  0.70it/s, v_num=shot, train_loss_step=3.820, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1127_00323.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1067_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1005_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1036_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1280]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 6, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  35%|███▌      | 7/20 [00:09<00:18,  0.70it/s, v_num=shot, train_loss_step=3.820, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1092_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1076_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1065_00130.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1126_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1284]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 7, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  40%|████      | 8/20 [00:11<00:17,  0.70it/s, v_num=shot, train_loss_step=3.800, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1046_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1111_00361.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1024_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1106_00061.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 8, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  45%|████▌     | 9/20 [00:12<00:15,  0.70it/s, v_num=shot, train_loss_step=3.770, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1117_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/102_00274.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1081_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1131_00010.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 9, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|█████     | 10/20 [00:14<00:14,  0.70it/s, v_num=shot, train_loss_step=3.750, val_loss_step=3.900, val_rouge_step=0.867, val_loss_epoch=3.910, val_rouge_epoch=0.845, train_loss_epoch=5.560]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:03<00:13,  0.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:04<00:07,  0.42it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:08<00:02,  0.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:10<00:00,  0.48it/s]\u001b[A\n",
      "Epoch 1:  50%|█████     | 10/20 [00:24<00:24,  0.40it/s, v_num=shot, train_loss_step=3.750, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1084_00106.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1108_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1068_00017.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1135_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 10, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  55%|█████▌    | 11/20 [00:26<00:21,  0.42it/s, v_num=shot, train_loss_step=3.730, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1069_00127.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1090_00165.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1130_00145.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/10_00048.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1296]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 11, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  60%|██████    | 12/20 [00:27<00:18,  0.43it/s, v_num=shot, train_loss_step=3.720, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1096_00208.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1136_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/107_00084.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1050_00018.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 12, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  65%|██████▌   | 13/20 [00:29<00:15,  0.44it/s, v_num=shot, train_loss_step=3.730, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1032_00158.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1029_00109.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1023_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1079_00082.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1275]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 13, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  70%|███████   | 14/20 [00:30<00:13,  0.46it/s, v_num=shot, train_loss_step=3.710, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1021_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1094_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1115_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1099_00087.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1283]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 14, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  75%|███████▌  | 15/20 [00:32<00:10,  0.47it/s, v_num=shot, train_loss_step=3.700, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/109_00214.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1145_00038.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1082_00070.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1027_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1302]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 15, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|████████  | 16/20 [00:33<00:08,  0.48it/s, v_num=shot, train_loss_step=3.680, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1059_00038.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1004_00271.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1022_00154.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1040_00131.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1299]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 16, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  85%|████████▌ | 17/20 [00:35<00:06,  0.49it/s, v_num=shot, train_loss_step=3.670, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1056_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1006_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1075_00022.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1121_00009.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1308]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 17, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  90%|█████████ | 18/20 [00:36<00:04,  0.49it/s, v_num=shot, train_loss_step=3.660, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1028_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1052_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1129_00181.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1018_00083.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1281]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 18, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  95%|█████████▌| 19/20 [00:37<00:01,  0.50it/s, v_num=shot, train_loss_step=3.680, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1064_00116.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1124_00042.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1077_00265.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1100_00111.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1296]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 19, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 20/20 [00:39<00:00,  0.51it/s, v_num=shot, train_loss_step=3.650, val_loss_step=3.730, val_rouge_step=0.867, val_loss_epoch=3.740, val_rouge_epoch=0.843, train_loss_epoch=5.560]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:01<00:04,  0.93it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:02<00:03,  0.87it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:06<00:04,  0.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:07<00:01,  0.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:08<00:00,  0.56it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/20 [00:00<?, ?it/s, v_num=shot, train_loss_step=3.650, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 2\n",
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1084_00106.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1127_00323.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1131_00010.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1018_00083.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1283]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 4\n",
      "INFO:__main__:Sample Training Prediction: INST] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cribe the scene, the driver's current gaze, and where that gaze will likely shift next a...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   5%|▌         | 1/20 [00:01<00:26,  0.72it/s, v_num=shot, train_loss_step=3.650, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1068_00017.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1124_00042.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1096_00208.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1039_00071.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  10%|█         | 2/20 [00:02<00:25,  0.70it/s, v_num=shot, train_loss_step=3.640, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1080_00285.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1050_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1119_00195.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1066_00005.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  15%|█▌        | 3/20 [00:04<00:24,  0.70it/s, v_num=shot, train_loss_step=3.630, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1135_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1129_00181.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/109_00214.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1105_00120.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  20%|██        | 4/20 [00:05<00:22,  0.70it/s, v_num=shot, train_loss_step=3.630, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1117_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1065_00130.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/111_00123.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1058_00033.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1301]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  25%|██▌       | 5/20 [00:07<00:21,  0.70it/s, v_num=shot, train_loss_step=3.620, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/106_00169.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1016_00053.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1099_00087.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1128_00030.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 5, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  30%|███       | 6/20 [00:08<00:19,  0.70it/s, v_num=shot, train_loss_step=3.630, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1079_00082.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1094_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1134_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1046_00005.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1288]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 6, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  35%|███▌      | 7/20 [00:09<00:18,  0.70it/s, v_num=shot, train_loss_step=3.630, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1027_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1067_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1024_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1022_00154.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1280]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 7, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  40%|████      | 8/20 [00:11<00:17,  0.70it/s, v_num=shot, train_loss_step=3.630, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/102_00274.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1136_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1142_00123.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1130_00145.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1296]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 8, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  45%|████▌     | 9/20 [00:12<00:15,  0.70it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1100_00111.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1104_00101.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1040_00131.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1029_00109.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1299]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 9, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  50%|█████     | 10/20 [00:14<00:14,  0.70it/s, v_num=shot, train_loss_step=3.600, val_loss_step=3.640, val_rouge_step=0.866, val_loss_epoch=3.640, val_rouge_epoch=0.846, train_loss_epoch=3.770]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:04<00:16,  0.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:05<00:07,  0.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:07<00:05,  0.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:10<00:02,  0.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:14<00:00,  0.35it/s]\u001b[A\n",
      "Epoch 2:  50%|█████     | 10/20 [00:28<00:28,  0.35it/s, v_num=shot, train_loss_step=3.600, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1044_00041.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1102_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/10_00048.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1028_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 10, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  55%|█████▌    | 11/20 [00:30<00:24,  0.36it/s, v_num=shot, train_loss_step=3.620, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1082_00070.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1093_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1126_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1115_00018.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1284]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 11, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  60%|██████    | 12/20 [00:31<00:21,  0.38it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1106_00061.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1108_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1004_00271.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1069_00127.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1287]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 12, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  65%|██████▌   | 13/20 [00:33<00:17,  0.39it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1091_00103.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1020_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1005_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/105_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 13, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  70%|███████   | 14/20 [00:34<00:14,  0.40it/s, v_num=shot, train_loss_step=3.620, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1090_00165.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1116_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1071_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1092_00119.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1283]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 14, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  75%|███████▌  | 15/20 [00:35<00:11,  0.42it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1081_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1075_00022.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1064_00116.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1032_00158.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1279]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 15, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|████████  | 16/20 [00:37<00:09,  0.43it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1056_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1006_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/107_00084.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1036_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1271]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 16, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  85%|████████▌ | 17/20 [00:38<00:06,  0.44it/s, v_num=shot, train_loss_step=3.620, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1021_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1000_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1009_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1121_00009.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1308]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 17, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  90%|█████████ | 18/20 [00:40<00:04,  0.45it/s, v_num=shot, train_loss_step=3.620, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1023_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1095_00016.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1111_00361.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1059_00038.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 18, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  95%|█████████▌| 19/20 [00:41<00:02,  0.46it/s, v_num=shot, train_loss_step=3.600, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1145_00038.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1052_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1077_00265.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1076_00009.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1302]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 19, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 20/20 [00:43<00:00,  0.46it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.620, val_rouge_step=0.845, val_loss_epoch=3.620, val_rouge_epoch=0.834, train_loss_epoch=3.770]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:03<00:15,  0.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:08<00:12,  0.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:11<00:07,  0.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:14<00:03,  0.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:16<00:00,  0.31it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/20 [00:00<?, ?it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 3\n",
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1067_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1023_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1127_00323.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1059_00038.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1289]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 4\n",
      "INFO:__main__:Sample Training Prediction: [INST] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shif...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:   5%|▌         | 1/20 [00:01<00:26,  0.73it/s, v_num=shot, train_loss_step=3.590, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1131_00010.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1066_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1004_00271.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/102_00274.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1281]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  10%|█         | 2/20 [00:02<00:25,  0.72it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1095_00016.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1121_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1077_00265.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1052_00287.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1308]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  15%|█▌        | 3/20 [00:04<00:24,  0.70it/s, v_num=shot, train_loss_step=3.590, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/105_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1046_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1134_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1021_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  20%|██        | 4/20 [00:05<00:22,  0.70it/s, v_num=shot, train_loss_step=3.600, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1044_00041.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1136_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1029_00109.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1094_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  25%|██▌       | 5/20 [00:07<00:21,  0.70it/s, v_num=shot, train_loss_step=3.600, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1106_00061.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1093_00009.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1009_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1084_00106.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1287]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 5, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  30%|███       | 6/20 [00:08<00:19,  0.70it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1081_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1105_00120.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1075_00022.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1128_00030.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1290]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 6, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  35%|███▌      | 7/20 [00:09<00:18,  0.70it/s, v_num=shot, train_loss_step=3.590, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1090_00165.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1126_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1040_00131.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1065_00130.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1299]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 7, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  40%|████      | 8/20 [00:11<00:17,  0.70it/s, v_num=shot, train_loss_step=3.590, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1027_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1018_00083.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/106_00169.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1108_00007.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1275]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 8, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  45%|████▌     | 9/20 [00:12<00:15,  0.71it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1079_00082.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1000_00007.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1006_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/train/1082_00070.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1278]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 9, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  50%|█████     | 10/20 [00:14<00:14,  0.70it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.610, val_rouge_step=0.866, val_loss_epoch=3.610, val_rouge_epoch=0.822, train_loss_epoch=3.620]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00085.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1737_00287.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00119.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1744_00219.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1293]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 4\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. Th...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next an...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 1/5 [00:04<00:17,  0.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1746_00008.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1747_00012.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00018.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1748_00243.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1282]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 2/5 [00:07<00:11,  0.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1753_00160.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1754_00079.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1755_00078.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1756_00013.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1292]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 3/5 [00:11<00:07,  0.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1760_00071.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1761_00005.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1764_00032.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1767_00017.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1285]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 4/5 [00:14<00:03,  0.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 4 samples\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1768_00239.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1771_00259.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1772_00192.png\n",
      "INFO:__main__:Loading image: fsdam/few_shot/val/1773_00089.png\n",
      "INFO:__main__:Processing 4 texts\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([4, 1304]), pixel_values=torch.Size([4, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 5/5 [00:18<00:00,  0.28it/s]\u001b[A\n",
      "Epoch 3:  50%|█████     | 10/20 [00:32<00:32,  0.31it/s, v_num=shot, train_loss_step=3.610, val_loss_step=3.610, val_rouge_step=0.843, val_loss_epoch=3.610, val_rouge_epoch=0.824, train_loss_epoch=3.600]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Lightning checkpoint saved to fsdam-s3/logs/final_model_few_shot.ckpt\n",
      "INFO:__main__:LoRA weights saved to fsdam-s3/logs/lora_weights_few_shot\n",
      "INFO:__main__:Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57329"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import lightning.pytorch as L\n",
    "from torchmetrics.text import ROUGEScore\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import logging\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "file_handler = logging.FileHandler(\"fsdam-s3/logs/training.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Set mixed precision\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Define paths and constants\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "DATASET_DIR = \"fsdam\"\n",
    "FEW_SHOT_DIR = os.path.join(DATASET_DIR, \"few_shot\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test_set\")\n",
    "TRAIN_JSON_FEW = os.path.join(FEW_SHOT_DIR, \"train_llava.json\")\n",
    "VAL_JSON = os.path.join(FEW_SHOT_DIR, \"val_llava.json\")\n",
    "TEST_JSON = os.path.join(TEST_DIR, \"test_llava.json\")\n",
    "CHECKPOINT_DIR = \"fsdam-s3/logs\"\n",
    "LORA_CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, \"lora_weights_few_shot\")\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# Create checkpoint directories\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LORA_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "logger.info(\"Few-shot train JSON exists: %s\", os.path.exists(TRAIN_JSON_FEW))\n",
    "logger.info(\"Val JSON exists: %s\", os.path.exists(VAL_JSON))\n",
    "logger.info(\"Test JSON exists: %s\", os.path.exists(TEST_JSON))\n",
    "\n",
    "# Load processor and model\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    processor.tokenizer.padding_side = \"left\"  # For decoder-only architecture\n",
    "    if processor.tokenizer.pad_token is None or processor.tokenizer.pad_token == \"<unk>\":\n",
    "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    logger.info(\"Pad token: %s\", processor.tokenizer.pad_token)\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_cfg,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading model or processor: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Configure LoRA\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"multi_modal_projector.linear_1\", \"multi_modal_projector.linear_2\"],\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "logger.info(\"Model device: %s\", next(model.parameters()).device)\n",
    "logger.info(\"Model and processor loaded successfully.\")\n",
    "\n",
    "# Define dataset class\n",
    "class LlavaDataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir):\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                self.data = json.load(f)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error loading JSON %s: %s\", json_path, e)\n",
    "            raise e\n",
    "        self.image_dir = image_dir\n",
    "        logger.info(\"Sample JSON entry from %s: %s\", json_path, json.dumps(self.data[0], indent=2)[:500] + \"...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.data[idx]\n",
    "            image_path = os.path.join(self.image_dir, os.path.basename(item['image']))\n",
    "            if 'conversations' in item:\n",
    "                for conv in item['conversations']:\n",
    "                    if conv.get('from') == 'gpt':\n",
    "                        return image_path, conv['value']\n",
    "            if 'response' in item:\n",
    "                return image_path, item['response']\n",
    "            if 'answer' in item:\n",
    "                return image_path, item['answer']\n",
    "            raise ValueError(f\"No valid response found for item {idx}: {json.dumps(item, indent=2)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error accessing item %d: %s\", idx, e)\n",
    "            raise e\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    train_dataset_few_shot = LlavaDataset(TRAIN_JSON_FEW, FEW_SHOT_DIR + \"/train\")\n",
    "    val_dataset = LlavaDataset(VAL_JSON, FEW_SHOT_DIR + \"/val\")\n",
    "    test_dataset = LlavaDataset(TEST_JSON, TEST_DIR + \"/test\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Dataset loading error: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Debug dataset sizes\n",
    "logger.info(\"Few-shot train size: %d\", len(train_dataset_few_shot))\n",
    "logger.info(\"Val size: %d\", len(val_dataset))\n",
    "logger.info(\"Test size: %d\", len(test_dataset))\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch, processor=processor, prompt_template=None, use_few_shot_examples=False, device=\"cuda\"):\n",
    "    logger.info(\"Processing batch with %d samples\", len(batch))\n",
    "    imgs, seqs = zip(*batch)\n",
    "    processed_imgs = []\n",
    "    for img_path in imgs:\n",
    "        logger.info(\"Loading image: %s\", img_path)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((336, 336))  # Updated to 336x336\n",
    "            processed_imgs.append(img)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error processing image %s: %s\", img_path, e)\n",
    "            raise e\n",
    "\n",
    "    prompt = prompt_template or \"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] {seq}\"\n",
    "    if use_few_shot_examples:\n",
    "        few_shot_prompt = \"\"\n",
    "        for i in range(min(2, len(train_dataset_few_shot))):\n",
    "            img_path, example_seq = train_dataset_few_shot[i]\n",
    "            few_shot_prompt += f\"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] {example_seq}\\n\"\n",
    "        conversation_texts = [few_shot_prompt + prompt.format(seq=seq) for seq in seqs]\n",
    "    else:\n",
    "        conversation_texts = [prompt.format(seq=seq) for seq in seqs]\n",
    "\n",
    "    logger.info(\"Processing %d texts\", len(conversation_texts))\n",
    "    try:\n",
    "        inputs = processor(\n",
    "            text=conversation_texts,\n",
    "            images=processed_imgs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=False,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        logger.info(\"Input shapes: input_ids=%s, pixel_values=%s\", inputs['input_ids'].shape, inputs['pixel_values'].shape)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Processor error: %s\", e)\n",
    "        raise e\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    return inputs\n",
    "\n",
    "# Lightning module\n",
    "class LlavaModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.rouge = ROUGEScore()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logger.info(\"Training batch %d, size: %d\", batch_idx, len(batch['input_ids']))\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        if batch_idx == 0:\n",
    "            predictions = self.processor.batch_decode(outputs.logits.argmax(-1), skip_special_tokens=True)\n",
    "            logger.info(\"Sample Training Prediction: %s...\", predictions[0][:100])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logger.info(\"Validation batch %d, size: %d\", batch_idx, len(batch['input_ids']))\n",
    "        \n",
    "        # Calculate validation loss first\n",
    "        outputs = self.model(**batch)\n",
    "        val_loss = outputs.loss\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        # Then generate predictions for ROUGE score\n",
    "        generation_kwargs = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "        generated_outputs = self.model.generate(\n",
    "            **generation_kwargs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.processor.tokenizer.eos_token_id\n",
    "        )\n",
    "        predictions = self.processor.batch_decode(generated_outputs, skip_special_tokens=True)\n",
    "        valid_labels = batch[\"labels\"].clone()\n",
    "        valid_labels[valid_labels == -100] = self.processor.tokenizer.pad_token_id\n",
    "        targets = self.processor.tokenizer.batch_decode(valid_labels, skip_special_tokens=True)\n",
    "        targets = [t.replace(self.processor.tokenizer.pad_token, \"\").strip() for t in targets]\n",
    "        predictions = [p.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in p else p for p in predictions]\n",
    "        rouge_score = self.rouge(predictions, targets)[\"rougeL_fmeasure\"]\n",
    "        self.log(\"val_rouge\", rouge_score, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            logger.info(\"Sample Validation Prediction: %s...\", predictions[0][:100])\n",
    "            logger.info(\"Sample Validation Target: %s...\", targets[0][:100])\n",
    "            # Save sample predictions\n",
    "            sample_output = [{\"prediction\": p, \"target\": t} for p, t in zip(predictions, targets)]\n",
    "            with open(os.path.join(CHECKPOINT_DIR, f\"val_samples_epoch_{self.current_epoch}.json\"), 'a') as f:\n",
    "                json.dump(sample_output, f, indent=2)\n",
    "                f.write('\\n')\n",
    "        \n",
    "        return {\"val_loss\": val_loss, \"val_rouge\": rouge_score}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config[\"lr\"])\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config[\"max_epochs\"])\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        logger.info(\"Training dataset size: %d\", len(train_dataset_few_shot))\n",
    "        return DataLoader(\n",
    "            train_dataset_few_shot,\n",
    "            collate_fn=lambda x: collate_fn(x, processor, use_few_shot_examples=False),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        logger.info(\"Validation dataset size: %d\", len(val_dataset))\n",
    "        return DataLoader(\n",
    "            val_dataset,\n",
    "            collate_fn=lambda x: collate_fn(x, processor, use_few_shot_examples=False),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        logger.info(\"Starting epoch %d\", self.current_epoch)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        save_path = os.path.join(CHECKPOINT_DIR, \"final_model_few_shot.ckpt\")\n",
    "        self.trainer.save_checkpoint(save_path)\n",
    "        logger.info(\"Lightning checkpoint saved to %s\", save_path)\n",
    "        self.model.save_pretrained(LORA_CHECKPOINT_DIR)\n",
    "        logger.info(\"LoRA weights saved to %s\", LORA_CHECKPOINT_DIR)\n",
    "\n",
    "# Training configuration\n",
    "config = {\"max_epochs\": 50, \"lr\": 5e-4, \"batch_size\": 4, \"use_contrastive\": False, \"contrastive_weight\": 0.05}\n",
    "\n",
    "# Train with early stopping, checkpointing, and CSV logging\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    precision=\"16-mixed\",\n",
    "    val_check_interval=0.5,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=5,\n",
    "    default_root_dir=CHECKPOINT_DIR,\n",
    "    logger=CSVLogger(save_dir=CHECKPOINT_DIR, name=\"metrics\", version=\"few_shot\"),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_rouge\", mode=\"max\", patience=3),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=CHECKPOINT_DIR,\n",
    "            filename=\"best_model_{epoch}_{val_rouge:.3f}\",\n",
    "            monitor=\"val_rouge\", #val_los\n",
    "            mode=\"max\",\n",
    "            save_top_k=1\n",
    "        )\n",
    "    ],\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "llava_module = LlavaModelPLModule(config, processor, model)\n",
    "trainer.fit(llava_module)\n",
    "logger.info(\"Training completed.\")\n",
    "\n",
    "# Clean up memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bf4207d-09f0-43fe-8b55-7b20e8531e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch  train_loss  val_loss\n",
      "0    0.0    5.562003  4.103061\n",
      "1    0.0    5.562003  3.908311\n",
      "2    1.0    3.765382  3.737016\n",
      "3    1.0    3.765382  3.642731\n",
      "4    2.0    3.619897  3.617633\n",
      "5    2.0    3.619897  3.610172\n",
      "6    3.0    3.599836  3.605660\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('fsdam-s3/logs/metrics/few_shot/metrics.csv')\n",
    "\n",
    "\n",
    "train = df[['epoch', 'train_loss_epoch']].dropna().rename(columns={'train_loss_epoch': 'train_loss'})\n",
    "val = df[['epoch', 'val_loss_epoch']].dropna().rename(columns={'val_loss_epoch': 'val_loss'})\n",
    "\n",
    "# Merge on epoch\n",
    "merged = pd.merge(train, val, on='epoch', how='outer').sort_values('epoch')\n",
    "print(merged.head(10))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(merged['epoch'], merged['train_loss'], label='Train Loss (Epoch)')\n",
    "plt.plot(merged['epoch'], merged['val_loss'], label='Val Loss (Epoch)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss (Epoch)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88954fb1-3c09-4b7b-82d6-3c8d3715735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pad token: </s>\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.97s/it]\n",
      "INFO:__main__:Model loaded from fsdam-s3/logs/lora_weights_few_shot and moved to cuda\n",
      "INFO:__main__:Loaded test JSON with 81 entries\n",
      "INFO:__main__:Processing batch of 81 images\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Batch predictions: The scene shows a city street with cars and buildings. The current gaze focuses on the blue car in t...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Batch predictions: The scene shows a city street with cars, buildings, and a traffic light. The current gaze focuses on...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "from torchmetrics.text import ROUGEScore\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths and constants\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "CHECKPOINT_DIR = \"fsdam-s3/logs\"\n",
    "LORA_CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, \"lora_weights_few_shot\")\n",
    "TEST_JSON = \"fsdam/test_set/test_llava.json\"\n",
    "TEST_IMAGE_DIR = \"fsdam/test_set/test\"\n",
    "OUTPUT_JSON = \"fsdam-s3/logs/predictions_few_shot.json\"\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "# Load processor\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    if processor.tokenizer.pad_token is None or processor.tokenizer.pad_token == \"<unk>\":\n",
    "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    logger.info(\"Pad token: %s\", processor.tokenizer.pad_token)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading processor: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Load fine-tuned model\n",
    "try:\n",
    "    if not os.path.exists(LORA_CHECKPOINT_DIR):\n",
    "        raise FileNotFoundError(f\"LoRA checkpoint directory not found at {LORA_CHECKPOINT_DIR}\")\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_cfg,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        LORA_CHECKPOINT_DIR,\n",
    "        is_trainable=False,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"Model loaded from %s and moved to %s\", LORA_CHECKPOINT_DIR, device)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading model: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Batch inference function\n",
    "def predict_batch(image_paths, prompt=None, batch_size=2):\n",
    "    logger.info(\"Processing batch of %d images\", len(image_paths))\n",
    "    try:\n",
    "        prompt = prompt or \"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST]\"\n",
    "        predictions = []\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_images = [Image.open(path).convert(\"RGB\").resize((336, 336)) for path in batch_paths]\n",
    "            batch_prompts = [prompt] * len(batch_paths)\n",
    "            inputs = processor(\n",
    "                text=batch_prompts,\n",
    "                images=batch_images,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=False\n",
    "            )\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            logger.info(\"Batch input shapes: input_ids=%s, pixel_values=%s\", \n",
    "                        inputs['input_ids'].shape, inputs['pixel_values'].shape)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=MAX_NEW_TOKENS,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                    eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                    min_length=10\n",
    "                )\n",
    "            batch_predictions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            batch_predictions = [p.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in p else p for p in batch_predictions]\n",
    "            predictions.extend(batch_predictions)\n",
    "            logger.info(\"Batch predictions: %s...\", batch_predictions[0][:100])\n",
    "            torch.cuda.empty_cache()  # Clear memory after each batch\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in batch inference: %s\", e)\n",
    "        return [None] * len(image_paths)\n",
    "\n",
    "# Batch inference on test dataset\n",
    "def run_batch_inference(json_path, image_dir):\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        logger.info(\"Loaded test JSON with %d entries\", len(test_data))\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading test JSON %s: %s\", json_path, e)\n",
    "        raise e\n",
    "\n",
    "    image_paths = [os.path.join(image_dir, os.path.basename(item['image'])) for item in test_data]\n",
    "    predictions = predict_batch(image_paths, batch_size=2)\n",
    "    results = []\n",
    "    rouge = ROUGEScore()\n",
    "    rouge_scores = []\n",
    "\n",
    "    for item, prediction in zip(test_data, predictions):\n",
    "        ground_truth = None\n",
    "        if 'conversations' in item:\n",
    "            for conv in item['conversations']:\n",
    "                if conv.get('from') == 'gpt':\n",
    "                    ground_truth = conv['value']\n",
    "                    break\n",
    "        elif 'response' in item:\n",
    "            ground_truth = item['response']\n",
    "        elif 'answer' in item:\n",
    "            ground_truth = item['answer']\n",
    "        if ground_truth is None:\n",
    "            logger.warning(\"No ground truth found for item %s\", item['id'])\n",
    "        if prediction is not None:\n",
    "            results.append({\n",
    "                \"id\": item['id'],\n",
    "                \"image\": item['image'],\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"prediction\": prediction\n",
    "            })\n",
    "            if ground_truth:\n",
    "                score = rouge([prediction], [ground_truth])['rougeL_fmeasure']\n",
    "                rouge_scores.append(score)\n",
    "        else:\n",
    "            logger.warning(\"Skipping item %s due to inference error\", item['id'])\n",
    "\n",
    "    # Save predictions\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "    with open(OUTPUT_JSON, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logger.info(\"Predictions saved to %s\", OUTPUT_JSON)\n",
    "\n",
    "    # Log ROUGE scores\n",
    "    if rouge_scores:\n",
    "        avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "        logger.info(\"Average ROUGE-L: %f\", avg_rouge)\n",
    "    else:\n",
    "        logger.info(\"No ROUGE scores computed (no ground truth available)\")\n",
    "\n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return results\n",
    "\n",
    "# Run inference\n",
    "if __name__ == \"__main__\":\n",
    "    predictions = run_batch_inference(TEST_JSON, TEST_IMAGE_DIR)\n",
    "    logger.info(\"Inference completed. %d predictions generated.\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49c7283-1de8-4f20-9c17-b374b2ce1c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Json path\n",
    "PRED_PATH = 'fsdam-s3/logs/predictions_few_shot.json'\n",
    "\n",
    "with open(PRED_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for entry in tqdm(data, desc=\"Scoring predictions\"):\n",
    "    gt = entry['ground_truth'].strip()\n",
    "    pr = entry['prediction'].strip()\n",
    "\n",
    "    # ROUGE-L\n",
    "    rougeL = rouge.score(gt, pr)['rougeL'].fmeasure\n",
    "\n",
    "    # BLEU-4\n",
    "    bleu4 = sentence_bleu([word_tokenize(gt)], word_tokenize(pr), smoothing_function=smoothie)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = meteor_score([word_tokenize(gt)], word_tokenize(pr))\n",
    "    \n",
    "    # Exact Match\n",
    "    exact = int(gt == pr)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": entry['id'],\n",
    "        \"image\": entry['image'],\n",
    "        \"ground_truth\": gt,\n",
    "        \"prediction\": pr,\n",
    "        \"ROUGE-L\": rougeL,\n",
    "        \"BLEU-4\": bleu4,\n",
    "        \"METEOR\": meteor,\n",
    "        \"Exact Match\": exact\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print mean scores\n",
    "print(\"\\n==== FSDAM Test Metrics ====\\n\")\n",
    "print(df[[\"ROUGE-L\", \"BLEU-4\", \"METEOR\", \"Exact Match\"]].mean())\n",
    "print(\"\\n==== Per-sample Results (first 5) ====\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Save detailed results as CSV if needed\n",
    "df.to_csv('fsdam-s3/logs/test_metrics_few_shot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06658edc-ba36-4451-97de-319e17335fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6e13f-cc65-4a53-b01c-b52ec249e8e4",
   "metadata": {},
   "source": [
    "# ONE SHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f559ad6a-e01a-4cb2-9114-73aba82851ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:One-shot train JSON exists: True\n",
      "INFO:__main__:Val JSON exists: True\n",
      "INFO:__main__:Test JSON exists: True\n",
      "INFO:__main__:Pad token: </s>\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.68s/it]\n",
      "INFO:__main__:Model device: cuda:0\n",
      "INFO:__main__:Model and processor loaded successfully.\n",
      "INFO:__main__:Dataset from fsdam/one_shot/train_llava.json: 1 samples\n",
      "INFO:__main__:Sample JSON entry: {\n",
      "  \"id\": \"1006_00018\",\n",
      "  \"image\": \"fsdam/one_shot/train/1006_00018.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image> Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"A street lined with palm trees and houses is visible, with a green traffic light. The current gaze focuses on the green light and the road ahead. The gaze will likely shift to the approaching intersecti...\n",
      "INFO:__main__:Dataset from fsdam/one_shot/val_llava.json: 20 samples\n",
      "INFO:__main__:Sample JSON entry: {\n",
      "  \"id\": \"1737_00085\",\n",
      "  \"image\": \"fsdam/one_shot/val/1737_00085.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image> Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. This...\n",
      "INFO:__main__:Dataset from fsdam/test_set/test_llava.json: 81 samples\n",
      "INFO:__main__:Sample JSON entry: {\n",
      "  \"id\": \"1003_00057\",\n",
      "  \"image\": \"fsdam/test_set/test/1003_00057.png\",\n",
      "  \"conversations\": [\n",
      "    {\n",
      "      \"from\": \"human\",\n",
      "      \"value\": \"<image> Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
      "    },\n",
      "    {\n",
      "      \"from\": \"gpt\",\n",
      "      \"value\": \"The scene shows a city street with cars and buildings. The current gaze focuses on the blue car ahead. The future gaze will likely remain on the blue car. This is because the driver needs to maintain a s...\n",
      "INFO:__main__:One-shot train size: 1\n",
      "INFO:__main__:Val size: 20\n",
      "INFO:__main__:Test size: 81\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory fsdam-s3/logs/metrics/one_shot exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:658: Checkpoint directory /home/ubuntu/fsdam-s3/logs exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | PeftModel  | 3.7 B  | train\n",
      "1 | rouge | ROUGEScore | 0      | train\n",
      "---------------------------------------------\n",
      "9.7 M     Trainable params\n",
      "3.7 B     Non-trainable params\n",
      "3.7 B     Total params\n",
      "14,694.490Total estimated model params size (MB)\n",
      "2023      Modules in train mode\n",
      "725       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation dataset size: 20\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=25` in the `DataLoader` to improve performance.\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 14.732455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Training dataset size: 1\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=25` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 0\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: sierp…ALL\n",
      "1\n",
      ", m.m\n",
      "mm trees trees trees... day2 of trees. The, day pal treesmt palmm and sky sky day trees trees,, The0 from trees pal palm trees,ing,,.. Miami A. ofmmt trees. oft of..0 y Palday light....\n",
      "INFO:__main__:Training loss: 15.448358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s, v_num=shot, train_loss_step=15.40]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 14.732455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:07,  2.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:02<00:11,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:13,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:11,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.32it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:09,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:10<00:06,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:10<00:05,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:04,  1.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:14<00:00,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:15<00:00,  1.32it/s]\u001b[A\n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 1\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: sierp…ALL\n",
      "1\n",
      ", m.m\n",
      "mm trees trees trees... day2 of trees. The, day pal treesmt palmm and sky sky day trees trees,, The0 from trees pal palm trees,ing,,.. Miami A. ofmmt trees. oft of..0 y Palday light....\n",
      "INFO:__main__:Training loss: 15.448358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 14.732455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.95it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:02<00:11,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:14,  1.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:12,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:10,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:06<00:09,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:08<00:10,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:10<00:08,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:10<00:07,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:11<00:06,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:05,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:13<00:04,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:14<00:02,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:15<00:00,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\u001b[A\n",
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 2\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: sierp…ALL\n",
      "1\n",
      ", m.m\n",
      "mm trees trees trees... day2 of trees. The, day pal treesmt palmm and sky sky day trees trees,, The0 from trees pal palm trees,ing,,.. Miami A. ofmmt trees. oft of..0 y Palday light....\n",
      "INFO:__main__:Training loss: 15.448358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 14.732455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:02<00:11,  1.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:11,  1.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:09,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:10<00:05,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:10<00:04,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:12<00:03,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:14<00:00,  1.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:14<00:00,  1.34it/s]\u001b[A\n",
      "Epoch 3:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 3\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: sierp…ALL\n",
      "1\n",
      ", m.m\n",
      "mm trees trees trees... day2 of trees. The, day pal treesmt palmm and sky sky day trees trees,, The0 from trees pal palm trees,ing,,.. Miami A. ofmmt trees. oft of..0 y Palday light....\n",
      "INFO:__main__:Training loss: 15.448358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1/1 [00:00<00:00,  2.07it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 14.732455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:02<00:11,  1.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:11,  1.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:09,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:10<00:05,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:10<00:04,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:12<00:03,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:14<00:00,  1.30it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:15<00:00,  1.33it/s]\u001b[A\n",
      "Epoch 4:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 4\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: sierp…ALL\n",
      "1\n",
      ", m.m\n",
      "mm trees trees trees... day2 of trees. The, day pal treesmt palmm and sky sky day trees trees,, The0 from trees pal palm trees,ing,,.. Miami A. ofmmt trees. oft of..0 y Palday light....\n",
      "INFO:__main__:Training loss: 15.448358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s, v_num=shot, train_loss_step=15.40, val_loss_step=15.20, val_rouge_step=0.836, val_loss_epoch=15.00, val_rouge_epoch=0.822, train_loss_epoch=15.40]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 9.022706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:02<00:11,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:03<00:10,  1.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:03<00:09,  1.54it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:04<00:08,  1.61it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:04<00:07,  1.66it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:05<00:06,  1.71it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:05<00:05,  1.74it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:07<00:05,  1.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:07<00:05,  1.57it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:08<00:04,  1.61it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:08<00:03,  1.64it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:10<00:03,  1.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:10<00:02,  1.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:11<00:01,  1.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:11<00:01,  1.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:12<00:00,  1.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:12<00:00,  1.56it/s]\u001b[A\n",
      "Epoch 5:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=15.40, val_loss_step=9.260, val_rouge_step=0.836, val_loss_epoch=9.180, val_rouge_epoch=0.833, train_loss_epoch=15.40]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 5\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: sierp…IT\n",
      "1\n",
      "ideymtm2mmero r.0202.   t  0dayetmm01mia,9 8002es220ankalkort2ing2:091a0202mmt trees0tsow0. trees200es2dayens.ammyyymino1.ing10 W 2s.as.angingmestymitoosestyest)inging1121.asagesetsas light...\n",
      "INFO:__main__:Training loss: 9.073468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s, v_num=shot, train_loss_step=9.070, val_loss_step=9.260, val_rouge_step=0.836, val_loss_epoch=9.180, val_rouge_epoch=0.833, train_loss_epoch=15.40]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 6.514228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:23,  0.76it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:17,  0.95it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:14,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:10,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:06<00:07,  1.49it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:06<00:06,  1.54it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:07<00:06,  1.40it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:08<00:05,  1.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:08<00:04,  1.48it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:09<00:03,  1.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:10<00:03,  1.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:11<00:02,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:11<00:02,  1.43it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:13<00:01,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:13<00:00,  1.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\u001b[A\n",
      "Epoch 6:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=9.070, val_loss_step=6.480, val_rouge_step=0.836, val_loss_epoch=6.490, val_rouge_epoch=0.829, train_loss_epoch=9.070]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 6\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: Unterscheidung…IT\n",
      "1..inm[ig2mm [0..s1.2.q\" t\" ..\"ia.an1.ia.q.{m\".t   ],..\"2. [p \" .{s [...\"\"2itot. [�lt�..\" 1A.asasat paly..inoos.g.ing �. ...1..w pal .os.osesyl. ing 211..et..asatedy. Pal.ito. ....2 ...\n",
      "INFO:__main__:Training loss: 6.544812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s, v_num=shot, train_loss_step=6.540, val_loss_step=6.480, val_rouge_step=0.836, val_loss_epoch=6.490, val_rouge_epoch=0.829, train_loss_epoch=9.070]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.790903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.32it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.99it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:02<00:11,  1.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:03<00:10,  1.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:03<00:08,  1.56it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:04<00:08,  1.62it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:04<00:07,  1.67it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:05<00:06,  1.72it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:05<00:05,  1.75it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:07<00:05,  1.55it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:07<00:05,  1.58it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:08<00:04,  1.62it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:08<00:03,  1.65it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:09<00:03,  1.53it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:10<00:02,  1.56it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:10<00:01,  1.58it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:12<00:01,  1.49it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:12<00:00,  1.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\u001b[A\n",
      "Epoch 7:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=6.540, val_loss_step=4.830, val_rouge_step=0.836, val_loss_epoch=4.800, val_rouge_epoch=0.832, train_loss_epoch=6.540]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 7\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: Unterscheidung…IT\n",
      "1m.elm.2�misos[..s1...em\".{\"..\"ia..1.ia.{.\"..ely\".\".,\".is\"..art.\"\"\"{.. …......{..l\".\"\"{H..asat.y..1us..ing{......= Pal.os.us.ing......asatedat...ino.......ers..at. Pal.....â{',ah.eme...\n",
      "INFO:__main__:Training loss: 4.714968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s, v_num=shot, train_loss_step=4.710, val_loss_step=4.830, val_rouge_step=0.836, val_loss_epoch=4.800, val_rouge_epoch=0.832, train_loss_epoch=6.540]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.351518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:02<00:14,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:12,  1.33it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:03<00:10,  1.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:03<00:09,  1.52it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:04<00:08,  1.59it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:04<00:07,  1.64it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:06<00:08,  1.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.40it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:08<00:06,  1.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:08<00:05,  1.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:09<00:05,  1.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:09<00:04,  1.43it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:11<00:03,  1.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:11<00:02,  1.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:12<00:02,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:13<00:01,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:13<00:00,  1.37it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:14<00:00,  1.39it/s]\u001b[A\n",
      "Epoch 8:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.710, val_loss_step=4.400, val_rouge_step=0.836, val_loss_epoch=4.360, val_rouge_epoch=0.827, train_loss_epoch=4.710]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 8\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: Unterscheidung…IT\n",
      "1... Bedeut..., \" \" \"... Pal nobodyp the image in including traffic,s perspective locatione, and the the gaze is likely go next. why./caption]\n",
      " driver sceneined with palm trees and a...\n",
      "INFO:__main__:Training loss: 4.349764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s, v_num=shot, train_loss_step=4.350, val_loss_step=4.400, val_rouge_step=0.836, val_loss_epoch=4.360, val_rouge_epoch=0.827, train_loss_epoch=4.710]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.234362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:18,  0.90it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:15,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:11,  1.27it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:09,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:09<00:05,  1.30it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:10<00:04,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:11<00:03,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:12<00:03,  1.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:12<00:02,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:14<00:00,  1.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:14<00:00,  1.33it/s]\u001b[A\n",
      "Epoch 9:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.350, val_loss_step=4.290, val_rouge_step=0.836, val_loss_epoch=4.250, val_rouge_epoch=0.823, train_loss_epoch=4.350]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 9\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: Unterscheidung…IT [1. \". \" nobodyp the image in including people's perspective locatione, and the the gaze is likely go next. why./RE]  scene sceneined with palm trees and houses. visible. with a gree...\n",
      "INFO:__main__:Training loss: 4.225982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s, v_num=shot, train_loss_step=4.230, val_loss_step=4.290, val_rouge_step=0.836, val_loss_epoch=4.250, val_rouge_epoch=0.823, train_loss_epoch=4.350]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.213659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:23,  0.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:22,  0.76it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:04<00:17,  0.91it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:14,  1.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:12,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:10,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:06<00:09,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:08<00:10,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:10<00:08,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:11<00:07,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:12<00:06,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:12<00:05,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:13<00:04,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:14<00:03,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:14<00:02,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:16<00:01,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:16<00:00,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\u001b[A\n",
      "Epoch 10:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.230, val_loss_step=4.280, val_rouge_step=0.836, val_loss_epoch=4.230, val_rouge_epoch=0.814, train_loss_epoch=4.230]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 10\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: Unterscheidung…IT [1 \" \" nobodyp the image in including people's actions locatione, and the the gaze is likely go next. why.1RE]\n",
      " scene sceneined with palm trees and houses. visible. with a green traf...\n",
      "INFO:__main__:Training loss: 4.205929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s, v_num=shot, train_loss_step=4.210, val_loss_step=4.280, val_rouge_step=0.836, val_loss_epoch=4.230, val_rouge_epoch=0.814, train_loss_epoch=4.230]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.138876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:20,  0.88it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:20,  0.83it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:04<00:16,  0.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:13,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:11,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:10,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.34it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:09<00:11,  1.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:09<00:09,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:11<00:09,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:11<00:07,  1.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:12<00:06,  1.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:13<00:05,  1.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:14<00:04,  1.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:15<00:03,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:15<00:02,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:16<00:01,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:17<00:00,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:17<00:00,  1.12it/s]\u001b[A\n",
      "Epoch 11:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.210, val_loss_step=4.200, val_rouge_step=0.836, val_loss_epoch=4.150, val_rouge_epoch=0.815, train_loss_epoch=4.210]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 11\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: Unterscheidung…IT [1 anch \"\n",
      "p the image in including people's perspective locatione, and the the gaze is likely move next. why.1RE] The scene sceneined with palm trees and houses is visible. with a gr...\n",
      "INFO:__main__:Training loss: 4.130141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s, v_num=shot, train_loss_step=4.130, val_loss_step=4.200, val_rouge_step=0.836, val_loss_epoch=4.150, val_rouge_epoch=0.815, train_loss_epoch=4.210]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.110312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:09,  2.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:26,  0.67it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:04<00:25,  0.67it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:04<00:19,  0.81it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:05<00:16,  0.92it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:13,  1.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:06<00:11,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:06<00:10,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:08<00:10,  1.03it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:09<00:09,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:10<00:08,  1.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:11<00:07,  1.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:12<00:06,  1.05it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:12<00:05,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:14<00:04,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:14<00:03,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:15<00:02,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:16<00:01,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:16<00:00,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:17<00:00,  1.14it/s]\u001b[A\n",
      "Epoch 12:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.130, val_loss_step=4.170, val_rouge_step=0.836, val_loss_epoch=4.120, val_rouge_epoch=0.817, train_loss_epoch=4.130]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 12\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: kwiet…IT [1 anch \"\n",
      "p the image in including people's perspective locatione, and the the gaze is likely move next. why./RE] The scene sceneined with palm trees and houses is visible. with a green traff...\n",
      "INFO:__main__:Training loss: 4.096413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s, v_num=shot, train_loss_step=4.100, val_loss_step=4.170, val_rouge_step=0.836, val_loss_epoch=4.120, val_rouge_epoch=0.817, train_loss_epoch=4.130]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.092103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:09,  2.01it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:24,  0.74it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:04<00:23,  0.72it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:04<00:18,  0.86it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:05<00:15,  0.98it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:13,  1.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:06<00:11,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:06<00:09,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:08<00:10,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:10<00:08,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:11<00:07,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:11<00:06,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:12<00:05,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:13<00:04,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:14<00:03,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:14<00:02,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:16<00:01,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:16<00:00,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\u001b[A\n",
      "Epoch 13:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.100, val_loss_step=4.160, val_rouge_step=0.836, val_loss_epoch=4.110, val_rouge_epoch=0.819, train_loss_epoch=4.100]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 13\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: …IT [1 anch \"\n",
      "cribe the image in including time's perspective locatione, and the the gaze is likely shift next. why./INST] The scene sceneined with palm trees and houses. visible. with a green traffic...\n",
      "INFO:__main__:Training loss: 4.073254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s, v_num=shot, train_loss_step=4.070, val_loss_step=4.160, val_rouge_step=0.836, val_loss_epoch=4.110, val_rouge_epoch=0.819, train_loss_epoch=4.100]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.065484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:09,  2.00it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:24,  0.75it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:04<00:23,  0.72it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:04<00:18,  0.85it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:05<00:15,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:13,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:06<00:11,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:06<00:09,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:08<00:10,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.14it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:10<00:08,  1.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:10<00:07,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:12<00:06,  1.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:12<00:05,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:14<00:04,  1.04it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:14<00:03,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:15<00:02,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:16<00:01,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:17<00:00,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:17<00:00,  1.13it/s]\u001b[A\n",
      "Epoch 14:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.070, val_loss_step=4.130, val_rouge_step=0.836, val_loss_epoch=4.080, val_rouge_epoch=0.820, train_loss_epoch=4.070]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 14\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: …IT [1 anch.\n",
      "cribe the scene in including time's perspective locatione, and the the gaze is likely shift next. why./INST] The scene lined with palm trees and houses. visible. with a green traffic ligh...\n",
      "INFO:__main__:Training loss: 4.040391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s, v_num=shot, train_loss_step=4.040, val_loss_step=4.130, val_rouge_step=0.836, val_loss_epoch=4.080, val_rouge_epoch=0.820, train_loss_epoch=4.070]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.050284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:18,  0.93it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:14,  1.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:10,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.36it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.42it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:08,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.29it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:10<00:05,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:04,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:15<00:00,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:15<00:00,  1.26it/s]\u001b[A\n",
      "Epoch 15:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.040, val_loss_step=4.120, val_rouge_step=0.836, val_loss_epoch=4.070, val_rouge_epoch=0.820, train_loss_epoch=4.040]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 15\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: kwiet…IT [1 anch.\n",
      "cribe the scene in including time's perspective situatione, and the the gaze is likely shift next. why./INST] A scene lined with palm trees and houses is visible. with a green traffi...\n",
      "INFO:__main__:Training loss: 4.018245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s, v_num=shot, train_loss_step=4.020, val_loss_step=4.120, val_rouge_step=0.836, val_loss_epoch=4.070, val_rouge_epoch=0.820, train_loss_epoch=4.040]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.042135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:09,  2.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.95it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:18,  0.92it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:14,  1.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:10,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:08,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:11<00:06,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:05,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:13<00:04,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:15<00:01,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:15<00:00,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:16<00:00,  1.22it/s]\u001b[A\n",
      "Epoch 16:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.020, val_loss_step=4.110, val_rouge_step=0.836, val_loss_epoch=4.060, val_rouge_epoch=0.820, train_loss_epoch=4.020]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 16\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: …IT [1 anch nobodycribe the scene in including time's perspective situatione, and the the gaze is likely shift next. why.INSTINST] A scene lined with palm trees and houses is visible. with a green tra...\n",
      "INFO:__main__:Training loss: 4.004328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s, v_num=shot, train_loss_step=4.000, val_loss_step=4.110, val_rouge_step=0.836, val_loss_epoch=4.060, val_rouge_epoch=0.820, train_loss_epoch=4.020]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.036854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:09,  1.92it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:24,  0.74it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:04<00:23,  0.71it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:04<00:19,  0.84it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:05<00:16,  0.94it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:05<00:13,  1.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:06<00:11,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:06<00:10,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:08<00:10,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:08<00:08,  1.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:10<00:08,  1.06it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:10<00:07,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:11<00:06,  1.09it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:12<00:05,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:13<00:04,  1.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:14<00:03,  1.13it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:14<00:02,  1.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:16<00:01,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:16<00:00,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:17<00:00,  1.17it/s]\u001b[A\n",
      "Epoch 17:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.000, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.821, train_loss_epoch=4.000]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 17\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: IT [1 anch\n",
      "cribe the scene in including time's perspective situatione, and the the gaze is likely shift next. why.INSTINST] A street lined with palm trees and houses is visible. with a green traffic l...\n",
      "INFO:__main__:Training loss: 3.996549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s, v_num=shot, train_loss_step=4.000, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.821, train_loss_epoch=4.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.033705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.96it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:18,  0.93it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:14,  1.07it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:10,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.35it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.41it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:08,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.15it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:10<00:06,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:11<00:05,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:04,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:15<00:01,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:15<00:00,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:16<00:00,  1.24it/s]\u001b[A\n",
      "Epoch 18:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=4.000, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.818, train_loss_epoch=4.000]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 18\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: IT [1 anch\n",
      "cribe the scene in including time's perspective situatione, and the the gaze will likely shift next. why.INSTINST] A street lined with palm trees and houses is visible. with a green traffic...\n",
      "INFO:__main__:Training loss: 3.992222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s, v_num=shot, train_loss_step=3.990, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.818, train_loss_epoch=4.000]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.032174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:17,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:14,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:10,  1.32it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.39it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.45it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:08,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.30it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:10<00:05,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:04,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:15<00:01,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:15<00:00,  1.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:16<00:00,  1.25it/s]\u001b[A\n",
      "Epoch 19:   0%|          | 0/1 [00:00<?, ?it/s, v_num=shot, train_loss_step=3.990, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.820, train_loss_epoch=3.990]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting epoch 19\n",
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/train/1006_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1271]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Training batch 0, size: 1\n",
      "INFO:__main__:Sample Training Prediction: IT [1 anch\n",
      "cribe the scene in including time's perspective situatione, and the the gaze will likely shift next. why.INSTINST] A street lined with palm trees and houses is visible. with a green traffic...\n",
      "INFO:__main__:Training loss: 3.989926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1/1 [00:00<00:00,  2.01it/s, v_num=shot, train_loss_step=3.990, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.820, train_loss_epoch=3.990]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00085.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Validation batch 0, size: 1\n",
      "INFO:__main__:Sample Validation Prediction: The scene shows a busy urban intersection with cars, buildings, and a yellow traffic light ahead. The current gaze focuses on the car in front. The future gaze will likely shift to the traffic light. ...\n",
      "INFO:__main__:Sample Validation Target: [INST] \n",
      "Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] The scene shows a busy urban intersection with cars, buildings, and a yellow traffic l...\n",
      "INFO:__main__:Validation Loss: 4.031752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:   5%|▌         | 1/20 [00:00<00:08,  2.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1737_00287.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1293]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 1, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  10%|█         | 2/20 [00:02<00:18,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00119.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 2, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  15%|█▌        | 3/20 [00:03<00:17,  0.97it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1744_00219.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1262]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 3, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  20%|██        | 4/20 [00:03<00:14,  1.12it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1746_00008.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1272]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 4, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  25%|██▌       | 5/20 [00:04<00:12,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1747_00012.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1282]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 5, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  30%|███       | 6/20 [00:04<00:10,  1.31it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00018.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 6, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  35%|███▌      | 7/20 [00:05<00:09,  1.38it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1748_00243.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1277]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 7, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  40%|████      | 8/20 [00:05<00:08,  1.44it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1753_00160.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1283]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 8, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  45%|████▌     | 9/20 [00:07<00:08,  1.25it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1754_00079.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1292]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 9, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  50%|█████     | 10/20 [00:07<00:07,  1.30it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1755_00078.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1269]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 10, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  55%|█████▌    | 11/20 [00:09<00:07,  1.17it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1756_00013.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1287]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 11, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  60%|██████    | 12/20 [00:09<00:06,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1760_00071.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1280]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 12, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  65%|██████▌   | 13/20 [00:10<00:05,  1.19it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1761_00005.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1285]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 13, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  70%|███████   | 14/20 [00:11<00:04,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1764_00032.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1274]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 14, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  75%|███████▌  | 15/20 [00:12<00:04,  1.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1767_00017.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1281]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 15, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  80%|████████  | 16/20 [00:13<00:03,  1.21it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1768_00239.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1304]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 16, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  85%|████████▌ | 17/20 [00:13<00:02,  1.24it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1771_00259.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1295]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 17, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  90%|█████████ | 18/20 [00:14<00:01,  1.20it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1772_00192.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1299]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 18, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0:  95%|█████████▌| 19/20 [00:15<00:00,  1.23it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processing batch with 1 samples\n",
      "INFO:__main__:Loading image: fsdam/one_shot/val/1773_00089.png\n",
      "INFO:__main__:Input shapes: input_ids=torch.Size([1, 1275]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Validation batch 19, size: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation DataLoader 0: 100%|██████████| 20/20 [00:15<00:00,  1.25it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 1/1 [00:26<00:00,  0.04it/s, v_num=shot, train_loss_step=3.990, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.821, train_loss_epoch=3.990]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1/1 [00:26<00:00,  0.04it/s, v_num=shot, train_loss_step=3.990, val_loss_step=4.100, val_rouge_step=0.836, val_loss_epoch=4.050, val_rouge_epoch=0.821, train_loss_epoch=3.990]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Lightning checkpoint saved to fsdam-s3/logs/final_model_one_shot.ckpt\n",
      "INFO:__main__:LoRA weights saved to fsdam-s3/logs/lora_weights_one_shot\n",
      "INFO:__main__:One-shot training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4292"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import lightning.pytorch as L\n",
    "from torchmetrics.text import ROUGEScore\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import logging\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "file_handler = logging.FileHandler(\"fsdam-s3/logs/one_shot_training.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Set mixed precision\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Define paths and constants\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "DATASET_DIR = \"fsdam\"\n",
    "ONE_SHOT_DIR = os.path.join(DATASET_DIR, \"one_shot\")\n",
    "TEST_DIR = os.path.join(DATASET_DIR, \"test_set\")\n",
    "TRAIN_JSON_ONE = os.path.join(ONE_SHOT_DIR, \"train_llava.json\")\n",
    "VAL_JSON = os.path.join(ONE_SHOT_DIR, \"val_llava.json\")\n",
    "TEST_JSON = os.path.join(TEST_DIR, \"test_llava.json\")\n",
    "CHECKPOINT_DIR = \"fsdam-s3/logs\"\n",
    "LORA_CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, \"lora_weights_one_shot\")\n",
    "MAX_LENGTH = 2048\n",
    "\n",
    "# Create checkpoint directories\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LORA_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Verify paths\n",
    "logger.info(\"One-shot train JSON exists: %s\", os.path.exists(TRAIN_JSON_ONE))\n",
    "logger.info(\"Val JSON exists: %s\", os.path.exists(VAL_JSON))\n",
    "logger.info(\"Test JSON exists: %s\", os.path.exists(TEST_JSON))\n",
    "\n",
    "# Load processor and model\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    if processor.tokenizer.pad_token is None or processor.tokenizer.pad_token == \"<unk>\":\n",
    "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    logger.info(\"Pad token: %s\", processor.tokenizer.pad_token)\n",
    "\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_cfg,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading model or processor: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Configure LoRA with higher learning capacity for one-shot\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  #8\n",
    "    lora_alpha=4,  #4\n",
    "    lora_dropout=0.02,  # Reduced dropout #0.05\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"multi_modal_projector.linear_1\", \"multi_modal_projector.linear_2\"],\n",
    "    init_lora_weights=\"gaussian\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "logger.info(\"Model device: %s\", next(model.parameters()).device)\n",
    "logger.info(\"Model and processor loaded successfully.\")\n",
    "\n",
    "# Define dataset class\n",
    "class LlavaDataset(Dataset):\n",
    "    def __init__(self, json_path, image_dir, limit_samples=None):\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                self.data = json.load(f)\n",
    "                # For one-shot, limit samples but don't restrict to just 1\n",
    "                if limit_samples:\n",
    "                    self.data = self.data[:limit_samples]\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error loading JSON %s: %s\", json_path, e)\n",
    "            raise e\n",
    "        self.image_dir = image_dir\n",
    "        logger.info(\"Dataset from %s: %d samples\", json_path, len(self.data))\n",
    "        if len(self.data) > 0:\n",
    "            logger.info(\"Sample JSON entry: %s\", json.dumps(self.data[0], indent=2)[:500] + \"...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.data[idx]\n",
    "            image_path = os.path.join(self.image_dir, os.path.basename(item['image']))\n",
    "            \n",
    "            # Debug: Check if image file exists\n",
    "            if not os.path.exists(image_path):\n",
    "                logger.warning(\"Image not found: %s\", image_path)\n",
    "                # Try alternative path\n",
    "                image_path = os.path.join(self.image_dir, item['image'])\n",
    "                if not os.path.exists(image_path):\n",
    "                    raise FileNotFoundError(f\"Image not found: {image_path}\")\n",
    "            \n",
    "            if 'conversations' in item:\n",
    "                for conv in item['conversations']:\n",
    "                    if conv.get('from') == 'gpt':\n",
    "                        return image_path, conv['value']\n",
    "            if 'response' in item:\n",
    "                return image_path, item['response']\n",
    "            if 'answer' in item:\n",
    "                return image_path, item['answer']\n",
    "            raise ValueError(f\"No valid response found for item {idx}: {json.dumps(item, indent=2)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error accessing item %d: %s\", idx, e)\n",
    "            raise e\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    # For one-shot, use a small number of training examples (1-5)\n",
    "    train_dataset_one_shot = LlavaDataset(TRAIN_JSON_ONE, ONE_SHOT_DIR + \"/train\", limit_samples=1)\n",
    "    val_dataset = LlavaDataset(VAL_JSON, ONE_SHOT_DIR + \"/val\")\n",
    "    test_dataset = LlavaDataset(TEST_JSON, TEST_DIR + \"/test\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Dataset loading error: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Debug dataset sizes\n",
    "logger.info(\"One-shot train size: %d\", len(train_dataset_one_shot))\n",
    "logger.info(\"Val size: %d\", len(val_dataset))\n",
    "logger.info(\"Test size: %d\", len(test_dataset))\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch, processor=processor, prompt_template=None, device=\"cuda\"):\n",
    "    logger.info(\"Processing batch with %d samples\", len(batch))\n",
    "    imgs, seqs = zip(*batch)\n",
    "    processed_imgs = []\n",
    "    \n",
    "    for img_path in imgs:\n",
    "        logger.info(\"Loading image: %s\", img_path)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\").resize((336, 336))\n",
    "            processed_imgs.append(img)\n",
    "        except Exception as e:\n",
    "            logger.error(\"Error processing image %s: %s\", img_path, e)\n",
    "            raise e\n",
    "\n",
    "    prompt = prompt_template or \"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST] {seq}\"\n",
    "    conversation_texts = [prompt.format(seq=seq) for seq in seqs]\n",
    "\n",
    "    try:\n",
    "        inputs = processor(\n",
    "            text=conversation_texts,\n",
    "            images=processed_imgs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,  # Enable truncation\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        logger.info(\"Input shapes: input_ids=%s, pixel_values=%s\", inputs['input_ids'].shape, inputs['pixel_values'].shape)\n",
    "    except Exception as e:\n",
    "        logger.error(\"Processor error: %s\", e)\n",
    "        raise e\n",
    "\n",
    "    labels = inputs[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    inputs[\"labels\"] = labels\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    return inputs\n",
    "\n",
    "# Lightning module\n",
    "class LlavaModelPLModule(L.LightningModule):\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        self.rouge = ROUGEScore()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        logger.info(\"Training batch %d, size: %d\", batch_idx, len(batch['input_ids']))\n",
    "        outputs = self.model(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        # Log more details for one-shot learning\n",
    "        if batch_idx == 0:\n",
    "            predictions = self.processor.batch_decode(outputs.logits.argmax(-1), skip_special_tokens=True)\n",
    "            logger.info(\"Sample Training Prediction: %s...\", predictions[0][:200])\n",
    "            logger.info(\"Training loss: %f\", loss.item())\n",
    "            \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logger.info(\"Validation batch %d, size: %d\", batch_idx, len(batch['input_ids']))\n",
    "        \n",
    "        # Calculate validation loss first\n",
    "        with torch.no_grad():\n",
    "            loss_outputs = self.model(**batch)\n",
    "            val_loss = loss_outputs.loss\n",
    "            self.log(\"val_loss\", val_loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        # Then do generation for ROUGE score\n",
    "        generation_kwargs = {k: v for k, v in batch.items() if k != \"labels\"}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **generation_kwargs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.processor.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "        \n",
    "        predictions = self.processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "        valid_labels = batch[\"labels\"].clone()\n",
    "        valid_labels[valid_labels == -100] = self.processor.tokenizer.pad_token_id\n",
    "        targets = self.processor.tokenizer.batch_decode(valid_labels, skip_special_tokens=True)\n",
    "        targets = [t.replace(self.processor.tokenizer.pad_token, \"\").strip() for t in targets]\n",
    "        predictions = [p.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in p else p for p in predictions]\n",
    "        \n",
    "        # Check if predictions are empty\n",
    "        for i, pred in enumerate(predictions):\n",
    "            if not pred.strip():\n",
    "                logger.warning(\"Empty prediction at index %d\", i)\n",
    "                logger.info(\"Full output before processing: %s\", self.processor.batch_decode([outputs[i]], skip_special_tokens=True)[0])\n",
    "        \n",
    "        rouge_score = self.rouge(predictions, targets)[\"rougeL_fmeasure\"]\n",
    "        self.log(\"val_rouge\", rouge_score, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            logger.info(\"Sample Validation Prediction: %s...\", predictions[0][:200])\n",
    "            logger.info(\"Sample Validation Target: %s...\", targets[0][:200])\n",
    "            logger.info(\"Validation Loss: %f\", val_loss.item())\n",
    "            \n",
    "        # Save sample predictions\n",
    "        sample_output = [{\"prediction\": p, \"target\": t, \"val_loss\": val_loss.item()} for p, t in zip(predictions, targets)]\n",
    "        with open(os.path.join(CHECKPOINT_DIR, f\"val_samples_epoch_{self.current_epoch}.json\"), 'a') as f:\n",
    "            json.dump(sample_output, f, indent=2)\n",
    "            f.write('\\n')\n",
    "        return {\"val_rouge\": rouge_score, \"val_loss\": val_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Higher learning rate for one-shot learning\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config[\"lr\"], weight_decay=0.01)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config[\"max_epochs\"])\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        logger.info(\"Training dataset size: %d\", len(train_dataset_one_shot))\n",
    "        return DataLoader(\n",
    "            train_dataset_one_shot,\n",
    "            collate_fn=lambda x: collate_fn(x, processor),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        logger.info(\"Validation dataset size: %d\", len(val_dataset))\n",
    "        return DataLoader(\n",
    "            val_dataset,\n",
    "            collate_fn=lambda x: collate_fn(x, processor),\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        logger.info(\"Starting epoch %d\", self.current_epoch)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        save_path = os.path.join(CHECKPOINT_DIR, \"final_model_one_shot.ckpt\")\n",
    "        self.trainer.save_checkpoint(save_path)\n",
    "        logger.info(\"Lightning checkpoint saved to %s\", save_path)\n",
    "        self.model.save_pretrained(LORA_CHECKPOINT_DIR)\n",
    "        logger.info(\"LoRA weights saved to %s\", LORA_CHECKPOINT_DIR)\n",
    "\n",
    "# Training configuration optimized for one-shot learning\n",
    "config = {\n",
    "    \"max_epochs\": 20,  \n",
    "    \"lr\": 1e-3,  \n",
    "    \"batch_size\": 1, \n",
    "    \"use_contrastive\": False, \n",
    "    \"contrastive_weight\": 0.05\n",
    "}\n",
    "\n",
    "# Train with early stopping, checkpointing, and CSV logging\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    max_epochs=config[\"max_epochs\"],\n",
    "    precision=\"16-mixed\",\n",
    "    val_check_interval=0.5,\n",
    "    enable_checkpointing=True,\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1,\n",
    "    default_root_dir=CHECKPOINT_DIR,\n",
    "    logger=CSVLogger(save_dir=CHECKPOINT_DIR, name=\"metrics\", version=\"one_shot\"),\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5),\n",
    "        ModelCheckpoint(\n",
    "            dirpath=CHECKPOINT_DIR,\n",
    "            filename=\"best_model_one_shot_{epoch}_{val_loss:.3f}_{val_rouge:.3f}\",\n",
    "            monitor=\"val_loss\",  \n",
    "            mode=\"min\",\n",
    "            save_top_k=3  \n",
    "        )\n",
    "    ],\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "llava_module = LlavaModelPLModule(config, processor, model)\n",
    "trainer.fit(llava_module)\n",
    "logger.info(\"One-shot training completed.\")\n",
    "\n",
    "# Clean up memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a7c3fed-2fea-44fa-8b50-67289864e519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch  train_loss   val_loss\n",
      "0    0.0   15.448358  15.006376\n",
      "1    1.0   15.448358  15.006376\n",
      "2    2.0   15.448358  15.006376\n",
      "3    3.0   15.448358  15.006376\n",
      "4    4.0   15.448358   9.180565\n",
      "5    5.0    9.073468   6.490113\n",
      "6    6.0    6.544812   4.796289\n",
      "7    7.0    4.714968   4.357900\n",
      "8    8.0    4.349764   4.252538\n",
      "9    9.0    4.225982   4.232544\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('fsdam-s3/logs/metrics/one_shot/metrics.csv')\n",
    "\n",
    "# Create a clean dataframe with epoch, train, val loss \n",
    "train = df[['epoch', 'train_loss_epoch']].dropna().rename(columns={'train_loss_epoch': 'train_loss'})\n",
    "val = df[['epoch', 'val_loss_epoch']].dropna().rename(columns={'val_loss_epoch': 'val_loss'})\n",
    "\n",
    "# Merge on epoch\n",
    "merged = pd.merge(train, val, on='epoch', how='outer').sort_values('epoch')\n",
    "print(merged.head(10))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(merged['epoch'], merged['train_loss'], label='Train Loss (Epoch)')\n",
    "plt.plot(merged['epoch'], merged['val_loss'], label='Val Loss (Epoch)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss (Epoch)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cadf0-7ce4-45a6-b953-e232318f5462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "def zip_folder(folder_path, output_zip):\n",
    "    with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                # Add file with a relative path\n",
    "                abs_file = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(abs_file, start=folder_path)\n",
    "                zipf.write(abs_file, arcname=rel_path)\n",
    "\n",
    "# Usage\n",
    "folder_to_zip = \"fsdam-s3\"        \n",
    "output_zip = \"fsdam-s3.zip\"       \n",
    "\n",
    "zip_folder(folder_to_zip, output_zip)\n",
    "print(f\"Zipped {folder_to_zip} to {output_zip}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "130ca45a-0c18-4e6b-970f-21e44df7a1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Pad token: </s>\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:13<00:00,  4.37s/it]\n",
      "INFO:__main__:Model loaded from fsdam-s3/logs/lora_weights_one_shot and moved to cuda\n",
      "INFO:__main__:Model type: <class 'peft.peft_model.PeftModel'>\n",
      "INFO:__main__:Number of parameters: 3673622528\n",
      "INFO:__main__:Loaded test JSON with 81 entries\n",
      "INFO:__main__:Found 81 valid images out of 81 items\n",
      "INFO:__main__:Processing batch of 81 images\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene depicts a bustling city street lined with various buildings. The driver' s current gaz e i...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with a variety of vehicles, including cars and a truck. The traffic...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene depicts a suburban neighborhood with houses lining both sides of the street. The driver' s...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy highway with multiple lanes of traffic. The driver' s current gaz e is focuse...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a city street with trees lining the road. The driver'S current gaz e is on the road ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars and buildings. The driver' s current gaz e is ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple lanes of traffic. The driver' s current gaz e is fo...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars in motion. The driver' s current gaz e is focu...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy highway with multiple lanes of traffic. The driver' s current gaz e is focuse...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene depicts a bustling city street lined with tall buildings. The sun is shining brightly, cas...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with various vehicles, including cars and a truck. The driver' s cu...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with various vehicles such as cars, buses, and trucks. The driver...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street with tall buildings on either side. The sun is shining brightly, casting a wa...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with pedestrians and vehicles. The driver' s current gaz e is focus...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with parked cars on both sides. The driver' s current gaz e is focu...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a bustling city street with various vehicles, including cars and a bus. The driver' ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars in close proximity. The driver' s current gaz ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A busy city street is filled with various vehicles, including cars and a truck. The road is lined wi...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars and buildings. The driver' s current gaz e is ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene depicts a bustling city street at night. The driver' s current gaz e is focused on the roa...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a city street with parked cars on both sides. The driver' s current gaz e is focused...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A busy highway with multiple lanes filled with cars is visible. The current gazes are on the road ah...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars and trucks. The current gazes are on the road ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A city street lined with trees and buildings is visible. The driver' s current gaz e is on the road ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy highway with multiple cars driving in both directions. The current gazes of t...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A city street lined with trees and buildings is visible. The driver' s current gaz e is on the road ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a bustling city street with tall buildings on both sides. The driver' s current gaz ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars parked on both sides. The driver' s current ga...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy highway with multiple cars in different lanes. The current gaz e of the drive...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene takes place on a busy city street at night. The driver' s current gaz e is focused on the ...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is lined with tall buildings, creating a canyon-like effect. The sun shines b...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene depicts a bustling city street at night. The driver' s current gaz e is focused on the roa...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with various vehicles, including cars and a truck. The driver' s cu...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a residential street with several parked cars on one side and houses on the other. T...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a residential street with houses on both sides. The driver'S current gaz e is on the...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with traffic. The driver' s current gaz e is focused on the road ah...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a bustling city street with multiple vehicles, including a bus and a car. The curren...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: A bustling city street is filled with various vehicles, including cars and a truck. The driver' s cu...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a busy city street with multiple cars and buildings. The driver'S current gaz e is o...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([2, 1210]), pixel_values=torch.Size([2, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene shows a rainy day with a truck driving down a wet road. The current gazer is looking at th...\n",
      "INFO:__main__:Batch input shapes: input_ids=torch.Size([1, 1210]), pixel_values=torch.Size([1, 3, 3, 336, 336])\n",
      "INFO:__main__:Sample prediction: The scene depicts a bustling city street with tall buildings on either side. The driver' s current g...\n",
      "INFO:__main__:Predictions saved to fsdam-s3/logs/predictions_one_shot.json\n",
      "INFO:__main__:Average ROUGE-L: 0.322925\n",
      "INFO:__main__:Number of items with ROUGE scores: 81\n",
      "INFO:__main__:Generated 81 non-empty predictions out of 81 total\n",
      "INFO:__main__:Inference completed. 81 predictions generated.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "from peft import PeftModel\n",
    "from torchmetrics.text import ROUGEScore\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Define paths and constants\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "CHECKPOINT_DIR = \"fsdam-s3/logs\"\n",
    "LORA_CHECKPOINT_DIR = os.path.join(CHECKPOINT_DIR, \"lora_weights_one_shot\")\n",
    "TEST_JSON = \"fsdam/test_set/test_llava.json\"\n",
    "TEST_IMAGE_DIR = \"fsdam/test_set/test\"\n",
    "OUTPUT_JSON = \"fsdam-s3/logs/predictions_one_shot.json\"\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "# Load processor\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    if processor.tokenizer.pad_token is None or processor.tokenizer.pad_token == \"<unk>\":\n",
    "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    logger.info(\"Pad token: %s\", processor.tokenizer.pad_token)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading processor: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Load fine-tuned model\n",
    "try:\n",
    "    if not os.path.exists(LORA_CHECKPOINT_DIR):\n",
    "        raise FileNotFoundError(f\"LoRA checkpoint directory not found at {LORA_CHECKPOINT_DIR}\")\n",
    "    \n",
    "    # Check if adapter files exist\n",
    "    adapter_files = [\"adapter_config.json\", \"adapter_model.safetensors\"]\n",
    "    for file in adapter_files:\n",
    "        file_path = os.path.join(LORA_CHECKPOINT_DIR, file)\n",
    "        if not os.path.exists(file_path):\n",
    "            logger.warning(\"Adapter file not found: %s\", file_path)\n",
    "    \n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    base_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_cfg,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT model\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        LORA_CHECKPOINT_DIR,\n",
    "        is_trainable=False,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    logger.info(\"Model loaded from %s and moved to %s\", LORA_CHECKPOINT_DIR, device)\n",
    "    \n",
    "    # Debug: Print model info\n",
    "    logger.info(\"Model type: %s\", type(model))\n",
    "    logger.info(\"Number of parameters: %d\", sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading model: %s\", e)\n",
    "    logger.info(\"Attempting to load base model instead...\")\n",
    "    try:\n",
    "        # Fallback to base model if LoRA loading fails\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            MODEL_ID,\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=bnb_cfg,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        logger.info(\"Base model loaded successfully\")\n",
    "    except Exception as e2:\n",
    "        logger.error(\"Failed to load base model: %s\", e2)\n",
    "        raise e2\n",
    "\n",
    "# Enhanced batch inference function\n",
    "def predict_batch(image_paths, prompt=None, batch_size=2):\n",
    "    logger.info(\"Processing batch of %d images\", len(image_paths))\n",
    "    try:\n",
    "        prompt = prompt or \"[INST] <image>\\nDescribe the scene, the driver's current gaze, and where that gaze will likely shift next and why.[/INST]\"\n",
    "        predictions = []\n",
    "        \n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            batch_images = []\n",
    "            \n",
    "            # Process images with error handling\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    if not os.path.exists(path):\n",
    "                        logger.error(\"Image file not found: %s\", path)\n",
    "                        continue\n",
    "                    img = Image.open(path).convert(\"RGB\").resize((336, 336))\n",
    "                    batch_images.append(img)\n",
    "                except Exception as e:\n",
    "                    logger.error(\"Error loading image %s: %s\", path, e)\n",
    "                    continue\n",
    "            \n",
    "            if not batch_images:\n",
    "                logger.warning(\"No valid images in batch\")\n",
    "                predictions.extend([None] * len(batch_paths))\n",
    "                continue\n",
    "                \n",
    "            batch_prompts = [prompt] * len(batch_images)\n",
    "            \n",
    "            try:\n",
    "                inputs = processor(\n",
    "                    text=batch_prompts,\n",
    "                    images=batch_images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=2048\n",
    "                )\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                logger.info(\"Batch input shapes: input_ids=%s, pixel_values=%s\", \n",
    "                            inputs['input_ids'].shape, inputs['pixel_values'].shape)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                        min_length=10,\n",
    "                        repetition_penalty=1.1,\n",
    "                        no_repeat_ngram_size=3\n",
    "                    )\n",
    "                \n",
    "                batch_predictions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "                \n",
    "                # Better post-processing\n",
    "                processed_predictions = []\n",
    "                for pred in batch_predictions:\n",
    "                    if \"[/INST]\" in pred:\n",
    "                        pred = pred.split(\"[/INST]\")[-1].strip()\n",
    "                    else:\n",
    "                        # If no [/INST] found, try to extract meaningful content\n",
    "                        pred = pred.strip()\n",
    "                    \n",
    "                    # Check if prediction is empty or too short\n",
    "                    if len(pred.strip()) < 10:\n",
    "                        logger.warning(\"Short or empty prediction: '%s'\", pred)\n",
    "                        pred = \"Unable to generate meaningful description.\"\n",
    "                    \n",
    "                    processed_predictions.append(pred)\n",
    "                \n",
    "                predictions.extend(processed_predictions)\n",
    "                \n",
    "                # Log first prediction for debugging\n",
    "                if processed_predictions:\n",
    "                    logger.info(\"Sample prediction: %s...\", processed_predictions[0][:100])\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(\"Error in generation: %s\", e)\n",
    "                predictions.extend([None] * len(batch_images))\n",
    "            \n",
    "            torch.cuda.empty_cache()  # Clear memory after each batch\n",
    "            \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in batch inference: %s\", e)\n",
    "        return [None] * len(image_paths)\n",
    "\n",
    "# Enhanced batch inference on test dataset\n",
    "def run_batch_inference(json_path, image_dir):\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        logger.info(\"Loaded test JSON with %d entries\", len(test_data))\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading test JSON %s: %s\", json_path, e)\n",
    "        raise e\n",
    "\n",
    "    # Build image paths with better error handling\n",
    "    image_paths = []\n",
    "    valid_items = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        image_path = os.path.join(image_dir, os.path.basename(item['image']))\n",
    "        if not os.path.exists(image_path):\n",
    "            # Try alternative path\n",
    "            image_path = os.path.join(image_dir, item['image'])\n",
    "            if not os.path.exists(image_path):\n",
    "                logger.warning(\"Image not found for item %s: %s\", item.get('id', 'unknown'), image_path)\n",
    "                continue\n",
    "        \n",
    "        image_paths.append(image_path)\n",
    "        valid_items.append(item)\n",
    "    \n",
    "    logger.info(\"Found %d valid images out of %d items\", len(image_paths), len(test_data))\n",
    "    \n",
    "    predictions = predict_batch(image_paths, batch_size=2)\n",
    "    results = []\n",
    "    rouge = ROUGEScore()\n",
    "    rouge_scores = []\n",
    "\n",
    "    for item, prediction in zip(valid_items, predictions):\n",
    "        ground_truth = None\n",
    "        if 'conversations' in item:\n",
    "            for conv in item['conversations']:\n",
    "                if conv.get('from') == 'gpt':\n",
    "                    ground_truth = conv['value']\n",
    "                    break\n",
    "        elif 'response' in item:\n",
    "            ground_truth = item['response']\n",
    "        elif 'answer' in item:\n",
    "            ground_truth = item['answer']\n",
    "            \n",
    "        if ground_truth is None:\n",
    "            logger.warning(\"No ground truth found for item %s\", item.get('id', 'unknown'))\n",
    "            \n",
    "        if prediction is not None:\n",
    "            results.append({\n",
    "                \"id\": item.get('id', 'unknown'),\n",
    "                \"image\": item['image'],\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"prediction\": prediction\n",
    "            })\n",
    "            \n",
    "            if ground_truth and prediction.strip():\n",
    "                try:\n",
    "                    score = rouge([prediction], [ground_truth])['rougeL_fmeasure']\n",
    "                    rouge_scores.append(score)\n",
    "                except Exception as e:\n",
    "                    logger.warning(\"Error computing ROUGE score: %s\", e)\n",
    "        else:\n",
    "            logger.warning(\"Skipping item %s due to inference error\", item.get('id', 'unknown'))\n",
    "\n",
    "    # Save predictions\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "    with open(OUTPUT_JSON, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logger.info(\"Predictions saved to %s\", OUTPUT_JSON)\n",
    "\n",
    "    # Log ROUGE scores\n",
    "    if rouge_scores:\n",
    "        avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "        logger.info(\"Average ROUGE-L: %f\", avg_rouge)\n",
    "        logger.info(\"Number of items with ROUGE scores: %d\", len(rouge_scores))\n",
    "    else:\n",
    "        logger.info(\"No ROUGE scores computed (no ground truth available)\")\n",
    "\n",
    "    # Log prediction statistics\n",
    "    non_empty_predictions = [p for p in predictions if p and p.strip()]\n",
    "    logger.info(\"Generated %d non-empty predictions out of %d total\", len(non_empty_predictions), len(predictions))\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return results\n",
    "\n",
    "# Run inference\n",
    "if __name__ == \"__main__\":\n",
    "    predictions = run_batch_inference(TEST_JSON, TEST_IMAGE_DIR)\n",
    "    logger.info(\"Inference completed. %d predictions generated.\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed0cdc6-a838-449f-9308-22c26a116c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:absl:Using default tokenizer.\n",
      "Scoring predictions: 100%|██████████| 81/81 [00:00<00:00, 99.78it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FSDAM Test Metrics ====\n",
      "\n",
      "ROUGE-L        0.335669\n",
      "BLEU-4         0.081356\n",
      "METEOR         0.396457\n",
      "Exact Match    0.000000\n",
      "dtype: float64\n",
      "\n",
      "==== Per-sample Results (first 5) ====\n",
      "\n",
      "           id                               image  \\\n",
      "0  1003_00057  fsdam/test_set/test/1003_00057.png   \n",
      "1  1003_00195  fsdam/test_set/test/1003_00195.png   \n",
      "2  1003_00385  fsdam/test_set/test/1003_00385.png   \n",
      "3  1008_00038  fsdam/test_set/test/1008_00038.png   \n",
      "4  1008_00226  fsdam/test_set/test/1008_00226.png   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  The scene shows a city street with cars and bu...   \n",
      "1  The scene shows a city street with cars, build...   \n",
      "2  The scene shows a city intersection with traff...   \n",
      "3  A residential street with parked cars and buil...   \n",
      "4  The scene shows a residential street with hous...   \n",
      "\n",
      "                                          prediction   ROUGE-L    BLEU-4  \\\n",
      "0  The scene depicts a bustling city street lined...  0.413223  0.073151   \n",
      "1  The scene shows a busy city street with multip...  0.403226  0.092709   \n",
      "2  A bustling city street is filled with a variet...  0.300000  0.042361   \n",
      "3  The scene shows a city street with multiple ca...  0.320611  0.021494   \n",
      "4  The scene depicts a suburban neighborhood with...  0.346457  0.077152   \n",
      "\n",
      "     METEOR  Exact Match  \n",
      "0  0.454411            0  \n",
      "1  0.486308            0  \n",
      "2  0.414935            0  \n",
      "3  0.337834            0  \n",
      "4  0.488232            0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Download NLTK punkt tokenizer if not already installed\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Path to your JSON file\n",
    "PRED_PATH = 'fsdam-s3/logs/predictions_one_shot.json'\n",
    "\n",
    "# Load predictions\n",
    "with open(PRED_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "# Initialize scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for entry in tqdm(data, desc=\"Scoring predictions\"):\n",
    "    gt = entry['ground_truth'].strip()\n",
    "    pr = entry['prediction'].strip()\n",
    "\n",
    "    # ROUGE-L\n",
    "    rougeL = rouge.score(gt, pr)['rougeL'].fmeasure\n",
    "\n",
    "    # BLEU-4\n",
    "    bleu4 = sentence_bleu([word_tokenize(gt)], word_tokenize(pr), smoothing_function=smoothie)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = meteor_score([word_tokenize(gt)], word_tokenize(pr))\n",
    "    \n",
    "    # Exact Match\n",
    "    exact = int(gt == pr)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": entry['id'],\n",
    "        \"image\": entry['image'],\n",
    "        \"ground_truth\": gt,\n",
    "        \"prediction\": pr,\n",
    "        \"ROUGE-L\": rougeL,\n",
    "        \"BLEU-4\": bleu4,\n",
    "        \"METEOR\": meteor,\n",
    "        \"Exact Match\": exact\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print mean scores\n",
    "print(\"\\n==== FSDAM Test Metrics ====\\n\")\n",
    "print(df[[\"ROUGE-L\", \"BLEU-4\", \"METEOR\", \"Exact Match\"]].mean())\n",
    "print(\"\\n==== Per-sample Results (first 5) ====\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Save detailed results as CSV if needed\n",
    "df.to_csv('fsdam-s3/logs/test_metrics_one_shot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1847dd-8cde-4adf-9011-b7d9599babc1",
   "metadata": {},
   "source": [
    "# Zero shot LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6af97d1d-eade-4d01-ae30-32b7a22db521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Processor loaded successfully\n",
      "INFO:__main__:Pad token: <unk> (ID: 0)\n",
      "INFO:__main__:EOS token: </s> (ID: 2)\n",
      "INFO:__main__:Loading base LLaVA model for zero-shot inference...\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.30s/it]\n",
      "INFO:__main__:Base model loaded successfully\n",
      "INFO:__main__:Model type: <class 'transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration'>\n",
      "INFO:__main__:Number of parameters: 3663947776\n",
      "INFO:__main__:Starting zero-shot evaluation...\n",
      "INFO:__main__:=== TESTING MODEL BASIC FUNCTIONALITY ===\n",
      "INFO:__main__:Basic test response: Hello! I'd be happy to try. références de la dernière réponse :\n",
      "INFO:__main__:Loaded test JSON with 81 entries\n",
      "INFO:__main__:Found 81 valid images out of 81 items\n",
      "INFO:__main__:Processing all 81 images\n",
      "INFO:__main__:Starting zero-shot inference...\n",
      "INFO:__main__:Processing batch of 81 images with zero-shot inference\n",
      "INFO:__main__:Processing image 1/81: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__:=== DEBUGGING SINGLE INFERENCE ===\n",
      "INFO:__main__:Image path: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__:Prompt: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
      "Describe what you see in this driving scene. Focus on the driver's gaze direction and where they might look next and why. ASSISTANT:\n",
      "INFO:__main__:Original image size: (1280, 720)\n",
      "INFO:__main__:Input shapes:\n",
      "INFO:__main__:  input_ids: torch.Size([1, 2018])\n",
      "INFO:__main__:  attention_mask: torch.Size([1, 2018])\n",
      "INFO:__main__:  pixel_values: torch.Size([1, 5, 3, 336, 336])\n",
      "INFO:__main__:  image_sizes: torch.Size([1, 2])\n",
      "INFO:__main__:Starting generation...\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 2237])\n",
      "INFO:__main__:Input length: 2018, Output length: 2237\n",
      "INFO:__main__:Full decoded output: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: \n",
      "Describe what you see in this driving scene. Focus on the driver's gaze direction and where they might look next and why. ASSISTANT: In the image, the driver's gaze direction appears to be focused on the road ahead, with their attention directed towards the right side of the image. This suggests that the driver is likely looking at the lane ahead, possibly scanning for traffic, pedestrians, or other obstacles.\n",
      "\n",
      "The driver might look next at the traffic light, which is currently green, indicating that it is safe to proceed. If the driver is following traffic rules and regulations, they should not be looking directly at the traffic light, but rather keeping their eyes on the road ahead to anticipate any potential changes in the traffic situation.\n",
      "\n",
      "It's important for the driver to maintain situational awareness, which includes looking ahead to anticipate potential hazards, checking the mirrors to ensure they are aware of their surroundings, and being prepared to react to any changes in the road environment. The driver should also be mindful of other road users, such as other drivers, pedestrians, and cyclists, and adjust their driving accordingly.\n",
      "INFO:__main__:New tokens only: In the image, the driver's gaze direction appears to be focused on the road ahead, with their attention directed towards the right side of the image. This suggests that the driver is likely looking at the lane ahead, possibly scanning for traffic, pedestrians, or other obstacles.\n",
      "\n",
      "The driver might look next at the traffic light, which is currently green, indicating that it is safe to proceed. If the driver is following traffic rules and regulations, they should not be looking directly at the traffic light, but rather keeping their eyes on the road ahead to anticipate any potential changes in the traffic situation.\n",
      "\n",
      "It's important for the driver to maintain situational awareness, which includes looking ahead to anticipate potential hazards, checking the mirrors to ensure they are aware of their surroundings, and being prepared to react to any changes in the road environment. The driver should also be mindful of other road users, such as other drivers, pedestrians, and cyclists, and adjust their driving accordingly.\n",
      "INFO:__main__:Processing image 2/81: fsdam/test_set/test/1003_00195.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00195.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze appears to be directed to the right, as indicated by the direction o\n",
      "INFO:__main__:Processing image 3/81: fsdam/test_set/test/1003_00385.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00385.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a view from the driver's perspective in a busy urban street intersection. The d\n",
      "INFO:__main__:Processing image 4/81: fsdam/test_set/test/1008_00038.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1008_00038.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a street scene with several cars parked on the side of the road, and one car in\n",
      "INFO:__main__:Processing image 5/81: fsdam/test_set/test/1008_00226.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1008_00226.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, we see a view from the perspective of the driver of a car. The driver's gaze \n",
      "INFO:__main__:Processing image 6/81: fsdam/test_set/test/100_00266.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/100_00266.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze is directed to their left, where a car is parked on the right side o\n",
      "INFO:__main__:Processing image 7/81: fsdam/test_set/test/1013_00063.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1013_00063.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image shows a driving scene captured from a perspective that appears to be inside a car. The dri\n",
      "INFO:__main__:Processing image 8/81: fsdam/test_set/test/1026_00266.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1026_00266.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a busy city street scene from the perspective of a driver's viewpoint. The drive\n",
      "INFO:__main__:Processing image 9/81: fsdam/test_set/test/1038_00138.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1038_00138.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction suggests they are looking ahead towards the intersection. \n",
      "INFO:__main__:Processing image 10/81: fsdam/test_set/test/1047_00011.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1047_00011.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction is towards the traffic light, which is currently green. Th\n",
      "INFO:__main__:Processing image 11/81: fsdam/test_set/test/1051_00143.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1051_00143.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the driving scene, we see a view from the driver's perspective inside a vehicle. The driver appea\n",
      "INFO:__main__:Processing image 12/81: fsdam/test_set/test/1054_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1054_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction appears to be focused on the road ahead, specifically on t\n",
      "INFO:__main__:Processing image 13/81: fsdam/test_set/test/1054_00172.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1054_00172.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze is directed towards the left side of the road, likely obser\n",
      "INFO:__main__:Processing image 14/81: fsdam/test_set/test/1060_00104.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1060_00104.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze is directed towards the road ahead, specifically towards the car tha\n",
      "INFO:__main__:Processing image 15/81: fsdam/test_set/test/1072_00093.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1072_00093.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction is towards the right side of the frame, likely because the\n",
      "INFO:__main__:Processing image 16/81: fsdam/test_set/test/1078_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1078_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a view from inside a vehicle, likely from the perspective of a driver looking ou\n",
      "INFO:__main__:Processing image 17/81: fsdam/test_set/test/1078_00155.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1078_00155.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, there is a car driving on a road with other cars ahead of it. The driver's gaze direct\n",
      "INFO:__main__:Processing image 18/81: fsdam/test_set/test/1086_00109.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1086_00109.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a driver's perspective from inside a car, looking out towards a narrow, tree-li\n",
      "INFO:__main__:Processing image 19/81: fsdam/test_set/test/1087_00482.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1087_00482.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze is directed towards the right side of the road, likely observing the\n",
      "INFO:__main__:Processing image 20/81: fsdam/test_set/test/108_00113.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/108_00113.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image shows a scene from the perspective of the driver inside a vehicle, looking out towards the\n",
      "INFO:__main__:Processing image 21/81: fsdam/test_set/test/1114_00066.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1114_00066.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the perspective is from the driver's viewpoint inside a car. The driver's gaz\n",
      "INFO:__main__:Processing image 22/81: fsdam/test_set/test/1122_00207.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1122_00207.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the focus is on the driver's gaze direction. The driver appears to be looking\n",
      "INFO:__main__:Processing image 23/81: fsdam/test_set/test/1123_00171.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1123_00171.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image shows a bustling city street with a variety of vehicles. The driver of the car in front ap\n",
      "INFO:__main__:Processing image 24/81: fsdam/test_set/test/1125_00312.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1125_00312.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, there is a black SUV stopped at a red traffic light. The driver appears to be looking \n",
      "INFO:__main__:Processing image 25/81: fsdam/test_set/test/112_00138.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/112_00138.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze direction appears to be focused on the traffic lights ahead\n",
      "INFO:__main__:Processing image 26/81: fsdam/test_set/test/112_00278.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/112_00278.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a street view from the perspective of a driver. The driver appears to be focuse\n",
      "INFO:__main__:Processing image 27/81: fsdam/test_set/test/1132_00247.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1132_00247.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a bustling city street scene. The driver's gaze is directed towards the right s\n",
      "INFO:__main__:Processing image 28/81: fsdam/test_set/test/1137_00245.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1137_00245.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a view from the perspective of a vehicle driving down a street in a residential\n",
      "INFO:__main__:Processing image 29/81: fsdam/test_set/test/1138_00081.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1138_00081.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a street scene with multiple cars parked on the side of the road, and a few car\n",
      "INFO:__main__:Processing image 30/81: fsdam/test_set/test/1141_00056.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1141_00056.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image captures a moment from the perspective of a car driver. The driver appears to be looking a\n",
      "INFO:__main__:Processing image 31/81: fsdam/test_set/test/1146_00099.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1146_00099.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a view from inside a vehicle, looking out towards the road ahead. The driver's \n",
      "INFO:__main__:Processing image 32/81: fsdam/test_set/test/1151_00193.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1151_00193.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a street scene with the sun shining brightly, creating a warm and inviting atmo\n",
      "INFO:__main__:Processing image 33/81: fsdam/test_set/test/115_00168.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/115_00168.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a view from the perspective of a driver's side mirror, capturing a busy city st\n",
      "INFO:__main__:Processing image 34/81: fsdam/test_set/test/1165_00111.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1165_00111.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a driving scene from the perspective of a car driver. The driver appears to be \n",
      "INFO:__main__:Processing image 35/81: fsdam/test_set/test/1167_00169.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1167_00169.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction appears to be looking towards the stop sign on the right s\n",
      "INFO:__main__:Processing image 36/81: fsdam/test_set/test/116_00263.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/116_00263.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze direction is towards the right side of the image. They are \n",
      "INFO:__main__:Processing image 37/81: fsdam/test_set/test/1180_00007.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1180_00007.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, we see a city street lined with buildings. The driver's gaze is directed towa\n",
      "INFO:__main__:Processing image 38/81: fsdam/test_set/test/1180_00150.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1180_00150.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the driving scene, there is a clear view of the road ahead, with various vehicles including a red\n",
      "INFO:__main__:Processing image 39/81: fsdam/test_set/test/1191_00079.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1191_00079.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a night-time city street scene. The driver of the vehicle appears to be looking\n",
      "INFO:__main__:Processing image 40/81: fsdam/test_set/test/1203_00057.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1203_00057.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction seems to be focused on the stop sign in the distance, whic\n",
      "INFO:__main__:Processing image 41/81: fsdam/test_set/test/1203_00197.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1203_00197.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze is directed to the right, likely observing a traffic light or road s\n",
      "INFO:__main__:Processing image 42/81: fsdam/test_set/test/1213_00131.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1213_00131.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, I see a view from the perspective of a driver on a road. The driver's gaze di\n",
      "INFO:__main__:Processing image 43/81: fsdam/test_set/test/1215_00018.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1215_00018.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a driver's view from inside a vehicle, likely taken from the driver's seat. The\n",
      "INFO:__main__:Processing image 44/81: fsdam/test_set/test/1216_00346.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1216_00346.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a view from the perspective of a driver looking out through the windshield of a \n",
      "INFO:__main__:Processing image 45/81: fsdam/test_set/test/121_00085.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/121_00085.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, I can see a car that appears to be moving forward on a street with other vehi\n",
      "INFO:__main__:Processing image 46/81: fsdam/test_set/test/121_00256.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/121_00256.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze direction is towards the right side of the image. This sugg\n",
      "INFO:__main__:Processing image 47/81: fsdam/test_set/test/1249_00159.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1249_00159.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction appears to be to the right, likely towards the road ahead.\n",
      "INFO:__main__:Processing image 48/81: fsdam/test_set/test/1250_00133.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1250_00133.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze direction is to the right, following the path of the street\n",
      "INFO:__main__:Processing image 49/81: fsdam/test_set/test/1251_00049.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1251_00049.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction appears to be towards the right, as indicated by the line \n",
      "INFO:__main__:Processing image 50/81: fsdam/test_set/test/1257_00152.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1257_00152.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the driving scene captured in the image, the driver's gaze direction appears to be focused toward\n",
      "INFO:__main__:Processing image 51/81: fsdam/test_set/test/1261_00188.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1261_00188.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the driving scene, the perspective is from the inside of a vehicle, likely from the driver's seat\n",
      "INFO:__main__:Processing image 52/81: fsdam/test_set/test/1266_00123.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1266_00123.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a public transit bus driving down a street lined with trees and buildings. The b\n",
      "INFO:__main__:Processing image 53/81: fsdam/test_set/test/128_00159.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/128_00159.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a street scene from the perspective of a driver inside a vehicle, likely a car,\n",
      "INFO:__main__:Processing image 54/81: fsdam/test_set/test/128_00215.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/128_00215.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a car approaching a stop sign at an intersection. The driver appears to be look\n",
      "INFO:__main__:Processing image 55/81: fsdam/test_set/test/1300_00162.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1300_00162.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a city street scene viewed from the perspective of a car window. The driver's g\n",
      "INFO:__main__:Processing image 56/81: fsdam/test_set/test/1307_00069.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1307_00069.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image shows a view from inside a car, looking out through the windshield onto a busy street. The\n",
      "INFO:__main__:Processing image 57/81: fsdam/test_set/test/132_00152.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/132_00152.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a car's windshield from the perspective of the driver. The driver's gaze directi\n",
      "INFO:__main__:Processing image 58/81: fsdam/test_set/test/1336_00153.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1336_00153.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze is directed straight ahead, which is typical for a driver approachin\n",
      "INFO:__main__:Processing image 59/81: fsdam/test_set/test/1355_00105.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1355_00105.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction is towards the right side of the road, where there are sev\n",
      "INFO:__main__:Processing image 60/81: fsdam/test_set/test/1358_00053.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1358_00053.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a view from the driver's perspective inside a car, looking out towards a street \n",
      "INFO:__main__:Processing image 61/81: fsdam/test_set/test/135_00092.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/135_00092.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, I see a colorful trolley car, which appears to be a tourist attraction, trave\n",
      "INFO:__main__:Processing image 62/81: fsdam/test_set/test/135_00216.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/135_00216.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a vibrant street scene with a classic trolley car in the center of the frame, dr\n",
      "INFO:__main__:Processing image 63/81: fsdam/test_set/test/1368_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1368_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a driving scene at night on a city street. The driver's gaze direction is toward\n",
      "INFO:__main__:Processing image 64/81: fsdam/test_set/test/1374_00009.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1374_00009.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction appears to be focused on the traffic lights ahead. The dri\n",
      "INFO:__main__:Processing image 65/81: fsdam/test_set/test/1378_00064.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1378_00064.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver appears to be looking to their right, possibly at a traffic light or a road\n",
      "INFO:__main__:Processing image 66/81: fsdam/test_set/test/1386_00222.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1386_00222.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze is directed to the right side of the road. It's likely that\n",
      "INFO:__main__:Processing image 67/81: fsdam/test_set/test/138_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/138_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze direction suggests they are looking to the right, possibly \n",
      "INFO:__main__:Processing image 68/81: fsdam/test_set/test/138_00074.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/138_00074.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a residential street scene with various vehicles parked on the side of the road\n",
      "INFO:__main__:Processing image 69/81: fsdam/test_set/test/1390_00211.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1390_00211.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a scene of a residential street with parked cars on the side of the road. The d\n",
      "INFO:__main__:Processing image 70/81: fsdam/test_set/test/1402_00128.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1402_00128.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the driving scene, the driver appears to be looking to their right, likely focusing on the road a\n",
      "INFO:__main__:Processing image 71/81: fsdam/test_set/test/1402_00350.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1402_00350.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, a car is driving on a city street with traffic. The driver's gaze is directed straight\n",
      "INFO:__main__:Processing image 72/81: fsdam/test_set/test/1403_00122.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1403_00122.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver appears to be looking ahead, likely focusing on the road and traff\n",
      "INFO:__main__:Processing image 73/81: fsdam/test_set/test/1404_00129.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1404_00129.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image shows a view from the driver's perspective inside a car. The driver's gaze direction is to\n",
      "INFO:__main__:Processing image 74/81: fsdam/test_set/test/1405_00109.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1405_00109.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze direction is to the right, looking at the intersection ahead. The dr\n",
      "INFO:__main__:Processing image 75/81: fsdam/test_set/test/1406_00285.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1406_00285.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver is approaching an intersection. They are looking to the right, lik\n",
      "INFO:__main__:Processing image 76/81: fsdam/test_set/test/1420_00121.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1420_00121.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze direction appears to be focused on the road ahead, likely l\n",
      "INFO:__main__:Processing image 77/81: fsdam/test_set/test/1425_00102.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1425_00102.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In this driving scene, the driver's gaze is directed towards the right side of the road, which is li\n",
      "INFO:__main__:Processing image 78/81: fsdam/test_set/test/1425_00230.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1425_00230.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, we see a perspective from the driver's seat of a vehicle looking towards a traffic lig\n",
      "INFO:__main__:Processing image 79/81: fsdam/test_set/test/1436_00095.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1436_00095.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: The image shows a view from inside a vehicle, looking out onto a wet street. The perspective is from\n",
      "INFO:__main__:Processing image 80/81: fsdam/test_set/test/1452_00066.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1452_00066.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, I see a busy city street with a variety of vehicles, including a white transit bus in \n",
      "INFO:__main__:Processing image 81/81: fsdam/test_set/test/1458_00260.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1458_00260.png, size: (1280, 720)\n",
      "INFO:__main__:Generated prediction: In the image, the driver's gaze appears to be directed to the left, likely scanning for traffic or p\n",
      "INFO:__main__:ROUGE score for 1003_00057: 0.1751\n",
      "INFO:__main__:ROUGE score for 1003_00195: 0.1526\n",
      "INFO:__main__:ROUGE score for 1003_00385: 0.1255\n",
      "INFO:__main__:ROUGE score for 1008_00038: 0.1610\n",
      "INFO:__main__:ROUGE score for 1008_00226: 0.1322\n",
      "INFO:__main__:ROUGE score for 100_00266: 0.1371\n",
      "INFO:__main__:ROUGE score for 1013_00063: 0.1680\n",
      "INFO:__main__:ROUGE score for 1026_00266: 0.1964\n",
      "INFO:__main__:ROUGE score for 1038_00138: 0.1951\n",
      "INFO:__main__:ROUGE score for 1047_00011: 0.1553\n",
      "INFO:__main__:ROUGE score for 1051_00143: 0.1478\n",
      "INFO:__main__:ROUGE score for 1054_00005: 0.1707\n",
      "INFO:__main__:ROUGE score for 1054_00172: 0.1744\n",
      "INFO:__main__:ROUGE score for 1060_00104: 0.1246\n",
      "INFO:__main__:ROUGE score for 1072_00093: 0.1974\n",
      "INFO:__main__:ROUGE score for 1078_00005: 0.1511\n",
      "INFO:__main__:ROUGE score for 1078_00155: 0.1887\n",
      "INFO:__main__:ROUGE score for 1086_00109: 0.1544\n",
      "INFO:__main__:ROUGE score for 1087_00482: 0.1970\n",
      "INFO:__main__:ROUGE score for 108_00113: 0.1583\n",
      "INFO:__main__:ROUGE score for 1114_00066: 0.1623\n",
      "INFO:__main__:ROUGE score for 1122_00207: 0.2010\n",
      "INFO:__main__:ROUGE score for 1123_00171: 0.1762\n",
      "INFO:__main__:ROUGE score for 1125_00312: 0.1417\n",
      "INFO:__main__:ROUGE score for 112_00138: 0.1351\n",
      "INFO:__main__:ROUGE score for 112_00278: 0.1259\n",
      "INFO:__main__:ROUGE score for 1132_00247: 0.1801\n",
      "INFO:__main__:ROUGE score for 1137_00245: 0.1233\n",
      "INFO:__main__:ROUGE score for 1138_00081: 0.1958\n",
      "INFO:__main__:ROUGE score for 1141_00056: 0.1439\n",
      "INFO:__main__:ROUGE score for 1146_00099: 0.1529\n",
      "INFO:__main__:ROUGE score for 1151_00193: 0.2093\n",
      "INFO:__main__:ROUGE score for 115_00168: 0.1731\n",
      "INFO:__main__:ROUGE score for 1165_00111: 0.1523\n",
      "INFO:__main__:ROUGE score for 1167_00169: 0.1572\n",
      "INFO:__main__:ROUGE score for 116_00263: 0.1235\n",
      "INFO:__main__:ROUGE score for 1180_00007: 0.2720\n",
      "INFO:__main__:ROUGE score for 1180_00150: 0.1925\n",
      "INFO:__main__:ROUGE score for 1191_00079: 0.1622\n",
      "INFO:__main__:ROUGE score for 1203_00057: 0.1607\n",
      "INFO:__main__:ROUGE score for 1203_00197: 0.1831\n",
      "INFO:__main__:ROUGE score for 1213_00131: 0.1301\n",
      "INFO:__main__:ROUGE score for 1215_00018: 0.1514\n",
      "INFO:__main__:ROUGE score for 1216_00346: 0.1556\n",
      "INFO:__main__:ROUGE score for 121_00085: 0.1532\n",
      "INFO:__main__:ROUGE score for 121_00256: 0.1407\n",
      "INFO:__main__:ROUGE score for 1249_00159: 0.1103\n",
      "INFO:__main__:ROUGE score for 1250_00133: 0.1257\n",
      "INFO:__main__:ROUGE score for 1251_00049: 0.1702\n",
      "INFO:__main__:ROUGE score for 1257_00152: 0.2362\n",
      "INFO:__main__:ROUGE score for 1261_00188: 0.1782\n",
      "INFO:__main__:ROUGE score for 1266_00123: 0.1399\n",
      "INFO:__main__:ROUGE score for 128_00159: 0.1504\n",
      "INFO:__main__:ROUGE score for 128_00215: 0.1532\n",
      "INFO:__main__:ROUGE score for 1300_00162: 0.1724\n",
      "INFO:__main__:ROUGE score for 1307_00069: 0.1429\n",
      "INFO:__main__:ROUGE score for 132_00152: 0.1985\n",
      "INFO:__main__:ROUGE score for 1336_00153: 0.1299\n",
      "INFO:__main__:ROUGE score for 1355_00105: 0.0719\n",
      "INFO:__main__:ROUGE score for 1358_00053: 0.1476\n",
      "INFO:__main__:ROUGE score for 135_00092: 0.1869\n",
      "INFO:__main__:ROUGE score for 135_00216: 0.1507\n",
      "INFO:__main__:ROUGE score for 1368_00005: 0.1684\n",
      "INFO:__main__:ROUGE score for 1374_00009: 0.1951\n",
      "INFO:__main__:ROUGE score for 1378_00064: 0.1244\n",
      "INFO:__main__:ROUGE score for 1386_00222: 0.2158\n",
      "INFO:__main__:ROUGE score for 138_00005: 0.1116\n",
      "INFO:__main__:ROUGE score for 138_00074: 0.1484\n",
      "INFO:__main__:ROUGE score for 1390_00211: 0.2443\n",
      "INFO:__main__:ROUGE score for 1402_00128: 0.1988\n",
      "INFO:__main__:ROUGE score for 1402_00350: 0.1595\n",
      "INFO:__main__:ROUGE score for 1403_00122: 0.1341\n",
      "INFO:__main__:ROUGE score for 1404_00129: 0.1315\n",
      "INFO:__main__:ROUGE score for 1405_00109: 0.1935\n",
      "INFO:__main__:ROUGE score for 1406_00285: 0.1284\n",
      "INFO:__main__:ROUGE score for 1420_00121: 0.0990\n",
      "INFO:__main__:ROUGE score for 1425_00102: 0.1854\n",
      "INFO:__main__:ROUGE score for 1425_00230: 0.1387\n",
      "INFO:__main__:ROUGE score for 1436_00095: 0.1788\n",
      "INFO:__main__:ROUGE score for 1452_00066: 0.1917\n",
      "INFO:__main__:ROUGE score for 1458_00260: 0.1325\n",
      "INFO:__main__:Zero-shot predictions saved to fsdam-s3/logs/predictions_zero_shot.json\n",
      "INFO:__main__:=== ZERO-SHOT EVALUATION RESULTS ===\n",
      "INFO:__main__:Average ROUGE-L: 0.1613\n",
      "INFO:__main__:Max ROUGE-L: 0.2720\n",
      "INFO:__main__:Min ROUGE-L: 0.0719\n",
      "INFO:__main__:Number of items with ROUGE scores: 81\n",
      "INFO:__main__:Generated 81 non-empty predictions out of 81 total\n",
      "INFO:__main__:=== SAMPLE ZERO-SHOT PREDICTIONS ===\n",
      "INFO:__main__:Sample 1:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__: Prediction: In the image, the driver's gaze direction appears to be focused on the road ahead, with their attention directed towards the right side of the image. This suggests that the driver is likely looking at the lane ahead, possibly scanning for traffic, pedestrians, or other obstacles.\n",
      "\n",
      "The driver might look next at the traffic light, which is currently green, indicating that it is safe to proceed. If the driver is following traffic rules and regulations, they should not be looking directly at the traffic light, but rather keeping their eyes on the road ahead to anticipate any potential changes in the traffic situation.\n",
      "\n",
      "It's important for the driver to maintain situational awareness, which includes looking ahead to anticipate potential hazards, checking the mirrors to ensure they are aware of their surroundings, and being prepared to react to any changes in the road environment. The driver should also be mindful of other road users, such as other drivers, pedestrians, and cyclists, and adjust their driving accordingly.\n",
      "INFO:__main__: Ground Truth: The scene shows a city street with cars and buildings. The current gaze focuses on the blue car ahead. The future gaze will likely remain on the blue car. This is because the driver needs to maintain \n",
      "INFO:__main__:Sample 2:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00195.png\n",
      "INFO:__main__: Prediction: In the image, the driver's gaze appears to be directed to the right, as indicated by the direction of the car's movement on the road. This could suggest that the driver is checking for oncoming traffic, pedestrians, or other vehicles that might be entering their lane from the right. The driver might be anticipating an upcoming traffic signal or an intersection where they need to be prepared to react to the changing traffic conditions.\n",
      "\n",
      "The driver might look next at the traffic signal to determine when it will change, as the current signal is red, which could mean the driver will need to stop or prepare to stop depending on the traffic situation. They may also be looking for any pedestrians or cyclists who might be crossing the road or about to cross. Additionally, the driver might be scanning for any potential obstacles, such as parked cars or vehicles that might be pulling out from a side road or parking lot.\n",
      "\n",
      "In general, the driver's gaze is likely focused on staying alert to potential hazards and anticipating the actions of other road users to ensure a safe journey.\n",
      "INFO:__main__: Ground Truth: The scene shows a city street with cars, buildings, and a KFC. The current gaze is focused on the blue car ahead. The future gaze will likely shift to the red traffic light. This is because the driver\n",
      "INFO:__main__:Sample 3:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00385.png\n",
      "INFO:__main__: Prediction: In the image, we see a view from the driver's perspective in a busy urban street intersection. The driver's gaze is directed to the left, likely due to the traffic light signaling a green light, which would typically cause drivers to proceed forward. The driver is also looking to the right, likely anticipating the actions of other vehicles and pedestrians at the intersection. This rightward gaze is a common practice for drivers to ensure their safety and the safety of others, especially in busy intersections.\n",
      "\n",
      "The driver might look next to the traffic lights, which are showing red for oncoming traffic, to ensure that they are following the correct signal. The driver might also be scanning the traffic around them, looking for any vehicles or pedestrians that may be crossing the intersection. Additionally, the driver might be looking ahead to assess the flow of traffic and anticipate any potential changes in traffic conditions.\n",
      "\n",
      "The reason for these gazes is to ensure that the driver is adhering to traffic rules, staying alert to the surroundings, and maintaining a safe driving environment. This is a typical driving behavior to prevent accidents and ensure smooth traffic flow in a busy urban setting.\n",
      "INFO:__main__: Ground Truth: The scene shows a city intersection with traffic and buildings. The current gaze focuses on the red traffic light. The future gaze will likely shift to the green light. This is because the light chang\n",
      "INFO:__main__:Zero-shot evaluation completed. 81 predictions generated.\n",
      "INFO:__main__:Results saved to: fsdam-s3/logs/predictions_zero_shot.json\n",
      "INFO:__main__:Check logs at: fsdam-s3/logs/zero_shot_inference.log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "from torchmetrics.text import ROUGEScore\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "file_handler = logging.FileHandler(\"fsdam-s3/logs/zero_shot_inference.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Define paths and constants\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "TEST_JSON = \"fsdam/test_set/test_llava.json\"\n",
    "TEST_IMAGE_DIR = \"fsdam/test_set/test\"\n",
    "OUTPUT_JSON = \"fsdam-s3/logs/predictions_zero_shot.json\"\n",
    "LOGS_DIR = \"fsdam-s3/logs\"\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Load processor\n",
    "try:\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    \n",
    "    # Fix tokenizer padding configuration\n",
    "    if processor.tokenizer.pad_token is None:\n",
    "        processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "        processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id\n",
    "    \n",
    "    # Set padding side to left for generation tasks\n",
    "    processor.tokenizer.padding_side = \"left\"\n",
    "    \n",
    "    logger.info(\"Processor loaded successfully\")\n",
    "    logger.info(\"Pad token: %s (ID: %d)\", processor.tokenizer.pad_token, processor.tokenizer.pad_token_id)\n",
    "    logger.info(\"EOS token: %s (ID: %d)\", processor.tokenizer.eos_token, processor.tokenizer.eos_token_id)\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading processor: %s\", e)\n",
    "    raise e\n",
    "\n",
    "# Load base model (no fine-tuning)\n",
    "try:\n",
    "    logger.info(\"Loading base LLaVA model for zero-shot inference...\")\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_cfg,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"  # Let transformers handle device placement\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    logger.info(\"Base model loaded successfully\")\n",
    "    logger.info(\"Model type: %s\", type(model))\n",
    "    logger.info(\"Number of parameters: %d\", sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading base model: %s\", e)\n",
    "    raise e\n",
    "\n",
    "def debug_single_inference(image_path, prompt):\n",
    "    \"\"\"\n",
    "    Debug function to test single image inference\n",
    "    \"\"\"\n",
    "    logger.info(\"=== DEBUGGING SINGLE INFERENCE ===\")\n",
    "    logger.info(\"Image path: %s\", image_path)\n",
    "    logger.info(\"Prompt: %s\", prompt)\n",
    "    \n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        logger.info(\"Original image size: %s\", image.size)\n",
    "        \n",
    "        # Don't resize - let the processor handle it\n",
    "        inputs = processor(\n",
    "            text=prompt,\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        logger.info(\"Input shapes:\")\n",
    "        for k, v in inputs.items():\n",
    "            logger.info(\"  %s: %s\", k, v.shape)\n",
    "        \n",
    "        # Generate with detailed logging\n",
    "        logger.info(\"Starting generation...\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=True,  # Try sampling instead of greedy\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                use_cache=True\n",
    "            )\n",
    "        \n",
    "        logger.info(\"Generation completed. Output shape: %s\", outputs.shape)\n",
    "        logger.info(\"Input length: %d, Output length: %d\", \n",
    "                   inputs['input_ids'].shape[1], outputs.shape[1])\n",
    "        \n",
    "        # Decode full output\n",
    "        full_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        logger.info(\"Full decoded output: %s\", full_output)\n",
    "        \n",
    "        # Decode only new tokens\n",
    "        new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "        new_output = processor.decode(new_tokens, skip_special_tokens=True)\n",
    "        logger.info(\"New tokens only: %s\", new_output)\n",
    "        \n",
    "        return new_output.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in debug inference: %s\", e)\n",
    "        import traceback\n",
    "        logger.error(\"Traceback: %s\", traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Zero-shot batch inference function\n",
    "def predict_batch_zero_shot(image_paths, prompt=None, batch_size=1):  # Reduced batch size\n",
    "    \"\"\"\n",
    "    Zero-shot batch inference using the base LLaVA model\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing batch of %d images with zero-shot inference\", len(image_paths))\n",
    "    \n",
    "   \n",
    "    if prompt is None:\n",
    "        prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\\nDescribe what you see in this driving scene. Focus on the driver's gaze direction and where they might look next and why. ASSISTANT:\"\n",
    "    \n",
    "    try:\n",
    "        predictions = []\n",
    "        \n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            logger.info(\"Processing image %d/%d: %s\", i+1, len(image_paths), image_path)\n",
    "            \n",
    "            # Debug first image in detail\n",
    "            if i == 0:\n",
    "                debug_result = debug_single_inference(image_path, prompt)\n",
    "                if debug_result:\n",
    "                    predictions.append(debug_result)\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                if not os.path.exists(image_path):\n",
    "                    logger.error(\"Image file not found: %s\", image_path)\n",
    "                    predictions.append(\"Image file not found.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load image\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                logger.info(\"Loaded image: %s, size: %s\", image_path, image.size)\n",
    "                \n",
    "                # Process inputs\n",
    "                inputs = processor(\n",
    "                    text=prompt,\n",
    "                    images=image,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                )\n",
    "                \n",
    "                # Move to device\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                # Generate\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=MAX_NEW_TOKENS,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.9,\n",
    "                        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                \n",
    "                # Decode only the new tokens\n",
    "                input_length = inputs['input_ids'].shape[1]\n",
    "                new_tokens = outputs[0][input_length:]\n",
    "                prediction = processor.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "                \n",
    "                logger.info(\"Generated prediction: %s\", prediction[:100])\n",
    "                \n",
    "                if len(prediction.strip()) < 5:\n",
    "                    logger.warning(\"Short prediction, trying alternative decoding...\")\n",
    "                    # Try decoding the full output and extract after ASSISTANT:\n",
    "                    full_output = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                    if \"ASSISTANT:\" in full_output:\n",
    "                        prediction = full_output.split(\"ASSISTANT:\")[-1].strip()\n",
    "                    elif \"assistant\" in full_output.lower():\n",
    "                        parts = full_output.lower().split(\"assistant\")\n",
    "                        if len(parts) > 1:\n",
    "                            prediction = full_output[full_output.lower().rfind(\"assistant\") + len(\"assistant\"):].strip()\n",
    "                \n",
    "                if len(prediction.strip()) < 5:\n",
    "                    prediction = \"Unable to generate description for this driving scene.\"\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(\"Error processing image %s: %s\", image_path, e)\n",
    "                predictions.append(\"Error processing image.\")\n",
    "            \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in zero-shot batch inference: %s\", e)\n",
    "        return [\"Error in inference\"] * len(image_paths)\n",
    "\n",
    "\n",
    "def test_model_basic():\n",
    "    \"\"\"\n",
    "    Test the model with a simple prompt to verify it's working\n",
    "    \"\"\"\n",
    "    logger.info(\"=== TESTING MODEL BASIC FUNCTIONALITY ===\")\n",
    "    \n",
    "    # Create a simple test\n",
    "    test_prompt = \"USER: Hello, can you help me? ASSISTANT:\"\n",
    "    \n",
    "    try:\n",
    "        inputs = processor(\n",
    "            text=test_prompt,\n",
    "            images=None,  \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "        new_tokens = outputs[0][input_length:]\n",
    "        response = processor.decode(new_tokens, skip_special_tokens=True)\n",
    "        \n",
    "        logger.info(\"Basic test response: %s\", response)\n",
    "        return len(response.strip()) > 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Basic test failed: %s\", e)\n",
    "        return False\n",
    "\n",
    "def run_zero_shot_evaluation(json_path, image_dir):\n",
    "    \"\"\"\n",
    "    Run zero-shot evaluation on the test dataset\n",
    "    \"\"\"\n",
    "    # First test basic model functionality\n",
    "    if not test_model_basic():\n",
    "        logger.error(\"Basic model test failed. Check model loading.\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        logger.info(\"Loaded test JSON with %d entries\", len(test_data))\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading test JSON %s: %s\", json_path, e)\n",
    "        raise e\n",
    "\n",
    "    # Build image paths with error handling\n",
    "    image_paths = []\n",
    "    valid_items = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        image_path = os.path.join(image_dir, os.path.basename(item['image']))\n",
    "        if not os.path.exists(image_path):\n",
    "            # Try alternative path\n",
    "            image_path = os.path.join(image_dir, item['image'])\n",
    "            if not os.path.exists(image_path):\n",
    "                logger.warning(\"Image not found for item %s: %s\", item.get('id', 'unknown'), image_path)\n",
    "                continue\n",
    "        \n",
    "        image_paths.append(image_path)\n",
    "        valid_items.append(item)\n",
    "    \n",
    "    logger.info(\"Found %d valid images out of %d items\", len(image_paths), len(test_data))\n",
    "    \n",
    "    # Process all images (remove debugging limitation)\n",
    "    logger.info(\"Processing all %d images\", len(image_paths))\n",
    "    \n",
    "    # Run zero-shot predictions\n",
    "    logger.info(\"Starting zero-shot inference...\")\n",
    "    predictions = predict_batch_zero_shot(image_paths, batch_size=1)\n",
    "    \n",
    "    # Process results and calculate metrics\n",
    "    results = []\n",
    "    rouge = ROUGEScore()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for item, prediction in zip(valid_items, predictions):\n",
    "        # Extract ground truth\n",
    "        ground_truth = None\n",
    "        if 'conversations' in item:\n",
    "            for conv in item['conversations']:\n",
    "                if conv.get('from') == 'gpt':\n",
    "                    ground_truth = conv['value']\n",
    "                    break\n",
    "        elif 'response' in item:\n",
    "            ground_truth = item['response']\n",
    "        elif 'answer' in item:\n",
    "            ground_truth = item['answer']\n",
    "        \n",
    "        if ground_truth is None:\n",
    "            logger.warning(\"No ground truth found for item %s\", item.get('id', 'unknown'))\n",
    "        \n",
    "        if prediction is not None:\n",
    "            results.append({\n",
    "                \"id\": item.get('id', 'unknown'),\n",
    "                \"image\": item['image'],\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"prediction\": prediction\n",
    "            })\n",
    "            \n",
    "           \n",
    "            if ground_truth and prediction.strip() and prediction != \"Error processing image.\":\n",
    "                try:\n",
    "                    score = rouge([prediction], [ground_truth])['rougeL_fmeasure']\n",
    "                    # Convert tensor to float for JSON serialization\n",
    "                    if hasattr(score, 'item'):\n",
    "                        score = score.item()\n",
    "                    else:\n",
    "                        score = float(score)\n",
    "                    rouge_scores.append(score)\n",
    "                    logger.info(\"ROUGE score for %s: %.4f\", item.get('id', 'unknown'), score)\n",
    "                except Exception as e:\n",
    "                    logger.warning(\"Error computing ROUGE score for %s: %s\", item.get('id', 'unknown'), e)\n",
    "        else:\n",
    "            logger.warning(\"Skipping item %s due to inference error\", item.get('id', 'unknown'))\n",
    "\n",
    "    # Save predictions\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "    with open(OUTPUT_JSON, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logger.info(\"Zero-shot predictions saved to %s\", OUTPUT_JSON)\n",
    "\n",
    "    # Log evaluation metrics\n",
    "    if rouge_scores:\n",
    "        rouge_scores = [float(score) if hasattr(score, 'item') else score for score in rouge_scores]\n",
    "        \n",
    "        avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "        max_rouge = max(rouge_scores)\n",
    "        min_rouge = min(rouge_scores)\n",
    "        logger.info(\"=== ZERO-SHOT EVALUATION RESULTS ===\")\n",
    "        logger.info(\"Average ROUGE-L: %.4f\", avg_rouge)\n",
    "        logger.info(\"Max ROUGE-L: %.4f\", max_rouge)\n",
    "        logger.info(\"Min ROUGE-L: %.4f\", min_rouge)\n",
    "        logger.info(\"Number of items with ROUGE scores: %d\", len(rouge_scores))\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary = {\n",
    "            \"method\": \"zero_shot\",\n",
    "            \"avg_rouge\": float(avg_rouge),\n",
    "            \"max_rouge\": float(max_rouge),\n",
    "            \"min_rouge\": float(min_rouge),\n",
    "            \"num_evaluated\": len(rouge_scores),\n",
    "            \"total_predictions\": len(results)\n",
    "        }\n",
    "        with open(os.path.join(LOGS_DIR, \"zero_shot_summary.json\"), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "    else:\n",
    "        logger.info(\"No ROUGE scores computed (no ground truth available)\")\n",
    "\n",
    "    # Log prediction statistics\n",
    "    non_empty_predictions = [r['prediction'] for r in results if r['prediction'] and r['prediction'].strip()]\n",
    "    logger.info(\"Generated %d non-empty predictions out of %d total\", len(non_empty_predictions), len(results))\n",
    "    \n",
    "    logger.info(\"=== SAMPLE ZERO-SHOT PREDICTIONS ===\")\n",
    "    for i, result in enumerate(results[:3]):  \n",
    "        logger.info(\"Sample %d:\", i+1)\n",
    "        logger.info(\" Image: %s\", result['image'])\n",
    "        logger.info(\" Prediction: %s\", result['prediction'])\n",
    "        if result['ground_truth']:\n",
    "            logger.info(\" Ground Truth: %s\", result['ground_truth'][:200])\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting zero-shot evaluation...\")\n",
    "    results = run_zero_shot_evaluation(TEST_JSON, TEST_IMAGE_DIR)\n",
    "    logger.info(\"Zero-shot evaluation completed. %d predictions generated.\", len(results))\n",
    "    \n",
    "    logger.info(\"Results saved to: %s\", OUTPUT_JSON)\n",
    "    logger.info(\"Check logs at: %s\", \"fsdam-s3/logs/zero_shot_inference.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "842961e5-95aa-4daa-a772-a1e5815672cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using default tokenizer.\n",
      "Scoring predictions: 100%|██████████| 81/81 [00:01<00:00, 58.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FSDAM Test Metrics ====\n",
      "\n",
      "ROUGE-L        0.170569\n",
      "BLEU-4         0.020395\n",
      "METEOR         0.301526\n",
      "Exact Match    0.000000\n",
      "dtype: float64\n",
      "\n",
      "==== Per-sample Results (first 5) ====\n",
      "\n",
      "           id                               image  \\\n",
      "0  1003_00057  fsdam/test_set/test/1003_00057.png   \n",
      "1  1003_00195  fsdam/test_set/test/1003_00195.png   \n",
      "2  1003_00385  fsdam/test_set/test/1003_00385.png   \n",
      "3  1008_00038  fsdam/test_set/test/1008_00038.png   \n",
      "4  1008_00226  fsdam/test_set/test/1008_00226.png   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  The scene shows a city street with cars and bu...   \n",
      "1  The scene shows a city street with cars, build...   \n",
      "2  The scene shows a city intersection with traff...   \n",
      "3  A residential street with parked cars and buil...   \n",
      "4  The scene shows a residential street with hous...   \n",
      "\n",
      "                                          prediction   ROUGE-L    BLEU-4  \\\n",
      "0  In the image, the driver's gaze direction appe...  0.184332  0.018887   \n",
      "1  In the image, the driver's gaze appears to be ...  0.168675  0.023147   \n",
      "2  In the image, we see a view from the driver's ...  0.133891  0.014830   \n",
      "3  In the image, we see a street scene with sever...  0.169492  0.012583   \n",
      "4  In this driving scene, we see a view from the ...  0.149780  0.010511   \n",
      "\n",
      "     METEOR  Exact Match  \n",
      "0  0.338376            0  \n",
      "1  0.337205            0  \n",
      "2  0.343962            0  \n",
      "3  0.295725            0  \n",
      "4  0.283202            0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PRED_PATH = 'fsdam-s3/logs/predictions_zero_shot.json'\n",
    "\n",
    "# Load predictions\n",
    "with open(PRED_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "# Initialize scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for entry in tqdm(data, desc=\"Scoring predictions\"):\n",
    "    gt = entry['ground_truth'].strip()\n",
    "    pr = entry['prediction'].strip()\n",
    "\n",
    "    # ROUGE-L\n",
    "    rougeL = rouge.score(gt, pr)['rougeL'].fmeasure\n",
    "\n",
    "    # BLEU-4\n",
    "    bleu4 = sentence_bleu([word_tokenize(gt)], word_tokenize(pr), smoothing_function=smoothie)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = meteor_score([word_tokenize(gt)], word_tokenize(pr))\n",
    "    \n",
    "    # Exact Match\n",
    "    exact = int(gt == pr)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": entry['id'],\n",
    "        \"image\": entry['image'],\n",
    "        \"ground_truth\": gt,\n",
    "        \"prediction\": pr,\n",
    "        \"ROUGE-L\": rougeL,\n",
    "        \"BLEU-4\": bleu4,\n",
    "        \"METEOR\": meteor,\n",
    "        \"Exact Match\": exact\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print mean scores\n",
    "print(\"\\n==== FSDAM Test Metrics ====\\n\")\n",
    "print(df[[\"ROUGE-L\", \"BLEU-4\", \"METEOR\", \"Exact Match\"]].mean())\n",
    "print(\"\\n==== Per-sample Results (first 5) ====\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Save detailed results as CSV if needed\n",
    "df.to_csv('fsdam-s3/logs/test_metrics_one_shot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c2f0e-8bf3-48a2-99df-ca78590fe20e",
   "metadata": {},
   "source": [
    "# Zero shot BLIP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3293aa2-e5b5-4231-94b0-5470f19512ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading BLIP-2 processor...\n",
      "INFO:__main__:Processor loaded successfully\n",
      "INFO:__main__:Loading BLIP-2 model...\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.69s/it]\n",
      "INFO:__main__:BLIP-2 model loaded successfully\n",
      "INFO:__main__:Model type: <class 'transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGeneration'>\n",
      "INFO:__main__:Number of parameters: 1993476096\n",
      "INFO:__main__:Starting BLIP-2 zero-shot evaluation...\n",
      "INFO:__main__:=== TESTING BLIP-2 BASIC FUNCTIONALITY ===\n",
      "INFO:__main__:Basic test response: What do you see in this image?\n",
      "\n",
      "INFO:__main__:Loaded test JSON with 81 entries\n",
      "INFO:__main__:Found 81 valid images out of 81 items\n",
      "INFO:__main__:Processing all 81 images\n",
      "INFO:__main__:Starting BLIP-2 zero-shot inference...\n",
      "INFO:__main__:Processing batch of 81 images with BLIP-2 zero-shot inference\n",
      "INFO:__main__:Processing image 1/81: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__:=== DEBUGGING SINGLE BLIP-2 INFERENCE ===\n",
      "INFO:__main__:Image path: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__:Prompt: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "INFO:__main__:Original image size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Input shapes:\n",
      "INFO:__main__:  input_ids: torch.Size([1, 55])\n",
      "INFO:__main__:  attention_mask: torch.Size([1, 55])\n",
      "INFO:__main__:  pixel_values: torch.Size([1, 3, 224, 224])\n",
      "INFO:__main__:Trying generation config 1: {'max_new_tokens': 512, 'do_sample': False, 'repetition_penalty': 1.1, 'no_repeat_ngram_size': 3}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Input token length: 55\n",
      "INFO:__main__:Full generated text: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "'\n",
      "INFO:__main__:Cleaned text: ''\n",
      "INFO:__main__:Trying generation config 2: {'max_new_tokens': 512, 'do_sample': True, 'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.1}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Input token length: 55\n",
      "INFO:__main__:Full generated text: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "'\n",
      "INFO:__main__:Cleaned text: ''\n",
      "INFO:__main__:Trying generation config 3: {'max_new_tokens': 100, 'do_sample': True, 'temperature': 1.0, 'top_k': 50}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Input token length: 55\n",
      "INFO:__main__:Full generated text: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "'\n",
      "INFO:__main__:Cleaned text: ''\n",
      "INFO:__main__:Trying prompt 2: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:Input shapes:\n",
      "INFO:__main__:  input_ids: torch.Size([1, 46])\n",
      "INFO:__main__:  attention_mask: torch.Size([1, 46])\n",
      "INFO:__main__:  pixel_values: torch.Size([1, 3, 224, 224])\n",
      "INFO:__main__:Trying generation config 1: {'max_new_tokens': 512, 'do_sample': False, 'repetition_penalty': 1.1, 'no_repeat_ngram_size': 3}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 49])\n",
      "INFO:__main__:Input token length: 46\n",
      "INFO:__main__:Full generated text: 'Question: What do you see in this driving scene? Answer: A car\n",
      "'\n",
      "INFO:__main__:Cleaned text: 'A car'\n",
      "INFO:__main__:Trying generation config 2: {'max_new_tokens': 512, 'do_sample': True, 'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.1}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Input token length: 46\n",
      "INFO:__main__:Full generated text: 'Question: What do you see in this driving scene? Answer: A white car that is parked on the left\n",
      "'\n",
      "INFO:__main__:Cleaned text: 'A white car that is parked on the left'\n",
      "INFO:__main__:SUCCESS: Generated valid text with prompt 2, config 2\n",
      "INFO:__main__:Processing image 2/81: fsdam/test_set/test/1003_00195.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00195.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 2: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: YouTube\n",
      "INFO:__main__:Processing image 3/81: fsdam/test_set/test/1003_00385.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00385.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 3: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 3: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a pedestrian and an intersection\n",
      "INFO:__main__:Processing image 4/81: fsdam/test_set/test/1008_00038.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1008_00038.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 4: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 4: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a street and some houses\n",
      "INFO:__main__:Processing image 5/81: fsdam/test_set/test/1008_00226.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1008_00226.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 5: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 5: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A street with houses on both sides\n",
      "INFO:__main__:Processing image 6/81: fsdam/test_set/test/100_00266.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/100_00266.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 6: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 6: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bus and an ambulance\n",
      "INFO:__main__:Processing image 7/81: fsdam/test_set/test/1013_00063.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1013_00063.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 7: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 7: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A white car on the right side of the road\n",
      "INFO:__main__:Processing image 8/81: fsdam/test_set/test/1026_00266.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1026_00266.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 8: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 8: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a pedestrian and an intersection\n",
      "INFO:__main__:Processing image 9/81: fsdam/test_set/test/1038_00138.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1038_00138.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 9: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: This seems an odd question to include on a driving test. It seems odd to me, anyway\n",
      "INFO:__main__:Processing image 10/81: fsdam/test_set/test/1047_00011.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1047_00011.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 10: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 10: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a pedestrian and a traffic light\n",
      "INFO:__main__:Processing image 11/81: fsdam/test_set/test/1051_00143.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1051_00143.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 11: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 11: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: The car driving in the left lane is stopped at an intersection.\n",
      "INFO:__main__:Processing image 12/81: fsdam/test_set/test/1054_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1054_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 12: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 12: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a street and a train\n",
      "INFO:__main__:Processing image 13/81: fsdam/test_set/test/1054_00172.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1054_00172.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 13: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 13: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a pedestrian and an intersection\n",
      "INFO:__main__:Processing image 14/81: fsdam/test_set/test/1060_00104.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1060_00104.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 14: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 14: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A street with cars parked on both sides\n",
      "INFO:__main__:Processing image 15/81: fsdam/test_set/test/1072_00093.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1072_00093.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 15: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 15: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a truck and a bus\n",
      "INFO:__main__:Processing image 16/81: fsdam/test_set/test/1078_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1078_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 16: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 16: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a truck and an overpass\n",
      "INFO:__main__:Processing image 17/81: fsdam/test_set/test/1078_00155.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1078_00155.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 17: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: (hint: the driver's gaze is most likely to be on the horizon and/or a road, the rearview camera)\n",
      "INFO:__main__:Processing image 18/81: fsdam/test_set/test/1086_00109.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1086_00109.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 18: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 18: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a truck and a bus\n",
      "INFO:__main__:Processing image 19/81: fsdam/test_set/test/1087_00482.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1087_00482.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 19: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 19: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A person walking across the street\n",
      "INFO:__main__:Processing image 20/81: fsdam/test_set/test/108_00113.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/108_00113.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 20: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 20: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a truck and an intersection\n",
      "INFO:__main__:Processing image 21/81: fsdam/test_set/test/1114_00066.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1114_00066.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 21: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 21: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a car, a tree and a street\n",
      "INFO:__main__:Processing image 22/81: fsdam/test_set/test/1122_00207.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1122_00207.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 22: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 22: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A stop sign\n",
      "INFO:__main__:Processing image 23/81: fsdam/test_set/test/1123_00171.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1123_00171.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 23: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: This was the first test in a long series meant to study how drivers and pedestrians interact on city\n",
      "INFO:__main__:Processing image 24/81: fsdam/test_set/test/1125_00312.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1125_00312.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 24: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 24: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: What do you see?\n",
      "INFO:__main__:Processing image 25/81: fsdam/test_set/test/112_00138.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/112_00138.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 25: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Describe both the vehicle, the people, how they interact and other environmental factors (such as li\n",
      "INFO:__main__:Processing image 26/81: fsdam/test_set/test/112_00278.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/112_00278.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 26: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 26: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a street with cars, buildings and trees\n",
      "INFO:__main__:Processing image 27/81: fsdam/test_set/test/1132_00247.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1132_00247.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 27: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 27: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a car, a person and a building\n",
      "INFO:__main__:Processing image 28/81: fsdam/test_set/test/1137_00245.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1137_00245.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 28: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 28: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 29/81: fsdam/test_set/test/1138_00081.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1138_00081.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 29: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 29: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 30/81: fsdam/test_set/test/1141_00056.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1141_00056.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 30: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Describe the scene\n",
      "INFO:__main__:Processing image 31/81: fsdam/test_set/test/1146_00099.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1146_00099.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 31: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 31: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 32/81: fsdam/test_set/test/1151_00193.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1151_00193.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 32: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 32: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 33/81: fsdam/test_set/test/115_00168.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/115_00168.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 33: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 33: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A stop sign\n",
      "INFO:__main__:Processing image 34/81: fsdam/test_set/test/1165_00111.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1165_00111.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 34: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 34: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A lot of cars, a lot of traffic and a lot of pedestrians\n",
      "INFO:__main__:Processing image 35/81: fsdam/test_set/test/1167_00169.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1167_00169.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 35: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 35: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: stop sign\n",
      "INFO:__main__:Processing image 36/81: fsdam/test_set/test/116_00263.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/116_00263.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 36: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 36: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: Two buses\n",
      "INFO:__main__:Processing image 37/81: fsdam/test_set/test/1180_00007.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1180_00007.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 37: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 37: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bus and an electric pole\n",
      "INFO:__main__:Processing image 38/81: fsdam/test_set/test/1180_00150.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1180_00150.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 38: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 38: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A bus, a car and a pedestrian\n",
      "INFO:__main__:Processing image 39/81: fsdam/test_set/test/1191_00079.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1191_00079.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 39: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 39: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:Trying prompt 3 for image 39: 'Describe this driving scene:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 2: a car is on the street at night\n",
      "INFO:__main__:Processing image 40/81: fsdam/test_set/test/1203_00057.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1203_00057.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 40: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Describe at least in one word what is happening.\n",
      "INFO:__main__:Processing image 41/81: fsdam/test_set/test/1203_00197.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1203_00197.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 41: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 41: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A street with parked cars, a few houses and a lot of trees\n",
      "INFO:__main__:Processing image 42/81: fsdam/test_set/test/1213_00131.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1213_00131.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 42: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 42: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a school bus\n",
      "INFO:__main__:Processing image 43/81: fsdam/test_set/test/1215_00018.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1215_00018.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 43: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: How does the driver look at the person he or she believes is not worth noticing and does not deserve\n",
      "INFO:__main__:Processing image 44/81: fsdam/test_set/test/1216_00346.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1216_00346.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 44: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: https //www-prod-ssl-de/excerpts-of-text/describe-the-scene-the-drivers-current-gaze-and-where-thats\n",
      "INFO:__main__:Processing image 45/81: fsdam/test_set/test/121_00085.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/121_00085.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 45: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 45: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A lot of cars, a lot of trucks and a lot of traffic\n",
      "INFO:__main__:Processing image 46/81: fsdam/test_set/test/121_00256.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/121_00256.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 46: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: You might suggest that they try to find a place to park their car in the next block as the way home \n",
      "INFO:__main__:Processing image 47/81: fsdam/test_set/test/1249_00159.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1249_00159.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 47: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 47: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 48/81: fsdam/test_set/test/1250_00133.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1250_00133.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 48: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 48: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: The sun\n",
      "INFO:__main__:Processing image 49/81: fsdam/test_set/test/1251_00049.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1251_00049.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 49: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Describe the scene.\n",
      "INFO:__main__:Processing image 50/81: fsdam/test_set/test/1257_00152.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1257_00152.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 50: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: What's the most likely direction that a driver's gaze will move next? What is the most likely direct\n",
      "INFO:__main__:Processing image 51/81: fsdam/test_set/test/1261_00188.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1261_00188.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 51: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 51: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A bus driving down a street\n",
      "INFO:__main__:Processing image 52/81: fsdam/test_set/test/1266_00123.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1266_00123.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 52: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 52: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: The city of san francisco\n",
      "INFO:__main__:Processing image 53/81: fsdam/test_set/test/128_00159.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/128_00159.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 53: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 53: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a streetcar\n",
      "INFO:__main__:Processing image 54/81: fsdam/test_set/test/128_00215.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/128_00215.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 54: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: youtube music video stop by nikko vitturi – youtube\n",
      "INFO:__main__:Processing image 55/81: fsdam/test_set/test/1300_00162.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1300_00162.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 55: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 55: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 56/81: fsdam/test_set/test/1307_00069.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1307_00069.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 56: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 56: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a car, a truck and an SUV\n",
      "INFO:__main__:Processing image 57/81: fsdam/test_set/test/132_00152.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/132_00152.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 57: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 57: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a truck and a motorcycle\n",
      "INFO:__main__:Processing image 58/81: fsdam/test_set/test/1336_00153.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1336_00153.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 58: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Use specific details and your own description to convey to the camera what's going on in the scene.\n",
      "INFO:__main__:Processing image 59/81: fsdam/test_set/test/1355_00105.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1355_00105.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 59: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 59: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a pedestrian and a cyclist\n",
      "INFO:__main__:Processing image 60/81: fsdam/test_set/test/1358_00053.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1358_00053.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 60: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 60: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a person\n",
      "INFO:__main__:Processing image 61/81: fsdam/test_set/test/135_00092.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/135_00092.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 61: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 61: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A cable car\n",
      "INFO:__main__:Processing image 62/81: fsdam/test_set/test/135_00216.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/135_00216.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 62: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 62: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A cable car\n",
      "INFO:__main__:Processing image 63/81: fsdam/test_set/test/1368_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1368_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 63: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Include a time stamp\n",
      "INFO:__main__:Processing image 64/81: fsdam/test_set/test/1374_00009.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1374_00009.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 64: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 64: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a pedestrian and a traffic light\n",
      "INFO:__main__:Processing image 65/81: fsdam/test_set/test/1378_00064.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1378_00064.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 65: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 65: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a building and a street\n",
      "INFO:__main__:Processing image 66/81: fsdam/test_set/test/1386_00222.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1386_00222.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 66: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: What's the next step\n",
      "INFO:__main__:Processing image 67/81: fsdam/test_set/test/138_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/138_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 67: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 67: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a street with cars parked on it\n",
      "INFO:__main__:Processing image 68/81: fsdam/test_set/test/138_00074.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/138_00074.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 68: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Note that we can also view the traffic light change just before the camera snaps this picture\n",
      "INFO:__main__:Processing image 69/81: fsdam/test_set/test/1390_00211.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1390_00211.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 69: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 69: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A street with a lot of parked cars\n",
      "INFO:__main__:Processing image 70/81: fsdam/test_set/test/1402_00128.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1402_00128.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 70: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 70: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a car, a bike and a pedestrian\n",
      "INFO:__main__:Processing image 71/81: fsdam/test_set/test/1402_00350.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1402_00350.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 71: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 71: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a truck and an SUV\n",
      "INFO:__main__:Processing image 72/81: fsdam/test_set/test/1403_00122.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1403_00122.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 72: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 72: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a street and a building\n",
      "INFO:__main__:Processing image 73/81: fsdam/test_set/test/1404_00129.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1404_00129.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 73: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 73: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bus and a tram\n",
      "INFO:__main__:Processing image 74/81: fsdam/test_set/test/1405_00109.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1405_00109.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 74: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 74: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bike and a streetcar\n",
      "INFO:__main__:Processing image 75/81: fsdam/test_set/test/1406_00285.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1406_00285.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 75: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 75: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: a car, a person and an intersection\n",
      "INFO:__main__:Processing image 76/81: fsdam/test_set/test/1420_00121.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1420_00121.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 76: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: The most telling visual behavior is the driver's current gaze.\n",
      "INFO:__main__:Processing image 77/81: fsdam/test_set/test/1425_00102.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1425_00102.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 77: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 77: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A stop sign\n",
      "INFO:__main__:Processing image 78/81: fsdam/test_set/test/1425_00230.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1425_00230.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 78: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 78: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: An intersection\n",
      "INFO:__main__:Processing image 79/81: fsdam/test_set/test/1436_00095.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1436_00095.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 79: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Describe five things you would like to do with the driver before you are pulled over.\n",
      "INFO:__main__:Processing image 80/81: fsdam/test_set/test/1452_00066.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1452_00066.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 80: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:Trying prompt 2 for image 80: 'Question: What do you see in this driving scene? Answer:'\n",
      "INFO:__main__:SUCCESS with fallback prompt 1: A car, a bus and a taxi\n",
      "INFO:__main__:Processing image 81/81: fsdam/test_set/test/1458_00260.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1458_00260.png, size: (1280, 720)\n",
      "INFO:__main__:Trying prompt 1 for image 81: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: See \"describe the scene\" in the left column\n",
      "INFO:__main__:ROUGE score for 1003_00057: 0.1356\n",
      "INFO:__main__:ROUGE score for 1003_00195: 0.0000\n",
      "INFO:__main__:ROUGE score for 1003_00385: 0.0889\n",
      "INFO:__main__:ROUGE score for 1008_00038: 0.1132\n",
      "INFO:__main__:ROUGE score for 1008_00226: 0.2041\n",
      "INFO:__main__:ROUGE score for 100_00266: 0.0784\n",
      "INFO:__main__:ROUGE score for 1013_00063: 0.1471\n",
      "INFO:__main__:ROUGE score for 1026_00266: 0.1290\n",
      "INFO:__main__:ROUGE score for 1038_00138: 0.0625\n",
      "INFO:__main__:ROUGE score for 1047_00011: 0.1569\n",
      "INFO:__main__:ROUGE score for 1051_00143: 0.1053\n",
      "INFO:__main__:ROUGE score for 1054_00005: 0.1200\n",
      "INFO:__main__:ROUGE score for 1054_00172: 0.1224\n",
      "INFO:__main__:ROUGE score for 1060_00104: 0.2222\n",
      "INFO:__main__:ROUGE score for 1072_00093: 0.0896\n",
      "INFO:__main__:ROUGE score for 1078_00005: 0.1034\n",
      "INFO:__main__:ROUGE score for 1078_00155: 0.1846\n",
      "INFO:__main__:ROUGE score for 1086_00109: 0.1053\n",
      "INFO:__main__:ROUGE score for 1087_00482: 0.1034\n",
      "INFO:__main__:ROUGE score for 108_00113: 0.0938\n",
      "INFO:__main__:ROUGE score for 1114_00066: 0.0789\n",
      "INFO:__main__:ROUGE score for 1122_00207: 0.1091\n",
      "INFO:__main__:ROUGE score for 1123_00171: 0.1026\n",
      "INFO:__main__:ROUGE score for 1125_00312: 0.0000\n",
      "INFO:__main__:ROUGE score for 112_00138: 0.0896\n",
      "INFO:__main__:ROUGE score for 112_00278: 0.1224\n",
      "INFO:__main__:ROUGE score for 1132_00247: 0.0938\n",
      "INFO:__main__:ROUGE score for 1137_00245: 0.0690\n",
      "INFO:__main__:ROUGE score for 1138_00081: 0.1224\n",
      "INFO:__main__:ROUGE score for 1141_00056: 0.0678\n",
      "INFO:__main__:ROUGE score for 1146_00099: 0.1538\n",
      "INFO:__main__:ROUGE score for 1151_00193: 0.1000\n",
      "INFO:__main__:ROUGE score for 115_00168: 0.0923\n",
      "INFO:__main__:ROUGE score for 1165_00111: 0.1489\n",
      "INFO:__main__:ROUGE score for 1167_00169: 0.0364\n",
      "INFO:__main__:ROUGE score for 116_00263: 0.0000\n",
      "INFO:__main__:ROUGE score for 1180_00007: 0.0968\n",
      "INFO:__main__:ROUGE score for 1180_00150: 0.1695\n",
      "INFO:__main__:ROUGE score for 1191_00079: 0.1154\n",
      "INFO:__main__:ROUGE score for 1203_00057: 0.0357\n",
      "INFO:__main__:ROUGE score for 1203_00197: 0.2500\n",
      "INFO:__main__:ROUGE score for 1213_00131: 0.1200\n",
      "INFO:__main__:ROUGE score for 1215_00018: 0.1290\n",
      "INFO:__main__:ROUGE score for 1216_00346: 0.2090\n",
      "INFO:__main__:ROUGE score for 121_00085: 0.1311\n",
      "INFO:__main__:ROUGE score for 121_00256: 0.1154\n",
      "INFO:__main__:ROUGE score for 1249_00159: 0.1154\n",
      "INFO:__main__:ROUGE score for 1250_00133: 0.0408\n",
      "INFO:__main__:ROUGE score for 1251_00049: 0.0755\n",
      "INFO:__main__:ROUGE score for 1257_00152: 0.1915\n",
      "INFO:__main__:ROUGE score for 1261_00188: 0.0755\n",
      "INFO:__main__:ROUGE score for 1266_00123: 0.0714\n",
      "INFO:__main__:ROUGE score for 128_00159: 0.1379\n",
      "INFO:__main__:ROUGE score for 128_00215: 0.0000\n",
      "INFO:__main__:ROUGE score for 1300_00162: 0.1000\n",
      "INFO:__main__:ROUGE score for 1307_00069: 0.0667\n",
      "INFO:__main__:ROUGE score for 132_00152: 0.1053\n",
      "INFO:__main__:ROUGE score for 1336_00153: 0.1270\n",
      "INFO:__main__:ROUGE score for 1355_00105: 0.1176\n",
      "INFO:__main__:ROUGE score for 1358_00053: 0.1356\n",
      "INFO:__main__:ROUGE score for 135_00092: 0.0417\n",
      "INFO:__main__:ROUGE score for 135_00216: 0.0392\n",
      "INFO:__main__:ROUGE score for 1368_00005: 0.0385\n",
      "INFO:__main__:ROUGE score for 1374_00009: 0.1481\n",
      "INFO:__main__:ROUGE score for 1378_00064: 0.1071\n",
      "INFO:__main__:ROUGE score for 1386_00222: 0.0364\n",
      "INFO:__main__:ROUGE score for 138_00005: 0.1754\n",
      "INFO:__main__:ROUGE score for 138_00074: 0.0952\n",
      "INFO:__main__:ROUGE score for 1390_00211: 0.2222\n",
      "INFO:__main__:ROUGE score for 1402_00128: 0.0784\n",
      "INFO:__main__:ROUGE score for 1402_00350: 0.1154\n",
      "INFO:__main__:ROUGE score for 1403_00122: 0.1154\n",
      "INFO:__main__:ROUGE score for 1404_00129: 0.1132\n",
      "INFO:__main__:ROUGE score for 1405_00109: 0.1304\n",
      "INFO:__main__:ROUGE score for 1406_00285: 0.1154\n",
      "INFO:__main__:ROUGE score for 1420_00121: 0.1356\n",
      "INFO:__main__:ROUGE score for 1425_00102: 0.1224\n",
      "INFO:__main__:ROUGE score for 1425_00230: 0.0392\n",
      "INFO:__main__:ROUGE score for 1436_00095: 0.0597\n",
      "INFO:__main__:ROUGE score for 1452_00066: 0.1509\n",
      "INFO:__main__:ROUGE score for 1458_00260: 0.1017\n",
      "INFO:__main__:BLIP-2 zero-shot predictions saved to fsdam-s3/logs/blip2_predictions_zero_shot.json\n",
      "INFO:__main__:=== BLIP-2 ZERO-SHOT EVALUATION RESULTS ===\n",
      "INFO:__main__:Average ROUGE-L: 0.1070\n",
      "INFO:__main__:Max ROUGE-L: 0.2500\n",
      "INFO:__main__:Min ROUGE-L: 0.0000\n",
      "INFO:__main__:Number of items with ROUGE scores: 81\n",
      "INFO:__main__:Generated 81 non-empty predictions out of 81 total\n",
      "INFO:__main__:=== SAMPLE BLIP-2 ZERO-SHOT PREDICTIONS ===\n",
      "INFO:__main__:Sample 1:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__: Prediction: A white car that is parked on the left\n",
      "INFO:__main__: Ground Truth: The scene shows a city street with cars and buildings. The current gaze focuses on the blue car ahead. The future gaze will likely remain on the blue car. This is because the driver needs to maintain \n",
      "INFO:__main__:Sample 2:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00195.png\n",
      "INFO:__main__: Prediction: YouTube\n",
      "INFO:__main__: Ground Truth: The scene shows a city street with cars, buildings, and a KFC. The current gaze is focused on the blue car ahead. The future gaze will likely shift to the red traffic light. This is because the driver\n",
      "INFO:__main__:Sample 3:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00385.png\n",
      "INFO:__main__: Prediction: A car, a pedestrian and an intersection\n",
      "INFO:__main__: Ground Truth: The scene shows a city intersection with traffic and buildings. The current gaze focuses on the red traffic light. The future gaze will likely shift to the green light. This is because the light chang\n",
      "INFO:__main__:BLIP-2 zero-shot evaluation completed. 81 predictions generated.\n",
      "INFO:__main__:Results saved to: fsdam-s3/logs/blip2_predictions_zero_shot.json\n",
      "INFO:__main__:Check logs at: fsdam-s3/logs/blip2_zero_shot_inference.log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from torchmetrics.text import ROUGEScore\n",
    "import logging\n",
    "import gc\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "file_handler = logging.FileHandler(\"fsdam-s3/logs/blip2_zero_shot_inference.log\")\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(formatter)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Define paths and constants\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"  # You can also try \"Salesforce/blip2-flan-t5-xl\"\n",
    "TEST_JSON = \"fsdam/test_set/test_llava.json\"\n",
    "TEST_IMAGE_DIR = \"fsdam/test_set/test\"\n",
    "OUTPUT_JSON = \"fsdam-s3/logs/blip2_predictions_zero_shot.json\"\n",
    "LOGS_DIR = \"fsdam-s3/logs\"\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "# Create logs directory\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Load processor and model\n",
    "try:\n",
    "    logger.info(\"Loading BLIP-2 processor...\")\n",
    "    processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "    logger.info(\"Processor loaded successfully\")\n",
    "    \n",
    "    logger.info(\"Loading BLIP-2 model...\")\n",
    "    \n",
    "    # Configure quantization for memory efficiency\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    logger.info(\"BLIP-2 model loaded successfully\")\n",
    "    logger.info(\"Model type: %s\", type(model))\n",
    "    logger.info(\"Number of parameters: %d\", sum(p.numel() for p in model.parameters()))\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(\"Error loading BLIP-2 model: %s\", e)\n",
    "    raise e\n",
    "\n",
    "def debug_single_inference(image_path, prompt):\n",
    "    \"\"\"\n",
    "    Debug function to test single image inference with BLIP-2\n",
    "    \"\"\"\n",
    "    logger.info(\"=== DEBUGGING SINGLE BLIP-2 INFERENCE ===\")\n",
    "    logger.info(\"Image path: %s\", image_path)\n",
    "    logger.info(\"Prompt: %s\", prompt)\n",
    "    \n",
    "    try:\n",
    "        # Load and process image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        logger.info(\"Original image size: %s\", image.size)\n",
    "        \n",
    "        # Try different prompt styles for BLIP-2\n",
    "        prompts_to_try = [\n",
    "            prompt,  # Use the same prompt as the main function\n",
    "            \"Question: What do you see in this driving scene? Answer:\",\n",
    "            \"Describe the driving scene:\",\n",
    "            \"What is happening in this image?\",\n",
    "            \"\"  # Empty prompt for unconditional generation\n",
    "        ]\n",
    "        \n",
    "        for i, test_prompt in enumerate(prompts_to_try):\n",
    "            logger.info(\"Trying prompt %d: '%s'\", i+1, test_prompt)\n",
    "            \n",
    "            try:\n",
    "                # Process inputs for BLIP-2\n",
    "                if test_prompt:\n",
    "                    inputs = processor(\n",
    "                        images=image,\n",
    "                        text=test_prompt,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                else:\n",
    "                    # Unconditional generation (no text prompt)\n",
    "                    inputs = processor(\n",
    "                        images=image,\n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                \n",
    "                # Move to device\n",
    "                device = next(model.parameters()).device\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                \n",
    "                logger.info(\"Input shapes:\")\n",
    "                for k, v in inputs.items():\n",
    "                    if hasattr(v, 'shape'):\n",
    "                        logger.info(\"  %s: %s\", k, v.shape)\n",
    "                \n",
    "                # Generate with different strategies\n",
    "                generation_configs = [\n",
    "                    {\n",
    "                        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "                        \"do_sample\": False,  # Greedy decoding\n",
    "                        \"repetition_penalty\": 1.1,\n",
    "                        \"no_repeat_ngram_size\": 3\n",
    "                    },\n",
    "                    {\n",
    "                        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "                        \"do_sample\": True,\n",
    "                        \"temperature\": 0.7,\n",
    "                        \"top_p\": 0.9,\n",
    "                        \"repetition_penalty\": 1.1\n",
    "                    },\n",
    "                    {\n",
    "                        \"max_new_tokens\": 100,  # Shorter generation\n",
    "                        \"do_sample\": True,\n",
    "                        \"temperature\": 1.0,\n",
    "                        \"top_k\": 50\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                for j, gen_config in enumerate(generation_configs):\n",
    "                    logger.info(\"Trying generation config %d: %s\", j+1, gen_config)\n",
    "                    \n",
    "                    try:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model.generate(**inputs, **gen_config)\n",
    "                        \n",
    "                        logger.info(\"Generation completed. Output shape: %s\", outputs.shape)\n",
    "                        logger.info(\"Input token length: %s\", inputs.get('input_ids', torch.tensor([[]])).shape[1] if 'input_ids' in inputs else 0)\n",
    "                        \n",
    "                        # Decode output\n",
    "                        generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                        logger.info(\"Full generated text: '%s'\", generated_text)\n",
    "                        \n",
    "                        # Clean up the output\n",
    "                        if test_prompt and test_prompt in generated_text:\n",
    "                            cleaned_text = generated_text.replace(test_prompt, \"\").strip()\n",
    "                        else:\n",
    "                            cleaned_text = generated_text.strip()\n",
    "                        \n",
    "                        logger.info(\"Cleaned text: '%s'\", cleaned_text)\n",
    "                        \n",
    "                        if len(cleaned_text.strip()) > 5:\n",
    "                            logger.info(\"SUCCESS: Generated valid text with prompt %d, config %d\", i+1, j+1)\n",
    "                            return cleaned_text\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        logger.warning(\"Generation config %d failed: %s\", j+1, e)\n",
    "                        continue\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(\"Prompt %d failed: %s\", i+1, e)\n",
    "                continue\n",
    "        \n",
    "        logger.warning(\"All prompt and generation strategies failed\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in debug BLIP-2 inference: %s\", e)\n",
    "        import traceback\n",
    "        logger.error(\"Traceback: %s\", traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def predict_batch_zero_shot_blip2(image_paths, prompt=None, batch_size=1):\n",
    "    \"\"\"\n",
    "    Zero-shot batch inference using BLIP-2\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing batch of %d images with BLIP-2 zero-shot inference\", len(image_paths))\n",
    "    \n",
    "    # Use the same prompt as LLaVA for fair comparison\n",
    "    if prompt is None:\n",
    "        prompt = \"Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\"\n",
    "    \n",
    "    # Fallback prompts to try if the main prompt fails\n",
    "    fallback_prompts = [\n",
    "        \"Question: What do you see in this driving scene? Answer:\",\n",
    "        \"Describe this driving scene:\",\n",
    "        \"What is happening in this image?\",\n",
    "        \"\"  # Unconditional generation\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        predictions = []\n",
    "        \n",
    "        for i, image_path in enumerate(image_paths):\n",
    "            logger.info(\"Processing image %d/%d: %s\", i+1, len(image_paths), image_path)\n",
    "            \n",
    "            # Debug first image in detail\n",
    "            if i == 0:\n",
    "                debug_result = debug_single_inference(image_path, prompt)\n",
    "                if debug_result and len(debug_result.strip()) > 5:\n",
    "                    predictions.append(debug_result)\n",
    "                    continue\n",
    "            \n",
    "            try:\n",
    "                if not os.path.exists(image_path):\n",
    "                    logger.error(\"Image file not found: %s\", image_path)\n",
    "                    predictions.append(\"Image file not found.\")\n",
    "                    continue\n",
    "                \n",
    "                # Load image\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                logger.info(\"Loaded image: %s, size: %s\", image_path, image.size)\n",
    "                \n",
    "                prediction = None\n",
    "                \n",
    "                # First try the main prompt (same as LLaVA)\n",
    "                prompts_to_try = [prompt] + fallback_prompts\n",
    "                \n",
    "                # Try different prompts until one works\n",
    "                for prompt_idx, test_prompt in enumerate(prompts_to_try):\n",
    "                    try:\n",
    "                        logger.info(\"Trying prompt %d for image %d: '%s'\", prompt_idx+1, i+1, test_prompt)\n",
    "                        \n",
    "                        # Process inputs for BLIP-2\n",
    "                        if test_prompt:\n",
    "                            inputs = processor(\n",
    "                                images=image,\n",
    "                                text=test_prompt,\n",
    "                                return_tensors=\"pt\"\n",
    "                            )\n",
    "                        else:\n",
    "                            # Unconditional generation\n",
    "                            inputs = processor(\n",
    "                                images=image,\n",
    "                                return_tensors=\"pt\"\n",
    "                            )\n",
    "                        \n",
    "                        # Move to device\n",
    "                        device = next(model.parameters()).device\n",
    "                        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                        \n",
    "                        # Try different generation strategies\n",
    "                        generation_configs = [\n",
    "                            {\"max_new_tokens\": 150, \"do_sample\": False, \"repetition_penalty\": 1.1},  # Greedy\n",
    "                            {\"max_new_tokens\": 150, \"do_sample\": True, \"temperature\": 0.7, \"top_p\": 0.9},  # Sampling\n",
    "                            {\"max_new_tokens\": 50, \"do_sample\": True, \"temperature\": 1.0, \"top_k\": 50}  # High temp, short\n",
    "                        ]\n",
    "                        \n",
    "                        for gen_config in generation_configs:\n",
    "                            try:\n",
    "                                with torch.no_grad():\n",
    "                                    outputs = model.generate(**inputs, **gen_config)\n",
    "                                \n",
    "                                # Decode output\n",
    "                                generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                                \n",
    "                                # Clean up the output\n",
    "                                if test_prompt and test_prompt in generated_text:\n",
    "                                    cleaned_text = generated_text.replace(test_prompt, \"\").strip()\n",
    "                                else:\n",
    "                                    cleaned_text = generated_text.strip()\n",
    "                                \n",
    "                                if len(cleaned_text.strip()) > 5:\n",
    "                                    prediction = cleaned_text\n",
    "                                    if prompt_idx == 0:\n",
    "                                        logger.info(\"SUCCESS with main prompt: %s\", prediction[:100])\n",
    "                                    else:\n",
    "                                        logger.info(\"SUCCESS with fallback prompt %d: %s\", prompt_idx, prediction[:100])\n",
    "                                    break\n",
    "                                    \n",
    "                            except Exception as e:\n",
    "                                logger.warning(\"Generation failed for prompt %d, config %s: %s\", prompt_idx+1, gen_config, e)\n",
    "                                continue\n",
    "                        \n",
    "                        if prediction:\n",
    "                            break\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        logger.warning(\"Prompt %d failed for image %d: %s\", prompt_idx+1, i+1, e)\n",
    "                        continue\n",
    "                \n",
    "                if not prediction or len(prediction.strip()) < 5:\n",
    "                    prediction = \"Unable to generate description for this driving scene.\"\n",
    "                \n",
    "                predictions.append(prediction)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(\"Error processing image %s: %s\", image_path, e)\n",
    "                predictions.append(\"Error processing image.\")\n",
    "            \n",
    "            # Clear memory after each image\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Error in BLIP-2 zero-shot batch inference: %s\", e)\n",
    "        return [\"Error in inference\"] * len(image_paths)\n",
    "\n",
    "def test_blip2_basic():\n",
    "    \"\"\"\n",
    "    Test BLIP-2 with a simple image to verify it's working\n",
    "    \"\"\"\n",
    "    logger.info(\"=== TESTING BLIP-2 BASIC FUNCTIONALITY ===\")\n",
    "    \n",
    "    try:\n",
    "        # Create a simple test image (white square)\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create a simple 224x224 white image for testing\n",
    "        test_image = Image.fromarray(np.ones((224, 224, 3), dtype=np.uint8) * 255)\n",
    "        test_prompt = \"What do you see in this image?\"\n",
    "        \n",
    "        inputs = processor(\n",
    "            images=test_image,\n",
    "            text=test_prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "        logger.info(\"Basic test response: %s\", response)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\"Basic BLIP-2 test failed: %s\", e)\n",
    "        return False\n",
    "\n",
    "def run_blip2_zero_shot_evaluation(json_path, image_dir):\n",
    "    \"\"\"\n",
    "    Run BLIP-2 zero-shot evaluation on the test dataset\n",
    "    \"\"\"\n",
    "    # Test basic model functionality\n",
    "    test_blip2_basic()\n",
    "    \n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        logger.info(\"Loaded test JSON with %d entries\", len(test_data))\n",
    "    except Exception as e:\n",
    "        logger.error(\"Error loading test JSON %s: %s\", json_path, e)\n",
    "        raise e\n",
    "\n",
    "    # Build image paths with error handling\n",
    "    image_paths = []\n",
    "    valid_items = []\n",
    "    \n",
    "    for item in test_data:\n",
    "        image_path = os.path.join(image_dir, os.path.basename(item['image']))\n",
    "        if not os.path.exists(image_path):\n",
    "            # Try alternative path\n",
    "            image_path = os.path.join(image_dir, item['image'])\n",
    "            if not os.path.exists(image_path):\n",
    "                logger.warning(\"Image not found for item %s: %s\", item.get('id', 'unknown'), image_path)\n",
    "                continue\n",
    "        \n",
    "        image_paths.append(image_path)\n",
    "        valid_items.append(item)\n",
    "    \n",
    "    logger.info(\"Found %d valid images out of %d items\", len(image_paths), len(test_data))\n",
    "    \n",
    "    # Process all images\n",
    "    logger.info(\"Processing all %d images\", len(image_paths))\n",
    "    \n",
    "    # Run BLIP-2 zero-shot predictions\n",
    "    logger.info(\"Starting BLIP-2 zero-shot inference...\")\n",
    "    predictions = predict_batch_zero_shot_blip2(image_paths, batch_size=1)\n",
    "    \n",
    "    # Process results and calculate metrics\n",
    "    results = []\n",
    "    rouge = ROUGEScore()\n",
    "    rouge_scores = []\n",
    "    \n",
    "    for item, prediction in zip(valid_items, predictions):\n",
    "        # Extract ground truth\n",
    "        ground_truth = None\n",
    "        if 'conversations' in item:\n",
    "            for conv in item['conversations']:\n",
    "                if conv.get('from') == 'gpt':\n",
    "                    ground_truth = conv['value']\n",
    "                    break\n",
    "        elif 'response' in item:\n",
    "            ground_truth = item['response']\n",
    "        elif 'answer' in item:\n",
    "            ground_truth = item['answer']\n",
    "        \n",
    "        if ground_truth is None:\n",
    "            logger.warning(\"No ground truth found for item %s\", item.get('id', 'unknown'))\n",
    "        \n",
    "        if prediction is not None:\n",
    "            results.append({\n",
    "                \"id\": item.get('id', 'unknown'),\n",
    "                \"image\": item['image'],\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"prediction\": prediction\n",
    "            })\n",
    "            \n",
    "            # Calculate ROUGE score if ground truth available\n",
    "            if ground_truth and prediction.strip() and prediction != \"Error processing image.\":\n",
    "                try:\n",
    "                    score = rouge([prediction], [ground_truth])['rougeL_fmeasure']\n",
    "                    # Convert tensor to float for JSON serialization\n",
    "                    if hasattr(score, 'item'):\n",
    "                        score = score.item()\n",
    "                    else:\n",
    "                        score = float(score)\n",
    "                    rouge_scores.append(score)\n",
    "                    logger.info(\"ROUGE score for %s: %.4f\", item.get('id', 'unknown'), score)\n",
    "                except Exception as e:\n",
    "                    logger.warning(\"Error computing ROUGE score for %s: %s\", item.get('id', 'unknown'), e)\n",
    "        else:\n",
    "            logger.warning(\"Skipping item %s due to inference error\", item.get('id', 'unknown'))\n",
    "\n",
    "    # Save predictions\n",
    "    os.makedirs(os.path.dirname(OUTPUT_JSON), exist_ok=True)\n",
    "    with open(OUTPUT_JSON, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    logger.info(\"BLIP-2 zero-shot predictions saved to %s\", OUTPUT_JSON)\n",
    "\n",
    "    # Log evaluation metrics\n",
    "    if rouge_scores:\n",
    "        # Ensure all scores are Python floats for JSON serialization\n",
    "        rouge_scores = [float(score) if hasattr(score, 'item') else score for score in rouge_scores]\n",
    "        \n",
    "        avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "        max_rouge = max(rouge_scores)\n",
    "        min_rouge = min(rouge_scores)\n",
    "        logger.info(\"=== BLIP-2 ZERO-SHOT EVALUATION RESULTS ===\")\n",
    "        logger.info(\"Average ROUGE-L: %.4f\", avg_rouge)\n",
    "        logger.info(\"Max ROUGE-L: %.4f\", max_rouge)\n",
    "        logger.info(\"Min ROUGE-L: %.4f\", min_rouge)\n",
    "        logger.info(\"Number of items with ROUGE scores: %d\", len(rouge_scores))\n",
    "        \n",
    "        # Save summary statistics\n",
    "        summary = {\n",
    "            \"model\": \"blip2\",\n",
    "            \"avg_rouge\": float(avg_rouge),\n",
    "            \"max_rouge\": float(max_rouge),\n",
    "            \"min_rouge\": float(min_rouge),\n",
    "            \"num_evaluated\": len(rouge_scores),\n",
    "            \"total_predictions\": len(results)\n",
    "        }\n",
    "        with open(os.path.join(LOGS_DIR, \"blip2_zero_shot_summary.json\"), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "    else:\n",
    "        logger.info(\"No ROUGE scores computed (no ground truth available)\")\n",
    "\n",
    "    # Log prediction statistics\n",
    "    non_empty_predictions = [r['prediction'] for r in results if r['prediction'] and r['prediction'].strip()]\n",
    "    logger.info(\"Generated %d non-empty predictions out of %d total\", len(non_empty_predictions), len(results))\n",
    "    \n",
    "    logger.info(\"=== SAMPLE BLIP-2 ZERO-SHOT PREDICTIONS ===\")\n",
    "    for i, result in enumerate(results[:3]):  # Show first 3 predictions\n",
    "        logger.info(\"Sample %d:\", i+1)\n",
    "        logger.info(\" Image: %s\", result['image'])\n",
    "        logger.info(\" Prediction: %s\", result['prediction'])\n",
    "        if result['ground_truth']:\n",
    "            logger.info(\" Ground Truth: %s\", result['ground_truth'][:200])\n",
    "    \n",
    "    # Clean up memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return results\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting BLIP-2 zero-shot evaluation...\")\n",
    "    results = run_blip2_zero_shot_evaluation(TEST_JSON, TEST_IMAGE_DIR)\n",
    "    logger.info(\"BLIP-2 zero-shot evaluation completed. %d predictions generated.\", len(results))\n",
    "    \n",
    "    logger.info(\"Results saved to: %s\", OUTPUT_JSON)\n",
    "    logger.info(\"Check logs at: %s\", \"fsdam-s3/logs/blip2_zero_shot_inference.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96041835-0fc2-438f-ac39-80cfce06ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading BLIP-2 processor...\n",
      "INFO:__main__:Processor loaded successfully\n",
      "INFO:__main__:Loading BLIP-2 model...\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]\n",
      "INFO:__main__:BLIP-2 model loaded successfully\n",
      "INFO:__main__:Model type: <class 'transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGeneration'>\n",
      "INFO:__main__:Number of parameters: 1993476096\n",
      "INFO:__main__:Starting BLIP-2 fair comparison zero-shot evaluation...\n",
      "INFO:__main__:Loaded test JSON with 81 entries\n",
      "INFO:__main__:Found 81 valid images out of 81 items\n",
      "INFO:__main__:Processing all 81 images\n",
      "INFO:__main__:Starting BLIP-2 zero-shot inference (fair comparison)...\n",
      "INFO:__main__:Processing batch of 81 images with BLIP-2 zero-shot inference\n",
      "INFO:__main__:Using ONLY the main prompt for fair comparison: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "INFO:__main__:Processing image 1/81: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__:=== DEBUGGING SINGLE BLIP-2 INFERENCE ===\n",
      "INFO:__main__:Image path: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__:Prompt: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "INFO:__main__:Original image size: (1280, 720)\n",
      "INFO:__main__:Using main prompt only for fair comparison\n",
      "INFO:__main__:Input shapes:\n",
      "INFO:__main__:  input_ids: torch.Size([1, 55])\n",
      "INFO:__main__:  attention_mask: torch.Size([1, 55])\n",
      "INFO:__main__:  pixel_values: torch.Size([1, 3, 224, 224])\n",
      "INFO:__main__:Trying generation config 1 with SAME prompt: {'max_new_tokens': 512, 'do_sample': False, 'repetition_penalty': 1.1, 'no_repeat_ngram_size': 3}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Generated text: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "\n",
      "INFO:__main__:Cleaned text: \n",
      "WARNING:__main__:Generation config 1 produced short output: ''\n",
      "INFO:__main__:Trying generation config 2 with SAME prompt: {'max_new_tokens': 512, 'do_sample': True, 'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.1}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Generated text: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "\n",
      "INFO:__main__:Cleaned text: \n",
      "WARNING:__main__:Generation config 2 produced short output: ''\n",
      "INFO:__main__:Trying generation config 3 with SAME prompt: {'max_new_tokens': 200, 'do_sample': True, 'temperature': 1.0, 'top_k': 50}\n",
      "INFO:__main__:Generation completed. Output shape: torch.Size([1, 56])\n",
      "INFO:__main__:Generated text: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\n",
      "\n",
      "INFO:__main__:Cleaned text: \n",
      "WARNING:__main__:Generation config 3 produced short output: ''\n",
      "WARNING:__main__:All generation strategies failed with main prompt\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00057.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 1: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: https://www.youtube-nocookie-window-fallout-nocookie-window-fallout/ video\n",
      "INFO:__main__:Processing image 2/81: fsdam/test_set/test/1003_00195.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00195.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 2: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 2\n",
      "INFO:__main__:Processing image 3/81: fsdam/test_set/test/1003_00385.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1003_00385.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 3: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 3\n",
      "INFO:__main__:Processing image 4/81: fsdam/test_set/test/1008_00038.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1008_00038.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 4: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 4\n",
      "INFO:__main__:Processing image 5/81: fsdam/test_set/test/1008_00226.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1008_00226.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 5: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: What is the next logical choice, what is the road like in the next block?\n",
      "INFO:__main__:Processing image 6/81: fsdam/test_set/test/100_00266.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/100_00266.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 6: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 6\n",
      "INFO:__main__:Processing image 7/81: fsdam/test_set/test/1013_00063.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1013_00063.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 7: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 7\n",
      "INFO:__main__:Processing image 8/81: fsdam/test_set/test/1026_00266.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1026_00266.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 8: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 8\n",
      "INFO:__main__:Processing image 9/81: fsdam/test_set/test/1038_00138.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1038_00138.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 9: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 9\n",
      "INFO:__main__:Processing image 10/81: fsdam/test_set/test/1047_00011.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1047_00011.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 10: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 10\n",
      "INFO:__main__:Processing image 11/81: fsdam/test_set/test/1051_00143.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1051_00143.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 11: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 11\n",
      "INFO:__main__:Processing image 12/81: fsdam/test_set/test/1054_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1054_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 12: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 12\n",
      "INFO:__main__:Processing image 13/81: fsdam/test_set/test/1054_00172.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1054_00172.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 13: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 13\n",
      "INFO:__main__:Processing image 14/81: fsdam/test_set/test/1060_00104.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1060_00104.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 14: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 14\n",
      "INFO:__main__:Processing image 15/81: fsdam/test_set/test/1072_00093.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1072_00093.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 15: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 15\n",
      "INFO:__main__:Processing image 16/81: fsdam/test_set/test/1078_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1078_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 16: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 16\n",
      "INFO:__main__:Processing image 17/81: fsdam/test_set/test/1078_00155.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1078_00155.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 17: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 17\n",
      "INFO:__main__:Processing image 18/81: fsdam/test_set/test/1086_00109.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1086_00109.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 18: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 18\n",
      "INFO:__main__:Processing image 19/81: fsdam/test_set/test/1087_00482.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1087_00482.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 19: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 19\n",
      "INFO:__main__:Processing image 20/81: fsdam/test_set/test/108_00113.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/108_00113.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 20: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 20\n",
      "INFO:__main__:Processing image 21/81: fsdam/test_set/test/1114_00066.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1114_00066.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 21: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 21\n",
      "INFO:__main__:Processing image 22/81: fsdam/test_set/test/1122_00207.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1122_00207.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 22: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Use the following terms: the driver, the action, the observation, the scene\n",
      "INFO:__main__:Processing image 23/81: fsdam/test_set/test/1123_00171.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1123_00171.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 23: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 23\n",
      "INFO:__main__:Processing image 24/81: fsdam/test_set/test/1125_00312.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1125_00312.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 24: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 24\n",
      "INFO:__main__:Processing image 25/81: fsdam/test_set/test/112_00138.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/112_00138.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 25: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 25\n",
      "INFO:__main__:Processing image 26/81: fsdam/test_set/test/112_00278.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/112_00278.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 26: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 26\n",
      "INFO:__main__:Processing image 27/81: fsdam/test_set/test/1132_00247.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1132_00247.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 27: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 27\n",
      "INFO:__main__:Processing image 28/81: fsdam/test_set/test/1137_00245.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1137_00245.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 28: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 28\n",
      "INFO:__main__:Processing image 29/81: fsdam/test_set/test/1138_00081.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1138_00081.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 29: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 29\n",
      "INFO:__main__:Processing image 30/81: fsdam/test_set/test/1141_00056.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1141_00056.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 30: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 30\n",
      "INFO:__main__:Processing image 31/81: fsdam/test_set/test/1146_00099.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1146_00099.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 31: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 31\n",
      "INFO:__main__:Processing image 32/81: fsdam/test_set/test/1151_00193.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1151_00193.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 32: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 32\n",
      "INFO:__main__:Processing image 33/81: fsdam/test_set/test/115_00168.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/115_00168.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 33: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 33\n",
      "INFO:__main__:Processing image 34/81: fsdam/test_set/test/1165_00111.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1165_00111.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 34: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 34\n",
      "INFO:__main__:Processing image 35/81: fsdam/test_set/test/1167_00169.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1167_00169.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 35: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: (photo courtesy of giphy )\n",
      "INFO:__main__:Processing image 36/81: fsdam/test_set/test/116_00263.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/116_00263.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 36: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 36\n",
      "INFO:__main__:Processing image 37/81: fsdam/test_set/test/1180_00007.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1180_00007.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 37: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Your answers can come in the form of a paragraph, a quote from an article, or other creative work. P\n",
      "INFO:__main__:Processing image 38/81: fsdam/test_set/test/1180_00150.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1180_00150.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 38: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 38\n",
      "INFO:__main__:Processing image 39/81: fsdam/test_set/test/1191_00079.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1191_00079.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 39: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 39\n",
      "INFO:__main__:Processing image 40/81: fsdam/test_set/test/1203_00057.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1203_00057.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 40: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 40\n",
      "INFO:__main__:Processing image 41/81: fsdam/test_set/test/1203_00197.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1203_00197.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 41: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 41\n",
      "INFO:__main__:Processing image 42/81: fsdam/test_set/test/1213_00131.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1213_00131.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 42: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 42\n",
      "INFO:__main__:Processing image 43/81: fsdam/test_set/test/1215_00018.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1215_00018.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 43: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 43\n",
      "INFO:__main__:Processing image 44/81: fsdam/test_set/test/1216_00346.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1216_00346.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 44: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 44\n",
      "INFO:__main__:Processing image 45/81: fsdam/test_set/test/121_00085.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/121_00085.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 45: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Your sentence should not list all of those, nor should it list all of the options.\n",
      "INFO:__main__:Processing image 46/81: fsdam/test_set/test/121_00256.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/121_00256.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 46: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 46\n",
      "INFO:__main__:Processing image 47/81: fsdam/test_set/test/1249_00159.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1249_00159.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 47: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 47\n",
      "INFO:__main__:Processing image 48/81: fsdam/test_set/test/1250_00133.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1250_00133.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 48: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: Be sure to describe the driver's current gaze as accurately as possible.\n",
      "INFO:__main__:Processing image 49/81: fsdam/test_set/test/1251_00049.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1251_00049.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 49: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 49\n",
      "INFO:__main__:Processing image 50/81: fsdam/test_set/test/1257_00152.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1257_00152.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 50: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 50\n",
      "INFO:__main__:Processing image 51/81: fsdam/test_set/test/1261_00188.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1261_00188.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 51: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 51\n",
      "INFO:__main__:Processing image 52/81: fsdam/test_set/test/1266_00123.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1266_00123.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 52: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 52\n",
      "INFO:__main__:Processing image 53/81: fsdam/test_set/test/128_00159.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/128_00159.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 53: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 53\n",
      "INFO:__main__:Processing image 54/81: fsdam/test_set/test/128_00215.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/128_00215.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 54: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 54\n",
      "INFO:__main__:Processing image 55/81: fsdam/test_set/test/1300_00162.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1300_00162.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 55: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: This will inform the person writing the review and your reader\n",
      "INFO:__main__:Processing image 56/81: fsdam/test_set/test/1307_00069.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1307_00069.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 56: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 56\n",
      "INFO:__main__:Processing image 57/81: fsdam/test_set/test/132_00152.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/132_00152.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 57: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 57\n",
      "INFO:__main__:Processing image 58/81: fsdam/test_set/test/1336_00153.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1336_00153.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 58: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 58\n",
      "INFO:__main__:Processing image 59/81: fsdam/test_set/test/1355_00105.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1355_00105.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 59: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 59\n",
      "INFO:__main__:Processing image 60/81: fsdam/test_set/test/1358_00053.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1358_00053.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 60: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 60\n",
      "INFO:__main__:Processing image 61/81: fsdam/test_set/test/135_00092.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/135_00092.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 61: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 61\n",
      "INFO:__main__:Processing image 62/81: fsdam/test_set/test/135_00216.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/135_00216.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 62: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 62\n",
      "INFO:__main__:Processing image 63/81: fsdam/test_set/test/1368_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1368_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 63: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 63\n",
      "INFO:__main__:Processing image 64/81: fsdam/test_set/test/1374_00009.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1374_00009.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 64: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 64\n",
      "INFO:__main__:Processing image 65/81: fsdam/test_set/test/1378_00064.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1378_00064.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 65: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 65\n",
      "INFO:__main__:Processing image 66/81: fsdam/test_set/test/1386_00222.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1386_00222.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 66: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "INFO:__main__:SUCCESS with main prompt: This test should take no more than 1 mnns. Study the picture you just took very hard pic\n",
      "INFO:__main__:Processing image 67/81: fsdam/test_set/test/138_00005.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/138_00005.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 67: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 67\n",
      "INFO:__main__:Processing image 68/81: fsdam/test_set/test/138_00074.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/138_00074.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 68: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 68\n",
      "INFO:__main__:Processing image 69/81: fsdam/test_set/test/1390_00211.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1390_00211.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 69: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 69\n",
      "INFO:__main__:Processing image 70/81: fsdam/test_set/test/1402_00128.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1402_00128.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 70: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 70\n",
      "INFO:__main__:Processing image 71/81: fsdam/test_set/test/1402_00350.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1402_00350.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 71: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 71\n",
      "INFO:__main__:Processing image 72/81: fsdam/test_set/test/1403_00122.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1403_00122.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 72: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 72\n",
      "INFO:__main__:Processing image 73/81: fsdam/test_set/test/1404_00129.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1404_00129.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 73: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 73\n",
      "INFO:__main__:Processing image 74/81: fsdam/test_set/test/1405_00109.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1405_00109.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 74: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 74\n",
      "INFO:__main__:Processing image 75/81: fsdam/test_set/test/1406_00285.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1406_00285.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 75: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 75\n",
      "INFO:__main__:Processing image 76/81: fsdam/test_set/test/1420_00121.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1420_00121.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 76: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 76\n",
      "INFO:__main__:Processing image 77/81: fsdam/test_set/test/1425_00102.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1425_00102.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 77: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 77\n",
      "INFO:__main__:Processing image 78/81: fsdam/test_set/test/1425_00230.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1425_00230.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 78: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 78\n",
      "INFO:__main__:Processing image 79/81: fsdam/test_set/test/1436_00095.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1436_00095.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 79: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 79\n",
      "INFO:__main__:Processing image 80/81: fsdam/test_set/test/1452_00066.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1452_00066.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 80: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 80\n",
      "INFO:__main__:Processing image 81/81: fsdam/test_set/test/1458_00260.png\n",
      "INFO:__main__:Loaded image: fsdam/test_set/test/1458_00260.png, size: (1280, 720)\n",
      "INFO:__main__:Using main prompt for image 81: 'Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.'\n",
      "WARNING:__main__:Could not generate prediction with main prompt for image 81\n",
      "INFO:__main__:ROUGE score for 1003_00057: 0.0000\n",
      "INFO:__main__:ROUGE score for 1003_00195: 0.0606\n",
      "INFO:__main__:ROUGE score for 1003_00385: 0.0870\n",
      "INFO:__main__:ROUGE score for 1008_00038: 0.0741\n",
      "INFO:__main__:ROUGE score for 1008_00226: 0.1404\n",
      "INFO:__main__:ROUGE score for 100_00266: 0.0769\n",
      "INFO:__main__:ROUGE score for 1013_00063: 0.0606\n",
      "INFO:__main__:ROUGE score for 1026_00266: 0.0317\n",
      "INFO:__main__:ROUGE score for 1038_00138: 0.0364\n",
      "INFO:__main__:ROUGE score for 1047_00011: 0.0784\n",
      "INFO:__main__:ROUGE score for 1051_00143: 0.0755\n",
      "INFO:__main__:ROUGE score for 1054_00005: 0.0392\n",
      "INFO:__main__:ROUGE score for 1054_00172: 0.0800\n",
      "INFO:__main__:ROUGE score for 1060_00104: 0.0741\n",
      "INFO:__main__:ROUGE score for 1072_00093: 0.0294\n",
      "INFO:__main__:ROUGE score for 1078_00005: 0.0678\n",
      "INFO:__main__:ROUGE score for 1078_00155: 0.0755\n",
      "INFO:__main__:ROUGE score for 1086_00109: 0.0690\n",
      "INFO:__main__:ROUGE score for 1087_00482: 0.1000\n",
      "INFO:__main__:ROUGE score for 108_00113: 0.0308\n",
      "INFO:__main__:ROUGE score for 1114_00066: 0.0519\n",
      "INFO:__main__:ROUGE score for 1122_00207: 0.1562\n",
      "INFO:__main__:ROUGE score for 1123_00171: 0.0303\n",
      "INFO:__main__:ROUGE score for 1125_00312: 0.0392\n",
      "INFO:__main__:ROUGE score for 112_00138: 0.0741\n",
      "INFO:__main__:ROUGE score for 112_00278: 0.0800\n",
      "INFO:__main__:ROUGE score for 1132_00247: 0.0308\n",
      "INFO:__main__:ROUGE score for 1137_00245: 0.0678\n",
      "INFO:__main__:ROUGE score for 1138_00081: 0.0800\n",
      "INFO:__main__:ROUGE score for 1141_00056: 0.0312\n",
      "INFO:__main__:ROUGE score for 1146_00099: 0.0755\n",
      "INFO:__main__:ROUGE score for 1151_00193: 0.0494\n",
      "INFO:__main__:ROUGE score for 115_00168: 0.0571\n",
      "INFO:__main__:ROUGE score for 1165_00111: 0.0449\n",
      "INFO:__main__:ROUGE score for 1167_00169: 0.0351\n",
      "INFO:__main__:ROUGE score for 116_00263: 0.0328\n",
      "INFO:__main__:ROUGE score for 1180_00007: 0.1250\n",
      "INFO:__main__:ROUGE score for 1180_00150: 0.0333\n",
      "INFO:__main__:ROUGE score for 1191_00079: 0.1154\n",
      "INFO:__main__:ROUGE score for 1203_00057: 0.0364\n",
      "INFO:__main__:ROUGE score for 1203_00197: 0.0392\n",
      "INFO:__main__:ROUGE score for 1213_00131: 0.0727\n",
      "INFO:__main__:ROUGE score for 1215_00018: 0.0364\n",
      "INFO:__main__:ROUGE score for 1216_00346: 0.0755\n",
      "INFO:__main__:ROUGE score for 121_00085: 0.0625\n",
      "INFO:__main__:ROUGE score for 121_00256: 0.0625\n",
      "INFO:__main__:ROUGE score for 1249_00159: 0.0755\n",
      "INFO:__main__:ROUGE score for 1250_00133: 0.1000\n",
      "INFO:__main__:ROUGE score for 1251_00049: 0.0690\n",
      "INFO:__main__:ROUGE score for 1257_00152: 0.0727\n",
      "INFO:__main__:ROUGE score for 1261_00188: 0.0727\n",
      "INFO:__main__:ROUGE score for 1266_00123: 0.0339\n",
      "INFO:__main__:ROUGE score for 128_00159: 0.0339\n",
      "INFO:__main__:ROUGE score for 128_00215: 0.0741\n",
      "INFO:__main__:ROUGE score for 1300_00162: 0.1250\n",
      "INFO:__main__:ROUGE score for 1307_00069: 0.0328\n",
      "INFO:__main__:ROUGE score for 132_00152: 0.0690\n",
      "INFO:__main__:ROUGE score for 1336_00153: 0.0385\n",
      "INFO:__main__:ROUGE score for 1355_00105: 0.0385\n",
      "INFO:__main__:ROUGE score for 1358_00053: 0.0667\n",
      "INFO:__main__:ROUGE score for 135_00092: 0.0755\n",
      "INFO:__main__:ROUGE score for 135_00216: 0.0714\n",
      "INFO:__main__:ROUGE score for 1368_00005: 0.0714\n",
      "INFO:__main__:ROUGE score for 1374_00009: 0.0741\n",
      "INFO:__main__:ROUGE score for 1378_00064: 0.0351\n",
      "INFO:__main__:ROUGE score for 1386_00222: 0.0294\n",
      "INFO:__main__:ROUGE score for 138_00005: 0.0690\n",
      "INFO:__main__:ROUGE score for 138_00074: 0.0370\n",
      "INFO:__main__:ROUGE score for 1390_00211: 0.0444\n",
      "INFO:__main__:ROUGE score for 1402_00128: 0.0769\n",
      "INFO:__main__:ROUGE score for 1402_00350: 0.0755\n",
      "INFO:__main__:ROUGE score for 1403_00122: 0.0377\n",
      "INFO:__main__:ROUGE score for 1404_00129: 0.0370\n",
      "INFO:__main__:ROUGE score for 1405_00109: 0.0851\n",
      "INFO:__main__:ROUGE score for 1406_00285: 0.0377\n",
      "INFO:__main__:ROUGE score for 1420_00121: 0.0357\n",
      "INFO:__main__:ROUGE score for 1425_00102: 0.0370\n",
      "INFO:__main__:ROUGE score for 1425_00230: 0.0702\n",
      "INFO:__main__:ROUGE score for 1436_00095: 0.0678\n",
      "INFO:__main__:ROUGE score for 1452_00066: 0.0741\n",
      "INFO:__main__:ROUGE score for 1458_00260: 0.0678\n",
      "INFO:__main__:BLIP-2 fair comparison predictions saved to fsdam-s3/logs/blip2_fair_predictions_zero_shot.json\n",
      "INFO:__main__:=== BLIP-2 FAIR COMPARISON ZERO-SHOT EVALUATION RESULTS ===\n",
      "INFO:__main__:Average ROUGE-L: 0.0617\n",
      "INFO:__main__:Max ROUGE-L: 0.1562\n",
      "INFO:__main__:Min ROUGE-L: 0.0000\n",
      "INFO:__main__:Number of items with ROUGE scores: 81\n",
      "INFO:__main__:Generated 81 non-empty predictions out of 81 total\n",
      "INFO:__main__:=== SAMPLE BLIP-2 FAIR COMPARISON ZERO-SHOT PREDICTIONS ===\n",
      "INFO:__main__:Sample 1:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00057.png\n",
      "INFO:__main__: Prediction: https://www.youtube-nocookie-window-fallout-nocookie-window-fallout/ video\n",
      "INFO:__main__: Ground Truth: The scene shows a city street with cars and buildings. The current gaze focuses on the blue car ahead. The future gaze will likely remain on the blue car. This is because the driver needs to maintain \n",
      "INFO:__main__:Sample 2:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00195.png\n",
      "INFO:__main__: Prediction: Unable to generate description for this driving scene.\n",
      "INFO:__main__: Ground Truth: The scene shows a city street with cars, buildings, and a KFC. The current gaze is focused on the blue car ahead. The future gaze will likely shift to the red traffic light. This is because the driver\n",
      "INFO:__main__:Sample 3:\n",
      "INFO:__main__: Image: fsdam/test_set/test/1003_00385.png\n",
      "INFO:__main__: Prediction: Unable to generate description for this driving scene.\n",
      "INFO:__main__: Ground Truth: The scene shows a city intersection with traffic and buildings. The current gaze focuses on the red traffic light. The future gaze will likely shift to the green light. This is because the light chang\n",
      "INFO:__main__:BLIP-2 fair comparison zero-shot evaluation completed. 81 predictions generated.\n",
      "INFO:__main__:Results saved to: fsdam-s3/logs/blip2_fair_predictions_zero_shot.json\n",
      "INFO:__main__:Check logs at: fsdam-s3/logs/blip2_fair_zero_shot_inference.log\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load model (simplified)\n",
    "MODEL_ID = \"Salesforce/blip2-opt-2.7b\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "def test_blip2_prompts(image_path):\n",
    "    \"\"\"Test different prompt approaches with BLIP-2\"\"\"\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Test different prompt styles\n",
    "    prompts_to_test = [\n",
    "        # Our main prompt\n",
    "        \"Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why.\",\n",
    "        \n",
    "        # Simpler versions\n",
    "        \"Describe this driving scene.\",\n",
    "        \"What do you see in this image?\",\n",
    "        \"Describe the image.\",\n",
    "        \n",
    "        # Question format\n",
    "        \"Question: What is happening in this driving scene? Answer:\",\n",
    "        \"Question: Describe the scene, the driver's current gaze, and where that gaze will likely shift next and why. Answer:\",\n",
    "        \n",
    "        # Direct instruction\n",
    "        \"Please describe this driving scene in detail.\",\n",
    "        \n",
    "        # Empty prompt (unconditional)\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, prompt in enumerate(prompts_to_test):\n",
    "        logger.info(f\"\\n=== Testing prompt {i+1}: '{prompt}' ===\")\n",
    "        \n",
    "        try:\n",
    "            # Process\n",
    "            if prompt:\n",
    "                inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "            else:\n",
    "                inputs = processor(images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            device = next(model.parameters()).device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Test different generation configs\n",
    "            gen_configs = [\n",
    "                {\"max_new_tokens\": 50, \"do_sample\": False},  # Short greedy\n",
    "                {\"max_new_tokens\": 100, \"do_sample\": True, \"temperature\": 0.7},  # Medium sampling\n",
    "                {\"max_new_tokens\": 200, \"do_sample\": True, \"temperature\": 1.0, \"top_k\": 50},  # Long high-temp\n",
    "            ]\n",
    "            \n",
    "            for j, gen_config in enumerate(gen_configs):\n",
    "                try:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model.generate(**inputs, **gen_config)\n",
    "                    \n",
    "                    # Decode\n",
    "                    generated_text = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "                    \n",
    "                    # Clean\n",
    "                    if prompt and prompt in generated_text:\n",
    "                        cleaned = generated_text.replace(prompt, \"\").strip()\n",
    "                    else:\n",
    "                        cleaned = generated_text.strip()\n",
    "                    \n",
    "                    logger.info(f\"  Config {j+1}: '{cleaned}'\")\n",
    "                    \n",
    "                    if len(cleaned) > 5:\n",
    "                        results[f\"prompt_{i+1}_config_{j+1}\"] = {\n",
    "                            \"prompt\": prompt,\n",
    "                            \"config\": gen_config,\n",
    "                            \"output\": cleaned,\n",
    "                            \"success\": True\n",
    "                        }\n",
    "                        break  # Found working config for this prompt\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"  Config {j+1} failed: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prompt {i+1} failed completely: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_basic_blip2():\n",
    "    \"\"\"Test basic BLIP-2 functionality\"\"\"\n",
    "    logger.info(\"=== TESTING BASIC BLIP-2 FUNCTIONALITY ===\")\n",
    "    \n",
    "    # Create simple test image\n",
    "    test_image = Image.fromarray(np.ones((224, 224, 3), dtype=np.uint8) * 255)\n",
    "    \n",
    "    # Test very simple prompt\n",
    "    inputs = processor(images=test_image, text=\"What do you see?\", return_tensors=\"pt\")\n",
    "    device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n",
    "    \n",
    "    result = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    logger.info(f\"Basic test result: '{result}'\")\n",
    "    \n",
    "    return \"What do you see?\" in result or len(result.replace(\"What do you see?\", \"\").strip()) > 0\n",
    "\n",
    "# Main diagnostic\n",
    "if __name__ == \"__main__\":\n",
    "    # Test basic functionality\n",
    "    basic_works = test_basic_blip2()\n",
    "    logger.info(f\"Basic BLIP-2 test: {'PASSED' if basic_works else 'FAILED'}\")\n",
    "    \n",
    "    # Test with a real driving image\n",
    "    test_image_path = \"fsdam/test_set/test/1003_00057.png\"\n",
    "    if os.path.exists(test_image_path):\n",
    "        logger.info(f\"\\n=== TESTING WITH REAL IMAGE: {test_image_path} ===\")\n",
    "        results = test_blip2_prompts(test_image_path)\n",
    "        \n",
    "        logger.info(f\"\\n=== SUMMARY ===\")\n",
    "        logger.info(f\"Working configurations: {len(results)}\")\n",
    "        \n",
    "        for key, result in results.items():\n",
    "            logger.info(f\"{key}: '{result['output']}'\")\n",
    "            \n",
    "        if results:\n",
    "            logger.info(\"\\n=== RECOMMENDATION ===\")\n",
    "            best_result = list(results.values())[0]\n",
    "            logger.info(f\"Best working prompt: '{best_result['prompt']}'\")\n",
    "            logger.info(f\"Best config: {best_result['config']}\")\n",
    "            logger.info(f\"Example output: '{best_result['output']}'\")\n",
    "        else:\n",
    "            logger.error(\"NO WORKING CONFIGURATIONS FOUND!\")\n",
    "    else:\n",
    "        logger.error(f\"Test image not found: {test_image_path}\")\n",
    "        \n",
    "    logger.info(\"Diagnostic complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d837f1e4-40d7-4b79-b727-9db85d71a8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:absl:Using default tokenizer.\n",
      "Scoring predictions: 100%|██████████| 81/81 [00:00<00:00, 269.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FSDAM Test Metrics ====\n",
      "\n",
      "ROUGE-L        0.115087\n",
      "BLEU-4         0.001743\n",
      "METEOR         0.051481\n",
      "Exact Match    0.000000\n",
      "dtype: float64\n",
      "\n",
      "==== Per-sample Results (first 5) ====\n",
      "\n",
      "           id                               image  \\\n",
      "0  1003_00057  fsdam/test_set/test/1003_00057.png   \n",
      "1  1003_00195  fsdam/test_set/test/1003_00195.png   \n",
      "2  1003_00385  fsdam/test_set/test/1003_00385.png   \n",
      "3  1008_00038  fsdam/test_set/test/1008_00038.png   \n",
      "4  1008_00226  fsdam/test_set/test/1008_00226.png   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  The scene shows a city street with cars and bu...   \n",
      "1  The scene shows a city street with cars, build...   \n",
      "2  The scene shows a city intersection with traff...   \n",
      "3  A residential street with parked cars and buil...   \n",
      "4  The scene shows a residential street with hous...   \n",
      "\n",
      "                                prediction   ROUGE-L    BLEU-4    METEOR  \\\n",
      "0   A white car that is parked on the left  0.135593  0.000506  0.060606   \n",
      "1                                  YouTube  0.000000  0.000000  0.000000   \n",
      "2  A car, a pedestrian and an intersection  0.088889  0.000537  0.038860   \n",
      "3          A car, a street and some houses  0.113208  0.000212  0.043668   \n",
      "4       A street with houses on both sides  0.204082  0.000593  0.101595   \n",
      "\n",
      "   Exact Match  \n",
      "0            0  \n",
      "1            0  \n",
      "2            0  \n",
      "3            0  \n",
      "4            0  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Path to your JSON file\n",
    "PRED_PATH = 'fsdam-s3/logs/blip2_predictions_zero_shot.json'\n",
    "\n",
    "# Load predictions\n",
    "with open(PRED_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Lists to store results\n",
    "results = []\n",
    "\n",
    "# Initialize scorer\n",
    "rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "for entry in tqdm(data, desc=\"Scoring predictions\"):\n",
    "    gt = entry['ground_truth'].strip()\n",
    "    pr = entry['prediction'].strip()\n",
    "\n",
    "    # ROUGE-L\n",
    "    rougeL = rouge.score(gt, pr)['rougeL'].fmeasure\n",
    "\n",
    "    # BLEU-4\n",
    "    bleu4 = sentence_bleu([word_tokenize(gt)], word_tokenize(pr), smoothing_function=smoothie)\n",
    "    \n",
    "    # METEOR\n",
    "    meteor = meteor_score([word_tokenize(gt)], word_tokenize(pr))\n",
    "    \n",
    "    # Exact Match\n",
    "    exact = int(gt == pr)\n",
    "\n",
    "    results.append({\n",
    "        \"id\": entry['id'],\n",
    "        \"image\": entry['image'],\n",
    "        \"ground_truth\": gt,\n",
    "        \"prediction\": pr,\n",
    "        \"ROUGE-L\": rougeL,\n",
    "        \"BLEU-4\": bleu4,\n",
    "        \"METEOR\": meteor,\n",
    "        \"Exact Match\": exact\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Print mean scores\n",
    "print(\"\\n==== FSDAM Test Metrics ====\\n\")\n",
    "print(df[[\"ROUGE-L\", \"BLEU-4\", \"METEOR\", \"Exact Match\"]].mean())\n",
    "print(\"\\n==== Per-sample Results (first 5) ====\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Save detailed results as CSV if needed\n",
    "df.to_csv('fsdam-s3/logs/test_metrics_one_shot.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7627adbe-d1c4-4973-8b03-e9cf07bff0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7942652f-5ce5-4eab-9254-15ad6e1541b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
